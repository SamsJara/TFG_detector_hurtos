{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamsJara/TFG_detector_hurtos/blob/main/TesisV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d374O-qcXHaQ"
      },
      "source": [
        "### **CONECTAR A GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PbYVRcQXHEb",
        "outputId": "e1f561a2-aafb-4431-e788-f18422e5dcd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMBbaVnXOkTf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Ruta real en tu Google Drive\n",
        "origen = \"/content/drive/MyDrive/ColabNotebooks/TesisV2\"\n",
        "\n",
        "# Ruta simbólica dentro de Colab\n",
        "destino = \"/content/tesisV2\"\n",
        "\n",
        "# Crear enlace simbólico (acceso directo)\n",
        "if not os.path.exists(destino):\n",
        "    os.symlink(origen, destino)\n",
        "\n",
        "# Cambiar el directorio actual a esa carpeta\n",
        "os.chdir(destino)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e6aXTdlEyW8"
      },
      "source": [
        "### **ARBOL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSFaGo_Ri5Nf",
        "outputId": "eb89142c-8262-41c2-c735-3f0b8dfd8a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tree is already the newest version (2.0.2-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "\u001b[01;34m/content/tesis/runsPersona/\u001b[0m\n",
            "└── \u001b[01;34mdetect\u001b[0m\n",
            "    ├── \u001b[01;34mpredict\u001b[0m\n",
            "    │   └── \u001b[01;35mhurto1.avi\u001b[0m\n",
            "    └── \u001b[01;34myolov8_personas_hurtos\u001b[0m\n",
            "        ├── \u001b[00margs.yaml\u001b[0m\n",
            "        ├── \u001b[00mBoxF1_curve.png\u001b[0m\n",
            "        ├── \u001b[00mBoxP_curve.png\u001b[0m\n",
            "        ├── \u001b[00mBoxPR_curve.png\u001b[0m\n",
            "        ├── \u001b[00mBoxR_curve.png\u001b[0m\n",
            "        ├── \u001b[00mconfusion_matrix_normalized.png\u001b[0m\n",
            "        ├── \u001b[00mconfusion_matrix.png\u001b[0m\n",
            "        ├── \u001b[01;35mlabels_correlogram.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mlabels.jpg\u001b[0m\n",
            "        ├── \u001b[00mresults.csv\u001b[0m\n",
            "        ├── \u001b[00mresults.png\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch0.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch1.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch2.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch33920.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch33921.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch33922.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch0_labels.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch0_pred.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch1_labels.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch1_pred.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch2_labels.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch2_pred.jpg\u001b[0m\n",
            "        └── \u001b[01;34mweights\u001b[0m\n",
            "            ├── \u001b[00mbest.pt\u001b[0m\n",
            "            └── \u001b[00mlast.pt\u001b[0m\n",
            "\n",
            "4 directories, 26 files\n"
          ]
        }
      ],
      "source": [
        "!apt install tree\n",
        "!tree /content/tesis/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi_mpvRHkss0"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/sample_data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1wlXlvnXfRI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "# Ruta base donde se va a crear el dataset_final\n",
        "base_path = 'datasetv1'  # O podés usar 'content/tesis/dataset_final' si estás en Colab\n",
        "\n",
        "# Crear carpetas necesarias\n",
        "splits = ['train', 'valid', 'test']\n",
        "for split in splits:\n",
        "    os.makedirs(os.path.join(base_path, split, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_path, split, 'labels'), exist_ok=True)\n",
        "\n",
        "# Crear archivo data.yaml\n",
        "data_yaml = {\n",
        "    'train': 'train/images',\n",
        "    'val': 'valid/images',\n",
        "    'test': 'test/images',\n",
        "    'nc': 2,\n",
        "    'names': ['normal', 'shoplifting']\n",
        "}\n",
        "\n",
        "with open(os.path.join(base_path, 'data.yaml'), 'w') as f:\n",
        "    yaml.dump(data_yaml, f)\n",
        "\n",
        "print(f\"✅ Estructura creada en: {base_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnTj4j9gNQww"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0nZ5qLQoxVN"
      },
      "source": [
        "## ***DESCOMPRIMIR ZIP***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXzPndh-c5JM",
        "outputId": "ba6fb696-0efd-47c9-b29e-e6f5180e67c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning [/content/dataset3.zip]:  514762 extra bytes at beginning or within zipfile\n",
            "  (attempting to process anyway)\n",
            "file #1:  bad zipfile offset (local header sig):  514762\n",
            "  (attempting to re-compensate)\n",
            "error: not enough memory for bomb detection\n",
            "file #2:  bad zipfile offset (local header sig):  895700260\n",
            "  (attempting to re-compensate)\n"
          ]
        }
      ],
      "source": [
        "!unzip -q /content/dataset3.zip -d  /content/tesis/dataset3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJnijYtdd7DK"
      },
      "source": [
        "## ***YOLO, OPENCV, SYMPY***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgmlH-XOBYuU",
        "outputId": "5da565af-ff61-46a8-fb12-ba198d10551d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.188-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.3.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.4.0)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.19.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.16-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy>=1.23.0 (from ultralytics)\n",
            "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.9.86)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.3.188-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.16-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: numpy, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.2\n",
            "    Uninstalling numpy-2.3.2:\n",
            "      Successfully uninstalled numpy-2.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.6 ultralytics-8.3.188 ultralytics-thop-2.0.16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "54464f76b16d40ac950ea02e62b3a6aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy==1.12\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.12/dist-packages (from sympy==1.12) (1.3.0)\n",
            "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "Successfully installed sympy-1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              },
              "id": "f51e1772eda344a291d4566bb50dabc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics opencv-python\n",
        "!pip install -U sympy==1.12\n",
        "import sympy\n",
        "print(sympy.__version__)  # debería mostrar 1.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc4I-AOkpTtG"
      },
      "source": [
        "## ***PRUEBA: TRACKING DE PERSONAS CON YOLO, COCO Y BYTETRACK PARA ID***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N8Bt61PXBpPZ",
        "outputId": "4936b5a1-9c3a-4ce9-d183-92c7ce1cd604"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100%|██████████| 21.5M/21.5M [00:00<00:00, 69.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.8s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "video 1/1 (frame 1/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 528.1ms\n",
            "video 1/1 (frame 2/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 364.4ms\n",
            "video 1/1 (frame 3/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 331.1ms\n",
            "video 1/1 (frame 4/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 353.2ms\n",
            "video 1/1 (frame 5/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 324.9ms\n",
            "video 1/1 (frame 6/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 342.5ms\n",
            "video 1/1 (frame 7/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 322.3ms\n",
            "video 1/1 (frame 8/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 345.7ms\n",
            "video 1/1 (frame 9/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 330.0ms\n",
            "video 1/1 (frame 10/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 346.9ms\n",
            "video 1/1 (frame 11/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 329.0ms\n",
            "video 1/1 (frame 12/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 340.2ms\n",
            "video 1/1 (frame 13/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 458.9ms\n",
            "video 1/1 (frame 14/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 529.9ms\n",
            "video 1/1 (frame 15/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 525.1ms\n",
            "video 1/1 (frame 16/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 530.7ms\n",
            "video 1/1 (frame 17/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 549.1ms\n",
            "video 1/1 (frame 18/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 394.5ms\n",
            "video 1/1 (frame 19/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 332.9ms\n",
            "video 1/1 (frame 20/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 348.5ms\n",
            "video 1/1 (frame 21/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 338.9ms\n",
            "video 1/1 (frame 22/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 400.9ms\n",
            "video 1/1 (frame 23/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.4ms\n",
            "video 1/1 (frame 24/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 327.9ms\n",
            "video 1/1 (frame 25/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 335.6ms\n",
            "video 1/1 (frame 26/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 325.5ms\n",
            "video 1/1 (frame 27/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 334.2ms\n",
            "video 1/1 (frame 28/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 334.4ms\n",
            "video 1/1 (frame 29/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 347.3ms\n",
            "video 1/1 (frame 30/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 324.7ms\n",
            "video 1/1 (frame 31/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 343.2ms\n",
            "video 1/1 (frame 32/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 328.2ms\n",
            "video 1/1 (frame 33/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 340.6ms\n",
            "video 1/1 (frame 34/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 336.2ms\n",
            "video 1/1 (frame 35/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 325.1ms\n",
            "video 1/1 (frame 36/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 355.7ms\n",
            "video 1/1 (frame 37/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 334.3ms\n",
            "video 1/1 (frame 38/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 355.6ms\n",
            "video 1/1 (frame 39/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 354.2ms\n",
            "video 1/1 (frame 40/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 544.3ms\n",
            "video 1/1 (frame 41/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 540.3ms\n",
            "video 1/1 (frame 42/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 527.5ms\n",
            "video 1/1 (frame 43/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 547.7ms\n",
            "video 1/1 (frame 44/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 535.3ms\n",
            "video 1/1 (frame 45/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 377.7ms\n",
            "video 1/1 (frame 46/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 355.5ms\n",
            "video 1/1 (frame 47/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 331.5ms\n",
            "video 1/1 (frame 48/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 336.9ms\n",
            "video 1/1 (frame 49/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 332.9ms\n",
            "video 1/1 (frame 50/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 330.4ms\n",
            "video 1/1 (frame 51/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 334.1ms\n",
            "video 1/1 (frame 52/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 338.2ms\n",
            "video 1/1 (frame 53/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 352.0ms\n",
            "video 1/1 (frame 54/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 333.9ms\n",
            "video 1/1 (frame 55/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 368.7ms\n",
            "video 1/1 (frame 56/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 335.5ms\n",
            "video 1/1 (frame 57/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 340.3ms\n",
            "video 1/1 (frame 58/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 331.2ms\n",
            "video 1/1 (frame 59/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 331.2ms\n",
            "video 1/1 (frame 60/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 349.1ms\n",
            "video 1/1 (frame 61/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 332.6ms\n",
            "video 1/1 (frame 62/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 346.5ms\n",
            "video 1/1 (frame 63/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 335.0ms\n",
            "video 1/1 (frame 64/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 329.3ms\n",
            "video 1/1 (frame 65/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 330.8ms\n",
            "video 1/1 (frame 66/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 435.6ms\n",
            "video 1/1 (frame 67/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 562.7ms\n",
            "video 1/1 (frame 68/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 629.1ms\n",
            "video 1/1 (frame 69/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 523.1ms\n",
            "video 1/1 (frame 70/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 526.4ms\n",
            "video 1/1 (frame 71/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 470.7ms\n",
            "video 1/1 (frame 72/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 336.5ms\n",
            "video 1/1 (frame 73/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.8ms\n",
            "video 1/1 (frame 74/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 337.8ms\n",
            "video 1/1 (frame 75/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 366.2ms\n",
            "video 1/1 (frame 76/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 339.0ms\n",
            "video 1/1 (frame 77/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.5ms\n",
            "video 1/1 (frame 78/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 338.9ms\n",
            "video 1/1 (frame 79/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 360.8ms\n",
            "video 1/1 (frame 80/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.1ms\n",
            "video 1/1 (frame 81/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 363.0ms\n",
            "video 1/1 (frame 82/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 328.9ms\n",
            "video 1/1 (frame 83/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 329.2ms\n",
            "video 1/1 (frame 84/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 327.9ms\n",
            "video 1/1 (frame 85/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 369.7ms\n",
            "video 1/1 (frame 86/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 343.9ms\n",
            "video 1/1 (frame 87/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 327.6ms\n",
            "video 1/1 (frame 88/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 337.8ms\n",
            "video 1/1 (frame 89/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 329.5ms\n",
            "video 1/1 (frame 90/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 339.0ms\n",
            "video 1/1 (frame 91/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 365.7ms\n",
            "video 1/1 (frame 92/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 480.6ms\n",
            "video 1/1 (frame 93/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 518.4ms\n",
            "video 1/1 (frame 94/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 518.7ms\n",
            "video 1/1 (frame 95/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 517.2ms\n",
            "video 1/1 (frame 96/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 523.1ms\n",
            "video 1/1 (frame 97/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 429.0ms\n",
            "video 1/1 (frame 98/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 323.2ms\n",
            "video 1/1 (frame 99/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 331.6ms\n",
            "video 1/1 (frame 100/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 324.8ms\n",
            "video 1/1 (frame 101/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 340.1ms\n",
            "video 1/1 (frame 102/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 347.5ms\n",
            "video 1/1 (frame 103/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 323.0ms\n",
            "video 1/1 (frame 104/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 325.8ms\n",
            "video 1/1 (frame 105/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 328.6ms\n",
            "video 1/1 (frame 106/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 352.4ms\n",
            "video 1/1 (frame 107/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 329.3ms\n",
            "video 1/1 (frame 108/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 327.8ms\n",
            "video 1/1 (frame 109/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 343.5ms\n",
            "video 1/1 (frame 110/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 335.7ms\n",
            "video 1/1 (frame 111/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 350.3ms\n",
            "video 1/1 (frame 112/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 326.3ms\n",
            "video 1/1 (frame 113/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 327.7ms\n",
            "video 1/1 (frame 114/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 391.5ms\n",
            "video 1/1 (frame 115/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 340.6ms\n",
            "video 1/1 (frame 116/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 358.3ms\n",
            "video 1/1 (frame 117/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 325.2ms\n",
            "video 1/1 (frame 118/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 352.0ms\n",
            "video 1/1 (frame 119/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 327.1ms\n",
            "video 1/1 (frame 120/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 412.3ms\n",
            "video 1/1 (frame 121/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 519.0ms\n",
            "video 1/1 (frame 122/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 1001.3ms\n",
            "video 1/1 (frame 123/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 1055.0ms\n",
            "video 1/1 (frame 124/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 524.7ms\n",
            "video 1/1 (frame 125/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 361.2ms\n",
            "video 1/1 (frame 126/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 335.2ms\n",
            "video 1/1 (frame 127/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 349.1ms\n",
            "video 1/1 (frame 128/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 337.4ms\n",
            "video 1/1 (frame 129/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 341.4ms\n",
            "video 1/1 (frame 130/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 352.5ms\n",
            "video 1/1 (frame 131/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 340.9ms\n",
            "video 1/1 (frame 132/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 346.6ms\n",
            "video 1/1 (frame 133/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 332.0ms\n",
            "video 1/1 (frame 134/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 356.8ms\n",
            "video 1/1 (frame 135/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 333.4ms\n",
            "video 1/1 (frame 136/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 349.1ms\n",
            "video 1/1 (frame 137/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 370.4ms\n",
            "video 1/1 (frame 138/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 341.5ms\n",
            "video 1/1 (frame 139/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 333.9ms\n",
            "video 1/1 (frame 140/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.9ms\n",
            "video 1/1 (frame 141/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 347.7ms\n",
            "video 1/1 (frame 142/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 331.9ms\n",
            "video 1/1 (frame 143/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 345.4ms\n",
            "video 1/1 (frame 144/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 332.4ms\n",
            "video 1/1 (frame 145/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 350.0ms\n",
            "video 1/1 (frame 146/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 521.8ms\n",
            "video 1/1 (frame 147/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 518.9ms\n",
            "video 1/1 (frame 148/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 524.2ms\n",
            "video 1/1 (frame 149/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 547.9ms\n",
            "video 1/1 (frame 150/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 538.4ms\n",
            "video 1/1 (frame 151/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 460.5ms\n",
            "video 1/1 (frame 152/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 394.2ms\n",
            "video 1/1 (frame 153/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 324.3ms\n",
            "video 1/1 (frame 154/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 329.3ms\n",
            "video 1/1 (frame 155/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 353.2ms\n",
            "video 1/1 (frame 156/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 324.7ms\n",
            "video 1/1 (frame 157/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 342.9ms\n",
            "video 1/1 (frame 158/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 329.1ms\n",
            "video 1/1 (frame 159/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 338.5ms\n",
            "video 1/1 (frame 160/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 390.6ms\n",
            "video 1/1 (frame 161/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 512.8ms\n",
            "video 1/1 (frame 162/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 505.1ms\n",
            "video 1/1 (frame 163/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 525.8ms\n",
            "video 1/1 (frame 164/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 517.9ms\n",
            "video 1/1 (frame 165/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 521.9ms\n",
            "video 1/1 (frame 166/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 526.0ms\n",
            "video 1/1 (frame 167/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 351.5ms\n",
            "video 1/1 (frame 168/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 325.7ms\n",
            "video 1/1 (frame 169/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 327.4ms\n",
            "video 1/1 (frame 170/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 328.1ms\n",
            "video 1/1 (frame 171/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 350.7ms\n",
            "video 1/1 (frame 172/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 475.2ms\n",
            "video 1/1 (frame 173/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 514.8ms\n",
            "video 1/1 (frame 174/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 531.4ms\n",
            "video 1/1 (frame 175/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 537.1ms\n",
            "video 1/1 (frame 176/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 508.1ms\n",
            "video 1/1 (frame 177/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 529.3ms\n",
            "video 1/1 (frame 178/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 395.6ms\n",
            "video 1/1 (frame 179/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 325.7ms\n",
            "video 1/1 (frame 180/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 339.6ms\n",
            "video 1/1 (frame 181/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 326.9ms\n",
            "video 1/1 (frame 182/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 348.5ms\n",
            "video 1/1 (frame 183/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 368.4ms\n",
            "video 1/1 (frame 184/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 333.0ms\n",
            "video 1/1 (frame 185/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 353.1ms\n",
            "video 1/1 (frame 186/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 338.9ms\n",
            "video 1/1 (frame 187/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 350.8ms\n",
            "video 1/1 (frame 188/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 329.5ms\n",
            "video 1/1 (frame 189/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 326.9ms\n",
            "video 1/1 (frame 190/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 328.0ms\n",
            "video 1/1 (frame 191/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 325.1ms\n",
            "video 1/1 (frame 192/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 350.8ms\n",
            "video 1/1 (frame 193/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 335.3ms\n",
            "video 1/1 (frame 194/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 345.5ms\n",
            "video 1/1 (frame 195/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 328.5ms\n",
            "video 1/1 (frame 196/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 331.3ms\n",
            "video 1/1 (frame 197/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 331.7ms\n",
            "video 1/1 (frame 198/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 331.3ms\n",
            "video 1/1 (frame 199/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 342.4ms\n",
            "video 1/1 (frame 200/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 328.1ms\n",
            "video 1/1 (frame 201/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 332.3ms\n",
            "video 1/1 (frame 202/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 511.8ms\n",
            "video 1/1 (frame 203/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 515.5ms\n",
            "video 1/1 (frame 204/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 515.3ms\n",
            "video 1/1 (frame 205/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 517.9ms\n",
            "video 1/1 (frame 206/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 635.0ms\n",
            "video 1/1 (frame 207/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 434.6ms\n",
            "video 1/1 (frame 208/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 358.8ms\n",
            "video 1/1 (frame 209/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 329.3ms\n",
            "video 1/1 (frame 210/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.8ms\n",
            "video 1/1 (frame 211/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 326.2ms\n",
            "video 1/1 (frame 212/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 349.8ms\n",
            "video 1/1 (frame 213/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 325.5ms\n",
            "video 1/1 (frame 214/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 350.4ms\n",
            "video 1/1 (frame 215/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 337.1ms\n",
            "video 1/1 (frame 216/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 346.1ms\n",
            "video 1/1 (frame 217/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 330.5ms\n",
            "video 1/1 (frame 218/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 335.7ms\n",
            "video 1/1 (frame 219/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 347.4ms\n",
            "video 1/1 (frame 220/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 331.6ms\n",
            "video 1/1 (frame 221/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 347.3ms\n",
            "video 1/1 (frame 222/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 326.0ms\n",
            "video 1/1 (frame 223/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 329.5ms\n",
            "video 1/1 (frame 224/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 343.9ms\n",
            "video 1/1 (frame 225/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 330.7ms\n",
            "video 1/1 (frame 226/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 346.9ms\n",
            "video 1/1 (frame 227/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 337.7ms\n",
            "video 1/1 (frame 228/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 345.8ms\n",
            "video 1/1 (frame 229/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 545.1ms\n",
            "video 1/1 (frame 230/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 517.8ms\n",
            "video 1/1 (frame 231/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 525.2ms\n",
            "video 1/1 (frame 232/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 579.9ms\n",
            "video 1/1 (frame 233/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 538.6ms\n",
            "video 1/1 (frame 234/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 530.3ms\n",
            "video 1/1 (frame 235/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 351.6ms\n",
            "video 1/1 (frame 236/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 369.3ms\n",
            "video 1/1 (frame 237/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 390.2ms\n",
            "video 1/1 (frame 238/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 333.6ms\n",
            "video 1/1 (frame 239/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 335.6ms\n",
            "video 1/1 (frame 240/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 349.8ms\n",
            "video 1/1 (frame 241/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 329.7ms\n",
            "video 1/1 (frame 242/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 330.9ms\n",
            "video 1/1 (frame 243/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 343.3ms\n",
            "video 1/1 (frame 244/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 335.3ms\n",
            "video 1/1 (frame 245/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 353.9ms\n",
            "video 1/1 (frame 246/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 338.4ms\n",
            "video 1/1 (frame 247/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 339.3ms\n",
            "video 1/1 (frame 248/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 344.0ms\n",
            "video 1/1 (frame 249/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 338.4ms\n",
            "video 1/1 (frame 250/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.3ms\n",
            "video 1/1 (frame 251/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.5ms\n",
            "video 1/1 (frame 252/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 396.6ms\n",
            "video 1/1 (frame 253/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 331.8ms\n",
            "video 1/1 (frame 254/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 361.3ms\n",
            "video 1/1 (frame 255/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 344.4ms\n",
            "video 1/1 (frame 256/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 410.3ms\n",
            "video 1/1 (frame 257/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 596.1ms\n",
            "video 1/1 (frame 258/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 539.1ms\n",
            "video 1/1 (frame 259/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 538.5ms\n",
            "video 1/1 (frame 260/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 567.3ms\n",
            "video 1/1 (frame 261/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 353.1ms\n",
            "video 1/1 (frame 262/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 348.2ms\n",
            "video 1/1 (frame 263/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 349.9ms\n",
            "video 1/1 (frame 264/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 371.1ms\n",
            "video 1/1 (frame 265/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 358.6ms\n",
            "video 1/1 (frame 266/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 347.3ms\n",
            "video 1/1 (frame 267/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 354.9ms\n",
            "video 1/1 (frame 268/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 348.0ms\n",
            "video 1/1 (frame 269/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 351.5ms\n",
            "video 1/1 (frame 270/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 337.2ms\n",
            "video 1/1 (frame 271/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.0ms\n",
            "video 1/1 (frame 272/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 340.1ms\n",
            "video 1/1 (frame 273/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 349.7ms\n",
            "video 1/1 (frame 274/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 372.9ms\n",
            "video 1/1 (frame 275/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 421.2ms\n",
            "video 1/1 (frame 276/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 342.5ms\n",
            "video 1/1 (frame 277/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 350.2ms\n",
            "video 1/1 (frame 278/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 336.3ms\n",
            "video 1/1 (frame 279/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 360.4ms\n",
            "video 1/1 (frame 280/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 605.2ms\n",
            "video 1/1 (frame 281/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 523.8ms\n",
            "video 1/1 (frame 282/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 534.8ms\n",
            "video 1/1 (frame 283/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 535.6ms\n",
            "video 1/1 (frame 284/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 438.0ms\n",
            "video 1/1 (frame 285/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 369.3ms\n",
            "video 1/1 (frame 286/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 343.2ms\n",
            "video 1/1 (frame 287/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 342.8ms\n",
            "video 1/1 (frame 288/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 361.0ms\n",
            "video 1/1 (frame 289/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 338.0ms\n",
            "video 1/1 (frame 290/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 366.7ms\n",
            "video 1/1 (frame 291/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 337.2ms\n",
            "video 1/1 (frame 292/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 362.5ms\n",
            "video 1/1 (frame 293/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 335.2ms\n",
            "video 1/1 (frame 294/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 348.9ms\n",
            "video 1/1 (frame 295/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 364.8ms\n",
            "video 1/1 (frame 296/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 353.1ms\n",
            "video 1/1 (frame 297/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 339.8ms\n",
            "video 1/1 (frame 298/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 392.3ms\n",
            "video 1/1 (frame 299/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 366.2ms\n",
            "video 1/1 (frame 300/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 357.9ms\n",
            "video 1/1 (frame 301/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 337.1ms\n",
            "video 1/1 (frame 302/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.8ms\n",
            "video 1/1 (frame 303/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 448.7ms\n",
            "video 1/1 (frame 304/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 533.9ms\n",
            "video 1/1 (frame 305/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 519.2ms\n",
            "video 1/1 (frame 306/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 516.5ms\n",
            "video 1/1 (frame 307/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 557.4ms\n",
            "video 1/1 (frame 308/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 502.2ms\n",
            "video 1/1 (frame 309/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 328.2ms\n",
            "video 1/1 (frame 310/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 334.0ms\n",
            "video 1/1 (frame 311/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 331.8ms\n",
            "video 1/1 (frame 312/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 331.8ms\n",
            "video 1/1 (frame 313/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 328.3ms\n",
            "video 1/1 (frame 314/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 326.5ms\n",
            "video 1/1 (frame 315/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.9ms\n",
            "video 1/1 (frame 316/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 327.2ms\n",
            "video 1/1 (frame 317/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 357.6ms\n",
            "video 1/1 (frame 318/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 330.6ms\n",
            "Speed: 4.7ms preprocess, 387.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Listo. Clips guardados en: /content/tesisV2/clips_personas\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "\n",
        "SOURCE_VIDEO = \"/content/tesisV2/videos/supermas1-1.mp4\"   # <-- cambia esto\n",
        "OUTPUT_DIR   = \"/content/tesisV2/clips_personas\"      # <-- cambia si querés\n",
        "MODEL_PATH   = \"yolov8s.pt\"                         # o 'yolov8n.pt' para más FPS\n",
        "CONF         = 0.25\n",
        "CLIP_SECS    = 3           # duración de cada clip\n",
        "IDLE_SECS    = 0.7         # si un ID no aparece por este tiempo, se cierra su writer\n",
        "TRACKER_CFG  = \"bytetrack.yaml\"  # liviano y sin re-id\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Abrimos para conocer FPS y frame size\n",
        "cap0 = cv2.VideoCapture(SOURCE_VIDEO)\n",
        "if not cap0.isOpened():\n",
        "    raise RuntimeError(f\"No pude abrir el video: {SOURCE_VIDEO}\")\n",
        "fps = cap0.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "width  = int(cap0.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap0.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "cap0.release()\n",
        "\n",
        "clip_len_frames = int(CLIP_SECS * fps)\n",
        "idle_frames     = int(IDLE_SECS * fps)\n",
        "\n",
        "# Estado por ID: writer actual, frames escritos en el segmento, índice de segmento, última vez visto\n",
        "state = {}\n",
        "# Para contar frames globales (necesario para medir idle)\n",
        "frame_idx = 0\n",
        "\n",
        "# Codec y helper para crear writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "\n",
        "def ensure_writer_for_id(person_id):\n",
        "    info = state.get(person_id)\n",
        "    if info is None:\n",
        "        person_dir = os.path.join(OUTPUT_DIR, f\"person_{person_id:04d}\")\n",
        "        os.makedirs(person_dir, exist_ok=True)\n",
        "        seg_idx = 0\n",
        "        out_path = os.path.join(\n",
        "            person_dir,\n",
        "            f\"id{person_id:04d}_seg{seg_idx:03d}.mp4\"\n",
        "        )\n",
        "        writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height))\n",
        "        state[person_id] = {\n",
        "            \"writer\": writer,\n",
        "            \"frames_in_seg\": 0,\n",
        "            \"seg_idx\": seg_idx,\n",
        "            \"last_seen\": frame_idx\n",
        "        }\n",
        "    return state[person_id]\n",
        "\n",
        "def rotate_segment(person_id):\n",
        "    info = state[person_id]\n",
        "    # Cerrar actual\n",
        "    if info[\"writer\"] is not None:\n",
        "        info[\"writer\"].release()\n",
        "    # Abrir nuevo\n",
        "    info[\"seg_idx\"] += 1\n",
        "    person_dir = os.path.join(OUTPUT_DIR, f\"person_{person_id:04d}\")\n",
        "    out_path = os.path.join(\n",
        "        person_dir,\n",
        "        f\"id{person_id:04d}_seg{info['seg_idx']:03d}.mp4\"\n",
        "    )\n",
        "    info[\"writer\"] = cv2.VideoWriter(out_path, fourcc, fps, (width, height))\n",
        "    info[\"frames_in_seg\"] = 0\n",
        "\n",
        "def close_and_remove(person_id):\n",
        "    info = state.get(person_id)\n",
        "    if not info:\n",
        "        return\n",
        "    if info[\"writer\"] is not None:\n",
        "        info[\"writer\"].release()\n",
        "    del state[person_id]\n",
        "\n",
        "# Cargar el modelo\n",
        "model = YOLO(MODEL_PATH)\n",
        "\n",
        "# Stream de tracking frame a frame\n",
        "for result in model.track(\n",
        "    source=SOURCE_VIDEO,\n",
        "    stream=True,\n",
        "    classes=[0],           # solo 'person'\n",
        "    conf=CONF,\n",
        "    tracker=TRACKER_CFG,\n",
        "    persist=True\n",
        "):\n",
        "    frame = result.orig_img  # BGR\n",
        "    ids = []\n",
        "    if result.boxes is not None and result.boxes.id is not None:\n",
        "        ids = result.boxes.id.int().tolist()\n",
        "\n",
        "    # Escribir frame por cada ID detectado\n",
        "    for pid in ids:\n",
        "        info = ensure_writer_for_id(pid)\n",
        "        info[\"writer\"].write(frame)\n",
        "        info[\"frames_in_seg\"] += 1\n",
        "        info[\"last_seen\"] = frame_idx\n",
        "\n",
        "        # rotar segmento cuando llegue a CLIP_SECS\n",
        "        if info[\"frames_in_seg\"] >= clip_len_frames:\n",
        "            rotate_segment(pid)\n",
        "\n",
        "    # Cerrar escritores de IDs que no aparecieron recientemente\n",
        "    to_close = []\n",
        "    for pid, info in list(state.items()):\n",
        "        if frame_idx - info[\"last_seen\"] > idle_frames:\n",
        "            to_close.append(pid)\n",
        "    for pid in to_close:\n",
        "        close_and_remove(pid)\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "# Flush final por si quedó algo abierto\n",
        "for pid in list(state.keys()):\n",
        "    close_and_remove(pid)\n",
        "\n",
        "print(f\"Listo. Clips guardados en: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSWhg5N3qyBl"
      },
      "source": [
        "## ***PRUEBA: CORRE YOLOv8 + TRACKER***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slvvHBELD1Jt",
        "outputId": "f592dfa4-7bf5-4630-98a1-dc2f19047437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.176 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8s summary (fused): 72 layers, 11,156,544 parameters, 0 gradients, 28.6 GFLOPs\n",
            "\n",
            "video 1/1 (frame 1/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 368.3ms\n",
            "video 1/1 (frame 2/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 347.5ms\n",
            "video 1/1 (frame 3/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 346.8ms\n",
            "video 1/1 (frame 4/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 443.9ms\n",
            "video 1/1 (frame 5/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 594.2ms\n",
            "video 1/1 (frame 6/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 556.7ms\n",
            "video 1/1 (frame 7/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 530.6ms\n",
            "video 1/1 (frame 8/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 543.5ms\n",
            "video 1/1 (frame 9/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 549.6ms\n",
            "video 1/1 (frame 10/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 335.9ms\n",
            "video 1/1 (frame 11/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 337.6ms\n",
            "video 1/1 (frame 12/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 342.9ms\n",
            "video 1/1 (frame 13/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 364.6ms\n",
            "video 1/1 (frame 14/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 343.7ms\n",
            "video 1/1 (frame 15/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 352.2ms\n",
            "video 1/1 (frame 16/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 332.4ms\n",
            "video 1/1 (frame 17/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 353.6ms\n",
            "video 1/1 (frame 18/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 331.0ms\n",
            "video 1/1 (frame 19/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 333.2ms\n",
            "video 1/1 (frame 20/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 370.6ms\n",
            "video 1/1 (frame 21/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 361.9ms\n",
            "video 1/1 (frame 22/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 376.6ms\n",
            "video 1/1 (frame 23/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 356.4ms\n",
            "video 1/1 (frame 24/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 338.1ms\n",
            "video 1/1 (frame 25/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 337.5ms\n",
            "video 1/1 (frame 26/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 362.5ms\n",
            "video 1/1 (frame 27/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 334.0ms\n",
            "video 1/1 (frame 28/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 351.1ms\n",
            "video 1/1 (frame 29/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 342.0ms\n",
            "video 1/1 (frame 30/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 413.7ms\n",
            "video 1/1 (frame 31/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 546.6ms\n",
            "video 1/1 (frame 32/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 518.6ms\n",
            "video 1/1 (frame 33/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 562.7ms\n",
            "video 1/1 (frame 34/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 520.3ms\n",
            "video 1/1 (frame 35/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 546.6ms\n",
            "video 1/1 (frame 36/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 357.1ms\n",
            "video 1/1 (frame 37/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 333.6ms\n",
            "video 1/1 (frame 38/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.7ms\n",
            "video 1/1 (frame 39/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 339.6ms\n",
            "video 1/1 (frame 40/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 328.2ms\n",
            "video 1/1 (frame 41/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 359.5ms\n",
            "video 1/1 (frame 42/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 348.9ms\n",
            "video 1/1 (frame 43/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 352.9ms\n",
            "video 1/1 (frame 44/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 335.5ms\n",
            "video 1/1 (frame 45/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 397.0ms\n",
            "video 1/1 (frame 46/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 332.3ms\n",
            "video 1/1 (frame 47/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 349.2ms\n",
            "video 1/1 (frame 48/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 336.6ms\n",
            "video 1/1 (frame 49/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 337.0ms\n",
            "video 1/1 (frame 50/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 332.6ms\n",
            "video 1/1 (frame 51/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 336.9ms\n",
            "video 1/1 (frame 52/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 356.8ms\n",
            "video 1/1 (frame 53/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 338.9ms\n",
            "video 1/1 (frame 54/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 361.9ms\n",
            "video 1/1 (frame 55/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 335.9ms\n",
            "video 1/1 (frame 56/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 360.0ms\n",
            "video 1/1 (frame 57/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 513.8ms\n",
            "video 1/1 (frame 58/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 523.4ms\n",
            "video 1/1 (frame 59/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 539.5ms\n",
            "video 1/1 (frame 60/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 511.8ms\n",
            "video 1/1 (frame 61/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 575.9ms\n",
            "video 1/1 (frame 62/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 587.8ms\n",
            "video 1/1 (frame 63/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 341.7ms\n",
            "video 1/1 (frame 64/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 346.8ms\n",
            "video 1/1 (frame 65/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 453.0ms\n",
            "video 1/1 (frame 66/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 694.4ms\n",
            "video 1/1 (frame 67/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 1582.0ms\n",
            "video 1/1 (frame 68/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 1329.9ms\n",
            "video 1/1 (frame 69/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 519.6ms\n",
            "video 1/1 (frame 70/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 359.4ms\n",
            "video 1/1 (frame 71/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 361.5ms\n",
            "video 1/1 (frame 72/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 351.9ms\n",
            "video 1/1 (frame 73/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 390.0ms\n",
            "video 1/1 (frame 74/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 369.8ms\n",
            "video 1/1 (frame 75/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 417.4ms\n",
            "video 1/1 (frame 76/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 549.4ms\n",
            "video 1/1 (frame 77/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 544.6ms\n",
            "video 1/1 (frame 78/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 528.0ms\n",
            "video 1/1 (frame 79/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 529.8ms\n",
            "video 1/1 (frame 80/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 547.3ms\n",
            "video 1/1 (frame 81/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 423.5ms\n",
            "video 1/1 (frame 82/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 352.4ms\n",
            "video 1/1 (frame 83/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 331.9ms\n",
            "video 1/1 (frame 84/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 331.0ms\n",
            "video 1/1 (frame 85/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 334.9ms\n",
            "video 1/1 (frame 86/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 329.4ms\n",
            "video 1/1 (frame 87/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 341.5ms\n",
            "video 1/1 (frame 88/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 331.2ms\n",
            "video 1/1 (frame 89/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 344.3ms\n",
            "video 1/1 (frame 90/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 332.6ms\n",
            "video 1/1 (frame 91/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 417.8ms\n",
            "video 1/1 (frame 92/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 332.7ms\n",
            "video 1/1 (frame 93/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 350.6ms\n",
            "video 1/1 (frame 94/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 334.2ms\n",
            "video 1/1 (frame 95/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 331.3ms\n",
            "video 1/1 (frame 96/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 339.2ms\n",
            "video 1/1 (frame 97/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 334.5ms\n",
            "video 1/1 (frame 98/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 345.6ms\n",
            "video 1/1 (frame 99/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 334.0ms\n",
            "video 1/1 (frame 100/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 357.3ms\n",
            "video 1/1 (frame 101/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 334.2ms\n",
            "video 1/1 (frame 102/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 462.7ms\n",
            "video 1/1 (frame 103/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 519.5ms\n",
            "video 1/1 (frame 104/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 533.6ms\n",
            "video 1/1 (frame 105/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 551.7ms\n",
            "video 1/1 (frame 106/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 515.4ms\n",
            "video 1/1 (frame 107/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 538.8ms\n",
            "video 1/1 (frame 108/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 376.3ms\n",
            "video 1/1 (frame 109/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 344.5ms\n",
            "video 1/1 (frame 110/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 8 persons, 369.6ms\n",
            "video 1/1 (frame 111/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 337.4ms\n",
            "video 1/1 (frame 112/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 340.0ms\n",
            "video 1/1 (frame 113/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 339.6ms\n",
            "video 1/1 (frame 114/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 381.1ms\n",
            "video 1/1 (frame 115/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 350.7ms\n",
            "video 1/1 (frame 116/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 336.0ms\n",
            "video 1/1 (frame 117/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 349.9ms\n",
            "video 1/1 (frame 118/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 347.4ms\n",
            "video 1/1 (frame 119/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 349.7ms\n",
            "video 1/1 (frame 120/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 335.4ms\n",
            "video 1/1 (frame 121/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 353.7ms\n",
            "video 1/1 (frame 122/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 336.5ms\n",
            "video 1/1 (frame 123/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 335.7ms\n",
            "video 1/1 (frame 124/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 331.8ms\n",
            "video 1/1 (frame 125/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 8 persons, 337.5ms\n",
            "video 1/1 (frame 126/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 362.7ms\n",
            "video 1/1 (frame 127/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 339.8ms\n",
            "video 1/1 (frame 128/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 350.4ms\n",
            "video 1/1 (frame 129/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 521.5ms\n",
            "video 1/1 (frame 130/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 550.2ms\n",
            "video 1/1 (frame 131/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 536.3ms\n",
            "video 1/1 (frame 132/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 8 persons, 515.2ms\n",
            "video 1/1 (frame 133/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 563.0ms\n",
            "video 1/1 (frame 134/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 491.8ms\n",
            "video 1/1 (frame 135/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 8 persons, 327.1ms\n",
            "video 1/1 (frame 136/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 342.7ms\n",
            "video 1/1 (frame 137/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 8 persons, 366.4ms\n",
            "video 1/1 (frame 138/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 328.7ms\n",
            "video 1/1 (frame 139/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 323.6ms\n",
            "video 1/1 (frame 140/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 326.0ms\n",
            "video 1/1 (frame 141/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 347.3ms\n",
            "video 1/1 (frame 142/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 326.9ms\n",
            "video 1/1 (frame 143/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 347.4ms\n",
            "video 1/1 (frame 144/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 326.3ms\n",
            "video 1/1 (frame 145/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 344.5ms\n",
            "video 1/1 (frame 146/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 324.1ms\n",
            "video 1/1 (frame 147/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 334.3ms\n",
            "video 1/1 (frame 148/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 320.7ms\n",
            "video 1/1 (frame 149/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 349.6ms\n",
            "video 1/1 (frame 150/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 364.8ms\n",
            "video 1/1 (frame 151/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 333.3ms\n",
            "video 1/1 (frame 152/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 344.8ms\n",
            "video 1/1 (frame 153/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 327.6ms\n",
            "video 1/1 (frame 154/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 349.6ms\n",
            "video 1/1 (frame 155/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 330.0ms\n",
            "video 1/1 (frame 156/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 524.1ms\n",
            "video 1/1 (frame 157/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 514.1ms\n",
            "video 1/1 (frame 158/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 553.1ms\n",
            "video 1/1 (frame 159/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 519.4ms\n",
            "video 1/1 (frame 160/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 652.3ms\n",
            "video 1/1 (frame 161/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 506.8ms\n",
            "video 1/1 (frame 162/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 328.3ms\n",
            "video 1/1 (frame 163/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 333.0ms\n",
            "video 1/1 (frame 164/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 323.5ms\n",
            "video 1/1 (frame 165/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 346.5ms\n",
            "video 1/1 (frame 166/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 325.9ms\n",
            "video 1/1 (frame 167/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 344.3ms\n",
            "video 1/1 (frame 168/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 347.7ms\n",
            "video 1/1 (frame 169/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 345.3ms\n",
            "video 1/1 (frame 170/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 331.5ms\n",
            "video 1/1 (frame 171/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 326.1ms\n",
            "video 1/1 (frame 172/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 324.6ms\n",
            "video 1/1 (frame 173/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 331.3ms\n",
            "video 1/1 (frame 174/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 345.6ms\n",
            "video 1/1 (frame 175/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 334.4ms\n",
            "video 1/1 (frame 176/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 347.9ms\n",
            "video 1/1 (frame 177/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 323.8ms\n",
            "video 1/1 (frame 178/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 351.3ms\n",
            "video 1/1 (frame 179/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 330.2ms\n",
            "video 1/1 (frame 180/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.2ms\n",
            "video 1/1 (frame 181/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 325.9ms\n",
            "video 1/1 (frame 182/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.6ms\n",
            "video 1/1 (frame 183/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 601.9ms\n",
            "video 1/1 (frame 184/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 531.5ms\n",
            "video 1/1 (frame 185/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 528.3ms\n",
            "video 1/1 (frame 186/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 557.1ms\n",
            "video 1/1 (frame 187/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 580.6ms\n",
            "video 1/1 (frame 188/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 590.1ms\n",
            "video 1/1 (frame 189/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 606.5ms\n",
            "video 1/1 (frame 190/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 550.4ms\n",
            "video 1/1 (frame 191/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 519.4ms\n",
            "video 1/1 (frame 192/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 531.7ms\n",
            "video 1/1 (frame 193/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 460.5ms\n",
            "video 1/1 (frame 194/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 331.5ms\n",
            "video 1/1 (frame 195/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 342.5ms\n",
            "video 1/1 (frame 196/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 330.5ms\n",
            "video 1/1 (frame 197/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.1ms\n",
            "video 1/1 (frame 198/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 330.8ms\n",
            "video 1/1 (frame 199/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 324.4ms\n",
            "video 1/1 (frame 200/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 344.0ms\n",
            "video 1/1 (frame 201/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.5ms\n",
            "video 1/1 (frame 202/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 345.4ms\n",
            "video 1/1 (frame 203/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.4ms\n",
            "video 1/1 (frame 204/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 331.8ms\n",
            "video 1/1 (frame 205/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 336.6ms\n",
            "video 1/1 (frame 206/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 375.9ms\n",
            "video 1/1 (frame 207/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 343.8ms\n",
            "video 1/1 (frame 208/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 334.5ms\n",
            "video 1/1 (frame 209/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 346.7ms\n",
            "video 1/1 (frame 210/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 513.0ms\n",
            "video 1/1 (frame 211/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 513.9ms\n",
            "video 1/1 (frame 212/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 539.3ms\n",
            "video 1/1 (frame 213/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 516.6ms\n",
            "video 1/1 (frame 214/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 558.6ms\n",
            "video 1/1 (frame 215/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 410.2ms\n",
            "video 1/1 (frame 216/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 328.7ms\n",
            "video 1/1 (frame 217/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 326.4ms\n",
            "video 1/1 (frame 218/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 342.0ms\n",
            "video 1/1 (frame 219/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.9ms\n",
            "video 1/1 (frame 220/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 351.7ms\n",
            "video 1/1 (frame 221/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 323.4ms\n",
            "video 1/1 (frame 222/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 344.7ms\n",
            "video 1/1 (frame 223/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 325.0ms\n",
            "video 1/1 (frame 224/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 326.8ms\n",
            "video 1/1 (frame 225/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.0ms\n",
            "video 1/1 (frame 226/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 334.3ms\n",
            "video 1/1 (frame 227/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 347.0ms\n",
            "video 1/1 (frame 228/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 325.6ms\n",
            "video 1/1 (frame 229/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 389.5ms\n",
            "video 1/1 (frame 230/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 335.7ms\n",
            "video 1/1 (frame 231/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 347.7ms\n",
            "video 1/1 (frame 232/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.7ms\n",
            "video 1/1 (frame 233/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 340.1ms\n",
            "video 1/1 (frame 234/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 324.3ms\n",
            "video 1/1 (frame 235/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 333.1ms\n",
            "video 1/1 (frame 236/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 339.8ms\n",
            "video 1/1 (frame 237/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 521.5ms\n",
            "video 1/1 (frame 238/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 536.8ms\n",
            "video 1/1 (frame 239/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 524.3ms\n",
            "video 1/1 (frame 240/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 510.1ms\n",
            "video 1/1 (frame 241/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 521.0ms\n",
            "video 1/1 (frame 242/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 453.9ms\n",
            "video 1/1 (frame 243/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 339.5ms\n",
            "video 1/1 (frame 244/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 353.4ms\n",
            "video 1/1 (frame 245/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 340.3ms\n",
            "video 1/1 (frame 246/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 326.2ms\n",
            "video 1/1 (frame 247/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 332.0ms\n",
            "video 1/1 (frame 248/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 324.2ms\n",
            "video 1/1 (frame 249/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 340.5ms\n",
            "video 1/1 (frame 250/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 324.8ms\n",
            "video 1/1 (frame 251/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 342.2ms\n",
            "video 1/1 (frame 252/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 370.6ms\n",
            "video 1/1 (frame 253/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 350.8ms\n",
            "video 1/1 (frame 254/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 339.1ms\n",
            "video 1/1 (frame 255/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.6ms\n",
            "video 1/1 (frame 256/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.3ms\n",
            "video 1/1 (frame 257/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.4ms\n",
            "video 1/1 (frame 258/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 347.5ms\n",
            "video 1/1 (frame 259/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 325.0ms\n",
            "video 1/1 (frame 260/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 350.6ms\n",
            "video 1/1 (frame 261/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 322.4ms\n",
            "video 1/1 (frame 262/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 346.1ms\n",
            "video 1/1 (frame 263/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 324.6ms\n",
            "video 1/1 (frame 264/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 578.9ms\n",
            "video 1/1 (frame 265/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 522.6ms\n",
            "video 1/1 (frame 266/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 544.7ms\n",
            "video 1/1 (frame 267/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 538.5ms\n",
            "video 1/1 (frame 268/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 534.1ms\n",
            "video 1/1 (frame 269/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 430.8ms\n",
            "video 1/1 (frame 270/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 340.1ms\n",
            "video 1/1 (frame 271/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 343.2ms\n",
            "video 1/1 (frame 272/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 335.7ms\n",
            "video 1/1 (frame 273/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 349.4ms\n",
            "video 1/1 (frame 274/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 332.5ms\n",
            "video 1/1 (frame 275/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 366.7ms\n",
            "video 1/1 (frame 276/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.8ms\n",
            "video 1/1 (frame 277/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 326.1ms\n",
            "video 1/1 (frame 278/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 345.2ms\n",
            "video 1/1 (frame 279/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 326.3ms\n",
            "video 1/1 (frame 280/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 340.1ms\n",
            "Speed: 4.9ms preprocess, 399.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1m/content/tesisV2/outputs/boxed2\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/track\n"
          ]
        }
      ],
      "source": [
        "!yolo track model=yolov8s.pt source=\"/content/tesisV2/videos/supermas1_2.mp4\" classes=0 conf=0.25 save=True project=\"/content/tesisV2/outputs\" name=\"boxed\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyahByY7v3mB"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers==4.43.3 timm==1.0.7 pytorchvideo==0.1.5 accelerate evaluate decord\n",
        "import os, pandas as pd, torch, decord\n",
        "from datasets import Dataset\n",
        "from transformers import AutoImageProcessor, VideoMAEForVideoClassification, TrainingArguments, Trainer\n",
        "decord.bridge.set_bridge('torch')\n",
        "\n",
        "BASE = \"/content/cortos/video_clips_224\"  # el mismo OUT_DIR\n",
        "label2id={\"normal\":0,\"shoplifting\":1}; id2label={v:k for k,v in label2id.items()}\n",
        "\n",
        "def load_csv(name):\n",
        "    df=pd.read_csv(os.path.join(BASE,f\"{name}.csv\"),header=None,names=[\"path\",\"label\"])\n",
        "    return Dataset.from_pandas(df.assign(label=df[\"label\"].map(label2id)))\n",
        "\n",
        "def load_video_torch(path,num_frames=16):\n",
        "    vr=decord.VideoReader(path)\n",
        "    import torch\n",
        "    idx=torch.linspace(0,len(vr)-1,steps=num_frames).long()\n",
        "    return vr.get_batch(idx).permute(0,3,1,2)\n",
        "\n",
        "train_ds, val_ds = load_csv(\"train\"), load_csv(\"val\")\n",
        "processor=AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "model=VideoMAEForVideoClassification.from_pretrained(\n",
        "    \"MCG-NJU/videomae-base\", num_labels=2, label2id=label2id, id2label=id2label\n",
        ")\n",
        "\n",
        "def preprocess(ex):\n",
        "    frames=load_video_torch(ex[\"path\"],num_frames=16)\n",
        "    ex[\"pixel_values\"]=processor(list(frames),return_tensors=\"pt\")[\"pixel_values\"][0]\n",
        "    return ex\n",
        "\n",
        "train_ds=train_ds.map(preprocess); val_ds=val_ds.map(preprocess)\n",
        "def collate(b):\n",
        "    return {\"pixel_values\":torch.stack([x[\"pixel_values\"] for x in b]),\n",
        "            \"labels\":torch.tensor([x[\"label\"] for x in b])}\n",
        "\n",
        "args=TrainingArguments(output_dir=\"/content/videomae_shoplift\",\n",
        "    per_device_train_batch_size=4, per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2, fp16=True, learning_rate=5e-5,\n",
        "    num_train_epochs=5, evaluation_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "    logging_steps=20, report_to=\"none\")\n",
        "\n",
        "trainer=Trainer(model=model,args=args,train_dataset=train_ds,eval_dataset=val_ds,data_collator=collate)\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jfOQgmS6szQ"
      },
      "source": [
        "## ***CREACION DE CLIPS NORMALES (SOLO CON PERSONAS)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbTHnl6jFRkd",
        "outputId": "b5e9bdb8-fc68-446c-bf82-c38abed0678f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: CPU\n",
            "POS existentes: 9834\n",
            "Normales a procesar (videos): 150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Segmentando NEG: 100%|██████████| 150/150 [4:06:24<00:00, 98.57s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NEG brutos nuevos: 2989  | TOTAL NEG brutos: 3681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filtrando NEG (person): 100%|██████████| 3681/3681 [1:01:24<00:00,  1.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NEG con persona: 2391\n",
            "Balance final -> POS: 9834  NEG: 2391\n",
            "train -> 10668\n",
            "val -> 787\n",
            "test -> 770\n",
            "CSV y clips listos en: /content/tesisV2/cortos/video_clips_192\n",
            "Espejo en Drive: /content/drive/MyDrive/tesisV2/cortos/video_clips_192\n"
          ]
        }
      ],
      "source": [
        "# ================= SOLO NEGATIVOS (masivos) + CACHE + ESPEJO A DRIVE =================\n",
        "import os, glob, random, subprocess, cv2, collections, numpy as np, json, time, torch\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ---------- PARÁMETROS RÁPIDOS ----------\n",
        "SRC_DIR = \"/content/tesisV2/Datasets\"                      # normal en: Training_/Testing_*\n",
        "OUT_DIR = \"/content/tesisV2/cortos/video_clips_192\"        # donde ya están tus POS\n",
        "DRIVE_MIRROR = \"/content/drive/MyDrive/tesisV2/cortos/video_clips_192\"  # espejo\n",
        "N_NEG_VIDEOS = 150     # ← subí esto para más negativos (antes 50/100/120). Poné 150–300 si querés muchos.\n",
        "HOP_NEG = 8.0          # ← bajá a 6.0 si querés aún más clips por video\n",
        "SIZE, FPS = 192, 12\n",
        "FFMPEG_PRESET = \"veryfast\"\n",
        "\n",
        "PERSON_CONF = 0.25\n",
        "NEG_PERSON_THR = 0.40\n",
        "MAX_FRAMES = 60\n",
        "VID_STRIDE = 3\n",
        "IMGSZ = 384\n",
        "DEVICE = 0 if (torch.cuda.is_available() and torch.cuda.device_count()>0) else \"cpu\"\n",
        "SYNC_EVERY = 10        # copia a Drive cada X videos segmentados\n",
        "\n",
        "print(\"Device:\", \"GPU\" if DEVICE==0 else \"CPU\")\n",
        "\n",
        "# ---------- RUTAS ----------\n",
        "POS_DIR = os.path.join(OUT_DIR, \"shoplifting\")\n",
        "NEG_DIR = os.path.join(OUT_DIR, \"normal\")\n",
        "CACHE_DIR = os.path.join(OUT_DIR, \"_cache_pr\")\n",
        "os.makedirs(NEG_DIR, exist_ok=True)\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_MIRROR, exist_ok=True)\n",
        "\n",
        "# ---------- FUNCIONES ----------\n",
        "def list_videos(folder):\n",
        "    vids=[]; exts=(\"*.mp4\",\"*.avi\",\"*.mov\",\"*.mkv\",\"*.MP4\",\"*.AVI\",\"*.MOV\",\"*.MKV\")\n",
        "    for e in exts: vids += glob.glob(os.path.join(folder, \"**\", e), recursive=True)\n",
        "    return vids\n",
        "\n",
        "def segment_to_clips_resume(src_path, out_dir, clip_s=3, hop_s=8.0, size=192, fps=12, preset=\"veryfast\"):\n",
        "    cap=cv2.VideoCapture(src_path)\n",
        "    if not cap.isOpened(): return []\n",
        "    vfps=cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "    frames=int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    cap.release()\n",
        "    if frames<=0: return []\n",
        "    dur=frames/vfps\n",
        "    starts=[]; t=0.0\n",
        "    while t+clip_s<=dur: starts.append(round(t,3)); t+=hop_s\n",
        "    base=os.path.splitext(os.path.basename(src_path))[0]\n",
        "    outs=[]\n",
        "    for ss in starts:\n",
        "        outp=os.path.join(out_dir, f\"{base}_ss{int(ss*1000):07d}.mp4\")\n",
        "        if os.path.exists(outp):\n",
        "            outs.append(outp); continue\n",
        "        cmd=[\"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\n",
        "             \"-ss\", f\"{ss:.3f}\", \"-t\", str(clip_s), \"-i\", src_path,\n",
        "             \"-vf\", f\"scale={size}:{size}:force_original_aspect_ratio=decrease,pad={size}:{size}:(ow-iw)/2:(oh-ih)/2,format=yuv420p,fps={fps}\",\n",
        "             \"-an\",\"-r\", str(fps), \"-preset\", preset, outp]\n",
        "        subprocess.run(cmd)\n",
        "        if os.path.exists(outp): outs.append(outp)\n",
        "    return outs\n",
        "\n",
        "def _pr_path(p):\n",
        "    return os.path.join(CACHE_DIR, os.path.basename(p)+\".pr.json\")\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "def person_ratio_cached(path, conf=PERSON_CONF, max_frames=MAX_FRAMES, vid_stride=VID_STRIDE, imgsz=IMGSZ):\n",
        "    c=_pr_path(path)\n",
        "    if os.path.exists(c):\n",
        "        try: return json.load(open(c))[\"pr\"]\n",
        "        except: pass\n",
        "    det=tot=0\n",
        "    gen = model.predict(source=path, classes=[0], conf=conf, stream=True, verbose=False,\n",
        "                        vid_stride=vid_stride, imgsz=imgsz, device=DEVICE)\n",
        "    for r in gen:\n",
        "        tot += 1\n",
        "        if r.boxes is not None and len(r.boxes)>0:\n",
        "            det += 1\n",
        "        if tot >= max_frames:\n",
        "            break\n",
        "    pr = (det/tot) if tot else 0.0\n",
        "    try: json.dump({\"pr\":pr}, open(c,\"w\"))\n",
        "    except: pass\n",
        "    return pr\n",
        "\n",
        "def rsync_to_drive():\n",
        "    # copia incremental, no pisa lo existente\n",
        "    os.system(f'rsync -a --ignore-existing \"{OUT_DIR}/\" \"{DRIVE_MIRROR}/\"')\n",
        "\n",
        "# ---------- POS EXISTENTES ----------\n",
        "pos_clips = sorted(glob.glob(os.path.join(POS_DIR, \"*.mp4\")))\n",
        "print(\"POS existentes:\", len(pos_clips))\n",
        "\n",
        "# ---------- ELEGIR NORMALES ----------\n",
        "train_norm = list_videos(os.path.join(SRC_DIR, \"Training_Normal_Videos_Anomaly\"))\n",
        "test_norm  = list_videos(os.path.join(SRC_DIR, \"Testing_Normal_Videos_Anomaly\"))\n",
        "normal_pool = train_norm + test_norm\n",
        "random.shuffle(normal_pool)\n",
        "normal_pool = normal_pool[:N_NEG_VIDEOS]\n",
        "print(\"Normales a procesar (videos):\", len(normal_pool))\n",
        "\n",
        "# ---------- SEGMENTAR + SYNC POR LOTES ----------\n",
        "neg_raw_before = len(glob.glob(os.path.join(NEG_DIR, \"*.mp4\")))\n",
        "processed = 0\n",
        "for v in tqdm(normal_pool, desc=\"Segmentando NEG\"):\n",
        "    _ = segment_to_clips_resume(v, NEG_DIR, 3, HOP_NEG, SIZE, FPS, FFMPEG_PRESET)\n",
        "    processed += 1\n",
        "    if processed % SYNC_EVERY == 0:\n",
        "        rsync_to_drive()\n",
        "neg_raw_after = len(glob.glob(os.path.join(NEG_DIR, \"*.mp4\")))\n",
        "print(\"NEG brutos nuevos:\", neg_raw_after - neg_raw_before, \" | TOTAL NEG brutos:\", neg_raw_after)\n",
        "\n",
        "# ---------- FILTRAR PERSONA (usa cache / reanuda) ----------\n",
        "neg_all = sorted(glob.glob(os.path.join(NEG_DIR, \"*.mp4\")))\n",
        "neg_kept = []\n",
        "for p in tqdm(neg_all, desc=\"Filtrando NEG (person)\"):\n",
        "    try:\n",
        "        if person_ratio_cached(p) >= NEG_PERSON_THR:\n",
        "            neg_kept.append(p)\n",
        "        else:\n",
        "            os.remove(p)\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"NEG con persona:\", len(neg_kept))\n",
        "\n",
        "# ---------- BALANCE FINAL vs POS (1.5x) ----------\n",
        "target_neg = int(len(pos_clips) * 1.5)\n",
        "random.shuffle(neg_kept)\n",
        "if len(neg_kept) > target_neg:\n",
        "    for p in neg_kept[target_neg:]:\n",
        "        try: os.remove(p)\n",
        "        except: pass\n",
        "    neg_kept = neg_kept[:target_neg]\n",
        "print(f\"Balance final -> POS: {len(pos_clips)}  NEG: {len(neg_kept)}\")\n",
        "\n",
        "# ---------- CSVs (split por video, sin fuga) ----------\n",
        "def base_from_clip(p): return os.path.basename(p).split(\"_ss\")[0]\n",
        "def group_by_video(clips):\n",
        "    d=collections.defaultdict(list)\n",
        "    for p in clips: d[base_from_clip(p)].append(p)\n",
        "    return list(d.values())\n",
        "def split_groups(groups, r=(0.8,0.1,0.1)):\n",
        "    n=len(groups); a=int(r[0]*n); b=int((r[0]+r[1])*n)\n",
        "    return groups[:a], groups[a:b], groups[b:]\n",
        "def flatten(gs):\n",
        "    out=[]; [out.extend(g) for g in gs]; return out\n",
        "\n",
        "pos_groups = group_by_video(pos_clips)\n",
        "neg_groups = group_by_video(neg_kept)\n",
        "random.shuffle(pos_groups); random.shuffle(neg_groups)\n",
        "p_tr,p_va,p_te = split_groups(pos_groups)\n",
        "n_tr,n_va,n_te = split_groups(neg_groups)\n",
        "\n",
        "splits = {\n",
        "  \"train\": [(p,\"shoplifting\") for p in flatten(p_tr)] + [(p,\"normal\") for p in flatten(n_tr)],\n",
        "  \"val\":   [(p,\"shoplifting\") for p in flatten(p_va)] + [(p,\"normal\") for p in flatten(n_va)],\n",
        "  \"test\":  [(p,\"shoplifting\") for p in flatten(p_te)] + [(p,\"normal\") for p in flatten(n_te)],\n",
        "}\n",
        "for name,items in splits.items():\n",
        "    random.shuffle(items)\n",
        "    with open(os.path.join(OUT_DIR,f\"{name}.csv\"),\"w\") as f:\n",
        "        for p,c in items: f.write(f\"{p},{c}\\n\")\n",
        "    print(name, \"->\", len(items))\n",
        "\n",
        "# sync final\n",
        "rsync_to_drive()\n",
        "print(\"CSV y clips listos en:\", OUT_DIR)\n",
        "print(\"Espejo en Drive:\", DRIVE_MIRROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb9LYZ4072Hz"
      },
      "source": [
        "## ***PRUEBA: JSON CON LA PROBABILIDAD DE HURTO. MP4 CON BARRAS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQoaHYfPhL13",
        "outputId": "5468e48e-79f6-4d1e-866f-945c5e24659d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === INFERENCIA en video largo (usa checkpoint linear probe) ===\n",
        "import os, json, cv2, math, torch, numpy as np\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "\n",
        "# RUTAS\n",
        "MODEL_PATH = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\"  # <- el que guardó el training\n",
        "VIDEO_IN   = \"/content/tesisV2/videos/supermas1-1.mp4\"                                 # <-- cambia a tu video\n",
        "OUT_JSON   = \"/content/tesisV2/preds_supermas1-1.json\"\n",
        "OUT_VIS    = \"/content/tesisV2/preds_supermas1-1_vis.mp4\"\n",
        "\n",
        "# Debe matchear lo que usaste en training rápido\n",
        "NUM_FRAMES = 8\n",
        "IMG_SIZE   = 96\n",
        "\n",
        "# Ventana y hop (en segundos)\n",
        "WIN_SEC = 3.0\n",
        "HOP_SEC = 1.0\n",
        "THRESH  = 0.55  # umbral para marcar \"alarma\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Modelo igual que en training (congelado, misma capa final)\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "def clip_to_tensor(frames_rgb):\n",
        "    x = np.stack(frames_rgb, axis=0).astype(np.float32)/255.0  # T,H,W,C\n",
        "    x = torch.from_numpy(x).permute(3,0,1,2).to(device)        # C,T,H,W\n",
        "    x = (x - mean) / std\n",
        "    return x.unsqueeze(0)  # B=1\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_IN)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "print(f\"Video: {n} frames, {fps:.1f} fps\")\n",
        "\n",
        "win = int(WIN_SEC*fps); hop = int(HOP_SEC*fps)\n",
        "scores = []  # (t0,t1,p_shop)\n",
        "\n",
        "for start in range(0, max(1, n - win + 1), hop):\n",
        "    idxs = np.linspace(start, start+win-1, num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    x = clip_to_tensor(frames)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)[0]\n",
        "        p_shop = torch.softmax(logits, dim=0)[1].item()\n",
        "    t0 = start/fps; t1 = (start+win)/fps\n",
        "    scores.append((t0, t1, p_shop))\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Suavizado (media móvil simple para reducir ruido)\n",
        "win_smooth = 3\n",
        "probs = np.array([p for _,_,p in scores], dtype=np.float32)\n",
        "if len(probs) >= win_smooth:\n",
        "    kernel = np.ones(win_smooth)/win_smooth\n",
        "    probs_s = np.convolve(probs, kernel, mode='same')\n",
        "else:\n",
        "    probs_s = probs\n",
        "\n",
        "# Guardar JSON\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump([{\"t0\":float(a),\"t1\":float(b),\"p\":float(p),\"p_smooth\":float(ps)}\n",
        "               for (a,b,_), ps in zip(scores, probs_s)], f, indent=2)\n",
        "print(\"Guardado:\", OUT_JSON)\n",
        "\n",
        "# Render con barra de probabilidad\n",
        "cap = cv2.VideoCapture(VIDEO_IN)\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(OUT_VIS, fourcc, fps, (w,h))\n",
        "bar_h = 18\n",
        "\n",
        "# Expande probs_smooth por frame para pintar\n",
        "score_by_frame = np.zeros(n, dtype=np.float32)\n",
        "for i, (t0,t1,_) in enumerate(scores):\n",
        "    a = int(t0*fps); b = min(n-1, int(t1*fps))\n",
        "    score_by_frame[a:b+1] = max(0.0, min(1.0, probs_s[i]))\n",
        "\n",
        "i=0\n",
        "while True:\n",
        "    ok, fr = cap.read()\n",
        "    if not ok: break\n",
        "    p = float(score_by_frame[i]) if i < len(score_by_frame) else 0.0\n",
        "    bw = int(p * w)\n",
        "    color = (0,0,255) if p >= THRESH else (0,255,0)\n",
        "    cv2.rectangle(fr, (0,0), (bw, bar_h), color, -1)\n",
        "    cv2.putText(fr, f\"shoplifting prob: {p:.2f}\", (10, bar_h+22),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
        "    out.write(fr); i+=1\n",
        "\n",
        "cap.release(); out.release()\n",
        "print(\"Video con barra:\", OUT_VIS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7CcPF0JmQLtr",
        "outputId": "d07ddb6f-2553-4237-a0da-fbc7eb6dcb39"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'R3D_18_Weights'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2401321534.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr3d_18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr3d_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR3D_18_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKINETICS400_V1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# dummy para que no binde? (ignorado)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr3d_18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr3d_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR3D_18_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKINETICS400_V1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'R3D_18_Weights'"
          ]
        }
      ],
      "source": [
        "# ===== Elegir umbral óptimo en VAL =====\n",
        "import os, torch, numpy as np\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "\n",
        "OUT_DIR  = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "VAL_CSV  = os.path.join(OUT_DIR, \"val.csv\")\n",
        "MODEL_P  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\"\n",
        "\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "NUM_FRAMES, IMG_SIZE = 8, 96\n",
        "\n",
        "def read_csv(p):\n",
        "    items=[];\n",
        "    for line in open(p):\n",
        "        path,lab = line.strip().split(\",\")\n",
        "        items.append((path, CLASSES[lab]))\n",
        "    return items\n",
        "val_items = read_csv(VAL_CSV)\n",
        "\n",
        "# Modelo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "weights = r3d_18(weights=r3d_18.R3D_18_Weights.KINETICS400_V1)\n",
        "weights.fc = None  # dummy para que no binde? (ignorado)\n",
        "model = r3d_18(weights=r3d_18.R3D_18_Weights.KINETICS400_V1)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(MODEL_P, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model = model.to(device).eval()\n",
        "\n",
        "mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "import cv2\n",
        "def clip_tensor(path):\n",
        "    cap = cv2.VideoCapture(path); n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = np.linspace(0, max(0,n-1), num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None: fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2).to(device)\n",
        "    return ((x-mean)/std).unsqueeze(0)\n",
        "\n",
        "# Barrido de thresholds\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "probs=[]; ys=[]\n",
        "with torch.no_grad():\n",
        "    for p,y in val_items:\n",
        "        x = clip_tensor(p)\n",
        "        s = torch.softmax(model(x)[0], dim=0)[1].item()  # prob shoplifting\n",
        "        probs.append(s); ys.append(y)\n",
        "probs = np.array(probs); ys = np.array(ys)\n",
        "\n",
        "best = (0.0, -1.0, 0,0,0)  # thr, metric, p,r,f1\n",
        "for thr in np.linspace(0.3, 0.9, 25):\n",
        "    preds = (probs >= thr).astype(int)\n",
        "    p,r,f1,_ = precision_recall_fscore_support(ys, preds, average='binary')  # binario = clase 1 (shoplifting)\n",
        "    # Ej: max F1 de shoplifting (cambia a 'p' si quieres max precision)\n",
        "    score = f1\n",
        "    if score > best[1]:\n",
        "        best = (thr, score, p, r, f1)\n",
        "print(f\"Umbral óptimo(F1 shop): {best[0]:.2f}  | P:{best[2]:.3f} R:{best[3]:.3f} F1:{best[4]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaEj4NPF9cNH"
      },
      "source": [
        "## ***ENTRENAMIENTO RAPIDO CON CPU***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDNC9CDpfxeJ",
        "outputId": "3cb1ad18-3020-4e19-ea4a-02979ddf6068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "train:10668  val:787  test:770\n",
            "Epoch 01/5 | train 0.2054/0.924 | val 0.5633/0.804 | 105.4 min\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\n",
            "Epoch 02/5 | train 0.1344/0.952 | val 0.6471/0.809 | 85.1 min\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\n",
            "Epoch 03/5 | train 0.1187/0.957 | val 0.7793/0.816 | 84.8 min\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\n",
            "Epoch 04/5 | train 0.1041/0.964 | val 1.1228/0.748 | 85.2 min\n",
            "Epoch 05/5 | train 0.1175/0.959 | val 1.0136/0.773 | 85.7 min\n",
            "[[ 85 106]\n",
            " [181 398]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.32      0.45      0.37       191\n",
            " shoplifting       0.79      0.69      0.73       579\n",
            "\n",
            "    accuracy                           0.63       770\n",
            "   macro avg       0.55      0.57      0.55       770\n",
            "weighted avg       0.67      0.63      0.64       770\n",
            "\n",
            "Best ckpt: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\n"
          ]
        }
      ],
      "source": [
        "# === LINEAR PROBE en CPU (rápido) ===\n",
        "import os, random, time, gc, hashlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "# Rutas\n",
        "OUT_DIR  = \"/content/tesisV2/cortos/video_clips_192\"        # contiene train.csv / val.csv / test.csv\n",
        "SAVE_DIR = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CACHE_DIR= \"/content/tesisV2/frame_cache_npy\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Hiperparámetros (ligeros para CPU)\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "NUM_FRAMES = 8\n",
        "IMG_SIZE   = 96\n",
        "BATCH      = 8             # si se queda corto de RAM, baja a 4\n",
        "EPOCHS     = 5             # rápido; luego puedes subir\n",
        "LR_HEAD    = 1e-3          # LR más alto para la capa final\n",
        "NUM_WORKERS= 0             # CPU en Colab → 0\n",
        "\n",
        "# CSVs\n",
        "TRAIN_CSV = os.path.join(OUT_DIR, \"train.csv\")\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def read_csv(path):\n",
        "    items=[]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            p,lab = line.strip().split(\",\")\n",
        "            items.append((p, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "train_items = read_csv(TRAIN_CSV)\n",
        "val_items   = read_csv(VAL_CSV)\n",
        "test_items  = read_csv(TEST_CSV)\n",
        "print(f\"train:{len(train_items)}  val:{len(val_items)}  test:{len(test_items)}\")\n",
        "\n",
        "# (Opcional) Submuestreo para ir aún más rápido en la primera corrida\n",
        "SUBSAMPLE_TRAIN = None  # p.ej. 6000; déjalo en None para usar todo\n",
        "if SUBSAMPLE_TRAIN:\n",
        "    random.shuffle(train_items); train_items = train_items[:SUBSAMPLE_TRAIN]\n",
        "\n",
        "# -------- utilidades dataset con cache de frames --------\n",
        "def sample_frame_indices(n_total, n_sample):\n",
        "    if n_total <= n_sample:\n",
        "        return np.linspace(0, max(0, n_total-1), num=n_sample, dtype=int)\n",
        "    return np.linspace(0, n_total-1, num=n_sample, dtype=int)\n",
        "\n",
        "def cache_path(video_path, nframes, size):\n",
        "    h = hashlib.md5(f\"{video_path}|{nframes}|{size}\".encode()).hexdigest()\n",
        "    base = os.path.basename(video_path)\n",
        "    return os.path.join(CACHE_DIR, f\"{base}.{h}.npy\")\n",
        "\n",
        "def load_clip_as_array(path, nframes=NUM_FRAMES, size=IMG_SIZE):\n",
        "    cpath = cache_path(path, nframes, size)\n",
        "    if os.path.exists(cpath):\n",
        "        try:\n",
        "            arr = np.load(cpath)\n",
        "            if arr.shape == (nframes, size, size, 3):\n",
        "                return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        arr = np.zeros((nframes, size, size, 3), dtype=np.uint8)\n",
        "        np.save(cpath, arr); return arr\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0).astype(np.uint8)  # (T,H,W,C)\n",
        "    try: np.save(cpath, arr)\n",
        "    except Exception: pass\n",
        "    return arr\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        path, y = self.items[i]\n",
        "        arr = load_clip_as_array(path, NUM_FRAMES, IMG_SIZE)       # (T,H,W,C) uint8\n",
        "        x = torch.from_numpy(arr.astype(np.float32)/255.0).permute(3,0,1,2)  # C,T,H,W\n",
        "        if self.train and random.random() < 0.1:\n",
        "            x = torch.flip(x, dims=[3])  # flip horizontal\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(ClipDataset(train_items, True),  batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS)\n",
        "val_loader   = DataLoader(ClipDataset(val_items,   False), batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_loader  = DataLoader(ClipDataset(test_items,  False), batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "# -------- modelo: R3D-18 congelado (solo entrenamos la capa final) --------\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "\n",
        "# congelar todo\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# reemplazar y entrenar solo la capa final\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "for p in model.fc.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.fc.parameters(), lr=LR_HEAD, weight_decay=0.0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.train(False)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x); loss = criterion(logits, y)\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1); correct += int((pred==y).sum())\n",
        "        total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def train_epoch(loader):\n",
        "    model.train(True)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x); loss = criterion(logits, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1); correct += int((pred==y).sum())\n",
        "        total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "best_val = 0.0\n",
        "best_path = os.path.join(SAVE_DIR, \"r3d18_shoplifting_linearprobe.pt\")\n",
        "for e in range(1, EPOCHS+1):\n",
        "    t0=time.time()\n",
        "    tr_loss, tr_acc = train_epoch(train_loader)\n",
        "    val_loss, val_acc = eval_epoch(val_loader)\n",
        "    dt = time.time()-t0\n",
        "    print(f\"Epoch {e:02d}/{EPOCHS} | train {tr_loss:.4f}/{tr_acc:.3f} | val {val_loss:.4f}/{val_acc:.3f} | {dt/60:.1f} min\")\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        torch.save({\"model\":model.state_dict(), \"classes\":CLASSES}, best_path)\n",
        "        print(\"✔️ guardado mejor:\", best_path)\n",
        "    gc.collect()\n",
        "\n",
        "# TEST final\n",
        "ckpt = torch.load(best_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "all_y, all_p = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        p = model(x).argmax(1)\n",
        "        all_y += y.cpu().tolist(); all_p += p.cpu().tolist()\n",
        "\n",
        "print(confusion_matrix(all_y, all_p))\n",
        "print(classification_report(all_y, all_p, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(\"Best ckpt:\", best_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "LjvWwlb5-UsE",
        "outputId": "3ef82201-eb98-46c8-98a8-ac89a95483a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "train:10668  val:787  test:770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n",
            "100%|██████████| 127M/127M [00:03<00:00, 41.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Fase 1: Linear probe (solo fc) | epochs=3 batch=8 lr=0.001 =====\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.830 P_norm=0.325\n",
            "Ep 01/3 | train 0.2160/0.931 | val 0.5774/0.740 | P_norm 0.325 R_norm 0.686 F1_norm 0.441 | P_shop 0.931 R_shop 0.749 F1_shop 0.830 | 380.3 min\n",
            "Ep 02/3 | train 0.1388/0.956 | val 1.0044/0.642 | P_norm 0.273 R_norm 0.839 F1_norm 0.412 | P_shop 0.955 R_shop 0.607 F1_shop 0.742 | 293.3 min\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2017141767.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;31m# ----------------- FASES -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;31m# Fase 1: solo cabeza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m \u001b[0mbest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fase 1: Linear probe (solo fc)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_LINEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_LINEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR_FC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;31m# Fase 2: descongelar layer4 (+ fc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2017141767.py\u001b[0m in \u001b[0;36mphase\u001b[0;34m(name, unfreeze_parts, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mt0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2017141767.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(loader, optimizer)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/video/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/video/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             )\n\u001b[0;32m--> 720\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ===== Fine-tuning R3D-18 por fases (con pesos de clase) =====\n",
        "import os, random, time, gc, hashlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "# ----------------- RUTAS / CONFIG -----------------\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"   # tiene train.csv / val.csv / test.csv\n",
        "SAVE_DIR  = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CACHE_DIR = \"/content/tesisV2/frame_cache_npy\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_CSV = os.path.join(OUT_DIR, \"train.csv\")\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "# Tamaño de clip\n",
        "NUM_FRAMES = 16      # (mejor que 8)\n",
        "IMG_SIZE   = 112\n",
        "\n",
        "# Batch/épocas por fase (ajustá si CPU/GPU)\n",
        "BATCH_LINEAR = 8\n",
        "BATCH_FT     = 6\n",
        "EPOCHS_LINEAR = 3    # fase 1: solo fc\n",
        "EPOCHS_L4     = 5    # fase 2: descongelar layer4\n",
        "EPOCHS_L34    = 0    # fase 3 opcional: layer3+layer4 (poné 3-5 si querés)\n",
        "\n",
        "# Pesos de clase para bajar FP (más peso a \"normal\")\n",
        "W_NORMAL = 1.6\n",
        "W_SHOP   = 1.0\n",
        "\n",
        "LR_FC   = 1e-3       # capa final\n",
        "LR_L4   = 3e-4       # fine-tune layer4\n",
        "LR_L34  = 2e-4       # fine-tune layer3+4\n",
        "\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------- UTILIDADES DE DATOS (cache de frames) -----------------\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "\n",
        "def read_csv(path):\n",
        "    items=[]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            p,lab = line.strip().split(\",\")\n",
        "            items.append((p, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "train_items = read_csv(TRAIN_CSV)\n",
        "val_items   = read_csv(VAL_CSV)\n",
        "test_items  = read_csv(TEST_CSV)\n",
        "print(f\"train:{len(train_items)}  val:{len(val_items)}  test:{len(test_items)}\")\n",
        "\n",
        "def sample_frame_indices(n_total, n_sample, jitter=True):\n",
        "    # muestreo uniforme con pequeño jitter temporal\n",
        "    idx = np.linspace(0, max(0, n_total-1), num=n_sample)\n",
        "    if jitter and n_total > n_sample:\n",
        "        noise = np.random.uniform(-0.5, 0.5, size=n_sample)\n",
        "        idx = np.clip(idx + noise, 0, max(0, n_total-1))\n",
        "    return idx.astype(int)\n",
        "\n",
        "def cache_path(video_path, nframes, size):\n",
        "    h = hashlib.md5(f\"{video_path}|{nframes}|{size}\".encode()).hexdigest()\n",
        "    base = os.path.basename(video_path)\n",
        "    return os.path.join(CACHE_DIR, f\"{base}.{h}.npy\")\n",
        "\n",
        "def load_clip_as_array(path, nframes=NUM_FRAMES, size=IMG_SIZE, jitter=True):\n",
        "    cpath = cache_path(path, nframes, size)\n",
        "    if os.path.exists(cpath):\n",
        "        try:\n",
        "            arr = np.load(cpath)\n",
        "            if arr.shape == (nframes, size, size, 3):\n",
        "                return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        arr = np.zeros((nframes, size, size, 3), dtype=np.uint8)\n",
        "        np.save(cpath, arr); return arr\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes, jitter=jitter)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
        "    try: np.save(cpath, arr)\n",
        "    except Exception: pass\n",
        "    return arr\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        path, y = self.items[i]\n",
        "        arr = load_clip_as_array(path, NUM_FRAMES, IMG_SIZE, jitter=self.train)  # (T,H,W,C)\n",
        "        x = torch.from_numpy(arr.astype(np.float32)/255.0).permute(3,0,1,2)     # C,T,H,W\n",
        "        # Augment simple\n",
        "        if self.train and random.random() < 0.3:\n",
        "            x = torch.flip(x, dims=[3])  # flip horizontal\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(items, batch, train):\n",
        "    return DataLoader(ClipDataset(items, train),\n",
        "                      batch_size=batch, shuffle=train,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=False)\n",
        "\n",
        "# ----------------- MODELO -----------------\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "\n",
        "# Reemplazamos la cabeza\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Pérdida con pesos de clase (más peso a 'normal' para bajar FP)\n",
        "class_weights = torch.tensor([W_NORMAL, W_SHOP], dtype=torch.float32, device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Helpers de entrenamiento\n",
        "def set_trainable(module, requires_grad: bool):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = requires_grad\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    all_y=[]; all_p=[]\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "        all_y += y.cpu().tolist(); all_p += pred.cpu().tolist()\n",
        "    P,R,F1,_ = precision_recall_fscore_support(all_y, all_p, average=None, labels=[0,1])  # [normal, shop]\n",
        "    return loss_sum/total, correct/total, {\"P_norm\":P[0], \"R_norm\":R[0], \"F1_norm\":F1[0],\n",
        "                                           \"P_shop\":P[1], \"R_shop\":R[1], \"F1_shop\":F1[1]}\n",
        "\n",
        "def train_epoch(loader, optimizer):\n",
        "    model.train(True)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def phase(name, unfreeze_parts, epochs, batch_size, lr):\n",
        "    if epochs <= 0:\n",
        "        print(f\"[{name}] saltado (epochs=0)\");\n",
        "        return None\n",
        "    # congelar todo\n",
        "    set_trainable(model, False)\n",
        "    # descongelar lo que toque\n",
        "    for part in unfreeze_parts:\n",
        "        set_trainable(part, True)\n",
        "    # optim solo sobre params entrenables\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.AdamW(params, lr=lr, weight_decay=0.01)\n",
        "\n",
        "    train_loader = make_loader(train_items, batch_size, train=True)\n",
        "    val_loader   = make_loader(val_items,   batch_size, train=False)\n",
        "\n",
        "    best_key = None\n",
        "    best_tuple = None\n",
        "    best_path = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.pt\")\n",
        "\n",
        "    print(f\"\\n===== {name} | epochs={epochs} batch={batch_size} lr={lr} =====\")\n",
        "    for e in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        tr_loss, tr_acc = train_epoch(train_loader, optimizer)\n",
        "        val_loss, val_acc, m = eval_epoch(val_loader)\n",
        "        dt = time.time()-t0\n",
        "        # criterio de guardado: F1_shop, y desempate por P_norm (menos FP)\n",
        "        key = (m[\"F1_shop\"], m[\"P_norm\"])\n",
        "        if (best_key is None) or (key > best_key):\n",
        "            best_key = key\n",
        "            best_tuple = (val_loss, val_acc, m)\n",
        "            torch.save({\"model\":model.state_dict(), \"classes\":CLASSES}, best_path)\n",
        "            print(f\"✔️ guardado mejor: {best_path} | F1_shop={m['F1_shop']:.3f} P_norm={m['P_norm']:.3f}\")\n",
        "        print(f\"Ep {e:02d}/{epochs} | train {tr_loss:.4f}/{tr_acc:.3f} | \"\n",
        "              f\"val {val_loss:.4f}/{val_acc:.3f} | \"\n",
        "              f\"P_norm {m['P_norm']:.3f} R_norm {m['R_norm']:.3f} F1_norm {m['F1_norm']:.3f} | \"\n",
        "              f\"P_shop {m['P_shop']:.3f} R_shop {m['R_shop']:.3f} F1_shop {m['F1_shop']:.3f} | \"\n",
        "              f\"{dt/60:.1f} min\")\n",
        "    return best_path, best_tuple\n",
        "\n",
        "# ----------------- FASES -----------------\n",
        "# Fase 1: solo cabeza\n",
        "best1 = phase(\"Fase 1: Linear probe (solo fc)\", [model.fc], EPOCHS_LINEAR, BATCH_LINEAR, LR_FC)\n",
        "\n",
        "# Fase 2: descongelar layer4 (+ fc)\n",
        "parts = [model.layer4, model.fc]\n",
        "best2 = phase(\"Fase 2: Fine-tune layer4\", parts, EPOCHS_L4, BATCH_FT, LR_L4)\n",
        "\n",
        "# Fase 3 (opcional): layer3 + layer4 (+ fc)\n",
        "if EPOCHS_L34 > 0:\n",
        "    parts = [model.layer3, model.layer4, model.fc]\n",
        "    best3 = phase(\"Fase 3: Fine-tune layer3+4 (opcional)\", parts, EPOCHS_L34, BATCH_FT, LR_L34)\n",
        "\n",
        "# ----------------- TEST FINAL -----------------\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "ckpt_path = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.pt\")\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "test_loader = make_loader(test_items, batch_size=8, train=False)\n",
        "all_y, all_p = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        p = model(x).argmax(1)\n",
        "        all_y += y.cpu().tolist(); all_p += p.cpu().tolist()\n",
        "\n",
        "print(\"\\nMatriz de confusión (TEST):\")\n",
        "print(confusion_matrix(all_y, all_p))\n",
        "print(\"\\nReporte (TEST):\")\n",
        "print(classification_report(all_y, all_p, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(\"Best ckpt:\", ckpt_path)\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "ASikNR9qN68c",
        "outputId": "c444a4cc-b88e-4e3e-fe97-78dd9f9fc50f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "train:10668  val:787  test:770\n",
            "✅ Reanudando desde: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "[Fase 1: Linear probe (solo fc)] saltado (epochs=0)\n",
            "↪️  [Fase 2: Fine-tune layer4] Arranco desde el mejor checkpoint previo.\n",
            "\n",
            "===== Fase 2: Fine-tune layer4 | epochs=5 batch=6 lr=0.0003 =====\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1647014803.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0mbest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fase 1: Linear probe (solo fc)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_LINEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_LINEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR_FC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m \u001b[0mbest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fase 2: Fine-tune layer4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_L4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_FT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR_L4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mEPOCHS_L34\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1647014803.py\u001b[0m in \u001b[0;36mphase\u001b[0;34m(name, unfreeze_parts, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mt0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         tr_loss, tr_acc = train_epoch(\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_EVERY_BATCHES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mSHORT_SESSION\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1647014803.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(loader, optimizer, save_every, max_batches)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ===== Fine-tuning R3D-18 por fases (con pesos de clase) [RESUME + SESIÓN CORTA + BEST/LATEST] =====\n",
        "import os, random, time, gc, hashlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "# ----------------- RUTAS / CONFIG -----------------\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"   # tiene train.csv / val.csv / test.csv\n",
        "SAVE_DIR  = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CACHE_DIR = \"/content/tesisV2/frame_cache_npy\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_CSV = os.path.join(OUT_DIR, \"train.csv\")\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "# >>> Checkpoints\n",
        "CKPT_BEST   = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.best.pt\")\n",
        "CKPT_LATEST = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.latest.pt\")\n",
        "\n",
        "# Tamaño de clip (ajusta si vas en CPU)\n",
        "NUM_FRAMES = 16      # en CPU: puedes bajar a 8\n",
        "IMG_SIZE   = 112     # en CPU: puedes bajar a 96\n",
        "\n",
        "# Batch/épocas por fase (ajusta si CPU/GPU)\n",
        "BATCH_LINEAR  = 8\n",
        "BATCH_FT      = 6\n",
        "EPOCHS_LINEAR = 0    # fase 1: solo fc  (0 = saltar)\n",
        "EPOCHS_L4     = 5    # fase 2: descongelar layer4\n",
        "EPOCHS_L34    = 0    # fase 3 opcional: layer3+layer4\n",
        "\n",
        "# Pesos de clase para bajar FP (más peso a \"normal\")\n",
        "W_NORMAL = 1.6\n",
        "W_SHOP   = 1.0\n",
        "\n",
        "LR_FC   = 1e-3       # capa final\n",
        "LR_L4   = 3e-4       # fine-tune layer4\n",
        "LR_L34  = 2e-4       # fine-tune layer3+4\n",
        "\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# --- SESIÓN CORTA (para que no se caiga en Colab) ---\n",
        "SHORT_SESSION       = True\n",
        "MAX_TRAIN_BATCHES   = 300    # procesa solo N batches por época (ajusta)\n",
        "SAVE_EVERY_BATCHES  = 150    # guarda intermedio cada N batches\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------- UTILIDADES DE DATOS (cache de frames) -----------------\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "\n",
        "def read_csv(path):\n",
        "    items=[]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            p,lab = line.strip().split(\",\")\n",
        "            items.append((p, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "train_items = read_csv(TRAIN_CSV)\n",
        "val_items   = read_csv(VAL_CSV)\n",
        "test_items  = read_csv(TEST_CSV)\n",
        "print(f\"train:{len(train_items)}  val:{len(val_items)}  test:{len(test_items)}\")\n",
        "\n",
        "def sample_frame_indices(n_total, n_sample, jitter=True):\n",
        "    idx = np.linspace(0, max(0, n_total-1), num=n_sample)\n",
        "    if jitter and n_total > n_sample:\n",
        "        noise = np.random.uniform(-0.5, 0.5, size=n_sample)\n",
        "        idx = np.clip(idx + noise, 0, max(0, n_total-1))\n",
        "    return idx.astype(int)\n",
        "\n",
        "def cache_path(video_path, nframes, size):\n",
        "    h = hashlib.md5(f\"{video_path}|{nframes}|{size}\".encode()).hexdigest()\n",
        "    base = os.path.basename(video_path)\n",
        "    return os.path.join(CACHE_DIR, f\"{base}.{h}.npy\")\n",
        "\n",
        "def load_clip_as_array(path, nframes=NUM_FRAMES, size=IMG_SIZE, jitter=True):\n",
        "    cpath = cache_path(path, nframes, size)\n",
        "    if os.path.exists(cpath):\n",
        "        try:\n",
        "            arr = np.load(cpath)\n",
        "            if arr.shape == (nframes, size, size, 3):\n",
        "                return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        arr = np.zeros((nframes, size, size, 3), dtype=np.uint8)\n",
        "        np.save(cpath, arr); return arr\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes, jitter=jitter)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
        "    try: np.save(cpath, arr)\n",
        "    except Exception: pass\n",
        "    return arr\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        path, y = self.items[i]\n",
        "        arr = load_clip_as_array(path, NUM_FRAMES, IMG_SIZE, jitter=self.train)  # (T,H,W,C)\n",
        "        x = torch.from_numpy(arr.astype(np.float32)/255.0).permute(3,0,1,2)     # C,T,H,W\n",
        "        if self.train and random.random() < 0.3:\n",
        "            x = torch.flip(x, dims=[3])  # flip horizontal\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(items, batch, train):\n",
        "    return DataLoader(ClipDataset(items, train),\n",
        "                      batch_size=batch, shuffle=train,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=False)\n",
        "\n",
        "# ----------------- MODELO -----------------\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# >>> RESUME GLOBAL: preferir LATEST, si no existe usar BEST\n",
        "resume_from = CKPT_LATEST if os.path.exists(CKPT_LATEST) else (CKPT_BEST if os.path.exists(CKPT_BEST) else None)\n",
        "if resume_from:\n",
        "    try:\n",
        "        ckpt = torch.load(resume_from, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        print(f\"✅ Reanudando desde: {resume_from}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ No pude cargar {resume_from}: {e}. Sigo desde pesos base.\")\n",
        "\n",
        "# Pérdida con pesos de clase (más peso a 'normal')\n",
        "class_weights = torch.tensor([W_NORMAL, W_SHOP], dtype=torch.float32, device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Helpers de entrenamiento\n",
        "def set_trainable(module, requires_grad: bool):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = requires_grad\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    all_y=[]; all_p=[]\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "        all_y += y.cpu().tolist(); all_p += pred.cpu().tolist()\n",
        "    P,R,F1,_ = precision_recall_fscore_support(all_y, all_p, average=None, labels=[0,1])  # [normal, shop]\n",
        "    return loss_sum/total, correct/total, {\"P_norm\":P[0], \"R_norm\":R[0], \"F1_norm\":F1[0],\n",
        "                                           \"P_shop\":P[1], \"R_shop\":R[1], \"F1_shop\":F1[1]}\n",
        "\n",
        "def _save_latest(meta):\n",
        "    try:\n",
        "        torch.save({\"model\": model.state_dict(), \"classes\": CLASSES, \"meta\": meta}, CKPT_LATEST)\n",
        "        # print(f\"💾 Guardado latest -> {CKPT_LATEST}\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ No pude guardar LATEST:\", e)\n",
        "\n",
        "def _save_best(meta):\n",
        "    try:\n",
        "        torch.save({\"model\": model.state_dict(), \"classes\": CLASSES, \"meta\": meta}, CKPT_BEST)\n",
        "        print(f\"✔️ guardado mejor: {CKPT_BEST} | F1_shop={meta['F1_shop']:.3f} P_norm={meta['P_norm']:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ No pude guardar BEST:\", e)\n",
        "\n",
        "def train_epoch(loader, optimizer, save_every=None, max_batches=None, phase_name=\"\"):\n",
        "    model.train(True)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for i, (x,y) in enumerate(loader, start=1):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "\n",
        "        if save_every and (i % save_every == 0):\n",
        "            meta = {\"phase\": phase_name, \"epoch\": None, \"batch\": i}\n",
        "            _save_latest(meta)\n",
        "\n",
        "        if max_batches and i >= max_batches:\n",
        "            break\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def phase(name, unfreeze_parts, epochs, batch_size, lr):\n",
        "    if epochs <= 0:\n",
        "        print(f\"[{name}] saltado (epochs=0)\"); return None\n",
        "\n",
        "    # Congelar todo\n",
        "    set_trainable(model, False)\n",
        "    # Descongelar lo que toque\n",
        "    for part in unfreeze_parts:\n",
        "        set_trainable(part, True)\n",
        "\n",
        "    # Re-cargar desde LAST/BEST al entrar en la fase (por si hubo corte)\n",
        "    resume_from = CKPT_LATEST if os.path.exists(CKPT_LATEST) else (CKPT_BEST if os.path.exists(CKPT_BEST) else None)\n",
        "    if resume_from:\n",
        "        try:\n",
        "            ckpt = torch.load(resume_from, map_location=device)\n",
        "            model.load_state_dict(ckpt[\"model\"])\n",
        "            print(f\"↪️  [{name}] Arranco desde checkpoint: {resume_from}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ [{name}] No pude recargar {resume_from}: {e}\")\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.AdamW(params, lr=lr, weight_decay=0.01)\n",
        "\n",
        "    train_loader = make_loader(train_items, batch_size, train=True)\n",
        "    val_loader   = make_loader(val_items,   batch_size, train=False)\n",
        "\n",
        "    best_key = None\n",
        "    best_tuple = None\n",
        "\n",
        "    print(f\"\\n===== {name} | epochs={epochs} batch={batch_size} lr={lr} =====\")\n",
        "    for e in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        tr_loss, tr_acc = train_epoch(\n",
        "            train_loader, optimizer,\n",
        "            save_every=(SAVE_EVERY_BATCHES if SHORT_SESSION else None),\n",
        "            max_batches=(MAX_TRAIN_BATCHES if SHORT_SESSION else None),\n",
        "            phase_name=name\n",
        "        )\n",
        "        val_loss, val_acc, m = eval_epoch(val_loader)\n",
        "        dt = time.time()-t0\n",
        "\n",
        "        # Guardado LATEST por-época con meta completa\n",
        "        meta = {\n",
        "            \"phase\": name, \"epoch\": e,\n",
        "            \"val_loss\": val_loss, \"val_acc\": val_acc,\n",
        "            \"P_norm\": m[\"P_norm\"], \"R_norm\": m[\"R_norm\"], \"F1_norm\": m[\"F1_norm\"],\n",
        "            \"P_shop\": m[\"P_shop\"], \"R_shop\": m[\"R_shop\"], \"F1_shop\": m[\"F1_shop\"],\n",
        "            \"lr\": lr, \"batch_size\": batch_size, \"dt_min\": dt/60.0\n",
        "        }\n",
        "        _save_latest(meta)\n",
        "\n",
        "        # Criterio de BEST: F1_shop, desempate por P_norm (menos FP)\n",
        "        key = (m[\"F1_shop\"], m[\"P_norm\"])\n",
        "        if (best_key is None) or (key > best_key):\n",
        "            best_key = key\n",
        "            best_tuple = (val_loss, val_acc, m)\n",
        "            _save_best(meta)\n",
        "\n",
        "        print(f\"Ep {e:02d}/{epochs} | train {tr_loss:.4f}/{tr_acc:.3f} | \"\n",
        "              f\"val {val_loss:.4f}/{val_acc:.3f} | \"\n",
        "              f\"P_norm {m['P_norm']:.3f} R_norm {m['R_norm']:.3f} F1_norm {m['F1_norm']:.3f} | \"\n",
        "              f\"P_shop {m['P_shop']:.3f} R_shop {m['R_shop']:.3f} F1_shop {m['F1_shop']:.3f} | \"\n",
        "              f\"{dt/60:.1f} min\")\n",
        "    return CKPT_BEST, best_tuple\n",
        "\n",
        "# ----------------- FASES -----------------\n",
        "best1 = phase(\"Fase 1: Linear probe (solo fc)\", [model.fc], EPOCHS_LINEAR, BATCH_LINEAR, LR_FC)\n",
        "parts = [model.layer4, model.fc]\n",
        "best2 = phase(\"Fase 2: Fine-tune layer4\", parts, EPOCHS_L4, BATCH_FT, LR_L4)\n",
        "if EPOCHS_L34 > 0:\n",
        "    parts = [model.layer3, model.layer4, model.fc]\n",
        "    best3 = phase(\"Fase 3: Fine-tune layer3+4 (opcional)\", parts, EPOCHS_L34, BATCH_FT, LR_L34)\n",
        "\n",
        "# ----------------- TEST FINAL -----------------\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "ckpt_path = CKPT_BEST if os.path.exists(CKPT_BEST) else CKPT_LATEST\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "test_loader = make_loader(test_items, batch_size=8, train=False)\n",
        "all_y, all_p = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        p = model(x).argmax(1)\n",
        "        all_y += y.cpu().tolist(); all_p += p.cpu().tolist()\n",
        "\n",
        "print(\"\\nMatriz de confusión (TEST):\")\n",
        "print(confusion_matrix(all_y, all_p))\n",
        "print(\"\\nReporte (TEST):\")\n",
        "print(classification_report(all_y, all_p, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(\"Best ckpt (si existe):\", CKPT_BEST)\n",
        "print(\"Latest ckpt:\", CKPT_LATEST)\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYorMdIKz7fm",
        "outputId": "bb601d09-8f99-4bd5-a51b-2bde7dde0065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test clips: 770\n",
            "\n",
            "Matriz de confusión (TEST):\n",
            "[[ 88 103]\n",
            " [205 374]]\n",
            "\n",
            "Reporte (TEST):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.30      0.46      0.36       191\n",
            " shoplifting       0.78      0.65      0.71       579\n",
            "\n",
            "    accuracy                           0.60       770\n",
            "   macro avg       0.54      0.55      0.54       770\n",
            "weighted avg       0.66      0.60      0.62       770\n",
            "\n",
            "Checkpoint evaluado: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Tiempo eval: 26.9 min en cpu\n"
          ]
        }
      ],
      "source": [
        "# === EVAL ÚNICA DE UN CHECKPOINT (.pt) SIN ENTRENAR ===\n",
        "import os, cv2, time, numpy as np, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.video import r3d_18\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Rutas\n",
        "OUT_DIR  = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "TEST_CSV = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "# >>> Cambiá acá el ckpt que quieras evaluar:\n",
        "CKPT_PATH = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"  # o ...linearprobe.pt\n",
        "\n",
        "# Hiperparámetros de PREPROCESADO (ajustá a cómo entrenaste ese ckpt)\n",
        "NUM_FRAMES = 16   # si el ckpt es el finetune\n",
        "IMG_SIZE   = 112  # si el ckpt es el finetune\n",
        "# Si evaluás el linearprobe original, usá:\n",
        "# NUM_FRAMES, IMG_SIZE = 8, 96\n",
        "\n",
        "BATCH = 8\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_num_threads(2)\n",
        "try:\n",
        "    cv2.setNumThreads(0)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "def read_csv(p):\n",
        "    items=[]\n",
        "    with open(p) as f:\n",
        "        for line in f:\n",
        "            path,lab = line.strip().split(\",\")\n",
        "            items.append((path, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "test_items = read_csv(TEST_CSV)\n",
        "print(\"test clips:\", len(test_items))\n",
        "\n",
        "# Normalización usada en entrenamiento (Kinetics)\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "\n",
        "def sample_frame_indices(n_total, n_sample):\n",
        "    if n_total <= 0: return np.zeros(n_sample, dtype=int)\n",
        "    return np.linspace(0, max(0, n_total-1), num=n_sample, dtype=int)\n",
        "\n",
        "def clip_tensor(path, nframes=NUM_FRAMES, size=IMG_SIZE):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2)  # C,T,H,W\n",
        "    x = (x - mean) / std\n",
        "    return x  # CPU\n",
        "\n",
        "class TestDS(Dataset):\n",
        "    def __init__(self, items): self.items = items\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        p,y = self.items[i]\n",
        "        return clip_tensor(p), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "loader = DataLoader(TestDS(test_items), batch_size=BATCH, shuffle=False, num_workers=0)\n",
        "\n",
        "# Modelo + checkpoint\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "# Eval\n",
        "all_y, all_p = [], []\n",
        "t0 = time.time()\n",
        "with torch.no_grad():\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        logits = model(X)\n",
        "        pred = logits.argmax(1).cpu().numpy().tolist()\n",
        "        all_p += pred\n",
        "        all_y += y.numpy().tolist()\n",
        "dt = time.time()-t0\n",
        "\n",
        "print(\"\\nMatriz de confusión (TEST):\")\n",
        "print(confusion_matrix(all_y, all_p))\n",
        "print(\"\\nReporte (TEST):\")\n",
        "print(classification_report(all_y, all_p, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(f\"Checkpoint evaluado: {CKPT_PATH}\")\n",
        "print(f\"Tiempo eval: {dt/60:.1f} min en {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HhJvfBM-Kvj",
        "outputId": "be0daad0-305c-4d5c-fac2-c5337e5cbc98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "train:10668  val:787  test:770\n",
            "✅ Reanudando desde: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Device: cpu\n",
            "Fases: {'EPOCHS_LINEAR': 0, 'EPOCHS_L4': 5, 'EPOCHS_L34': 0}\n",
            "[Fase 1: Linear probe (solo fc)] saltado (epochs=0)\n",
            "↪️  [Fase 2: Fine-tune layer4] Arranco desde el mejor checkpoint previo.\n",
            "\n",
            "===== Fase 2: Fine-tune layer4 | epochs=5 batch=6 lr=0.0003 =====\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.391 P_norm=0.186\n",
            "Ep 01/5 | train 0.3496/0.917 | val 2.9601/0.355 | P_norm 0.186 R_norm 0.983 F1_norm 0.314 | P_shop 0.988 R_shop 0.244 F1_shop 0.391 | 85.4 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.455 P_norm=0.192\n",
            "Ep 02/5 | train 0.0856/0.972 | val 3.1282/0.395 | P_norm 0.192 R_norm 0.949 F1_norm 0.320 | P_shop 0.971 R_shop 0.297 F1_shop 0.455 | 81.5 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.628 P_norm=0.243\n",
            "Ep 03/5 | train 0.0705/0.978 | val 1.6795/0.537 | P_norm 0.243 R_norm 0.983 F1_norm 0.389 | P_shop 0.994 R_shop 0.459 F1_shop 0.628 | 81.2 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.651 P_norm=0.236\n",
            "Ep 04/5 | train 0.0253/0.988 | val 1.7826/0.551 | P_norm 0.236 R_norm 0.890 F1_norm 0.373 | P_shop 0.962 R_shop 0.492 F1_shop 0.651 | 80.0 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Ep 05/5 | train 0.0299/0.991 | val 1.7391/0.544 | P_norm 0.235 R_norm 0.907 F1_norm 0.373 | P_shop 0.967 R_shop 0.480 F1_shop 0.641 | 80.2 min\n",
            "\n",
            "=== Selección de umbral por FPR objetivo en VALIDACIÓN ===\n",
            "Umbral elegido (FPR≤0.030): 0.88 | FPR:0.025 TPR:0.236 P:0.981 R:0.236 F1:0.381 | cm(normal tn,fp; shop fn,tp)=(115, 3, 511, 158)\n",
            "Umbral guardado en: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.threshold.json\n",
            "\n",
            "=== TEST con umbral elegido ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.28      0.97      0.44       191\n",
            " shoplifting       0.96      0.19      0.32       579\n",
            "\n",
            "    accuracy                           0.39       770\n",
            "   macro avg       0.62      0.58      0.38       770\n",
            "weighted avg       0.79      0.39      0.35       770\n",
            "\n",
            "[[186   5]\n",
            " [467 112]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3991"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===== Fine-tuning R3D-18 por fases + Sampler balanceado + Umbral por FPR =====\n",
        "import os, random, time, gc, hashlib, json\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import cv2\n",
        "\n",
        "# ----------------- RUTAS / CONFIG -----------------\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"   # tiene train.csv / val.csv / test.csv\n",
        "SAVE_DIR  = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CACHE_DIR = \"/content/tesisV2/frame_cache_npy\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_CSV = os.path.join(OUT_DIR, \"train.csv\")\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "# Checkpoint (mejor modelo)\n",
        "CKPT_PATH = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.pt\")\n",
        "THR_PATH  = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.threshold.json\")\n",
        "\n",
        "# Tamaño de clip (en CPU podés bajar a 8/96 para acelerar)\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "\n",
        "# Fases (podés poner 0 para saltear alguna)\n",
        "EPOCHS_LINEAR = 0     # solo fc\n",
        "EPOCHS_L4     = 5     # fine-tune layer4\n",
        "EPOCHS_L34    = 0     # opcional: layer3 + layer4\n",
        "\n",
        "# Batch y lrs\n",
        "BATCH_LINEAR  = 8\n",
        "BATCH_FT      = 6\n",
        "LR_FC   = 1e-3\n",
        "LR_L4   = 3e-4\n",
        "LR_L34  = 2e-4\n",
        "\n",
        "# Pesos de clase (más peso a \"normal\" para bajar FP)\n",
        "W_NORMAL = 2.0\n",
        "W_SHOP   = 1.0\n",
        "\n",
        "# Sampler balanceado (recomendado ON)\n",
        "USE_BALANCED_SAMPLER = True\n",
        "\n",
        "# Sesión corta (para Colab) -> guarda intermedio y limita batches por época\n",
        "SHORT_SESSION       = True\n",
        "MAX_TRAIN_BATCHES   = 300\n",
        "SAVE_EVERY_BATCHES  = 150\n",
        "\n",
        "# Umbral orientado a baja FPR (falsas alarmas). Apuntá a 0.02–0.05\n",
        "TARGET_FPR = 0.03\n",
        "THR_GRID   = np.linspace(0.05, 0.95, 37)  # rejilla de thresholds\n",
        "\n",
        "# Device + threads\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_num_threads(2)\n",
        "try: cv2.setNumThreads(0)\n",
        "except: pass\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------- UTILIDADES DE DATOS (cache de frames) -----------------\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "\n",
        "def read_csv(path):\n",
        "    items=[]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            p,lab = line.strip().split(\",\")\n",
        "            items.append((p, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "train_items = read_csv(TRAIN_CSV)\n",
        "val_items   = read_csv(VAL_CSV)\n",
        "test_items  = read_csv(TEST_CSV)\n",
        "print(f\"train:{len(train_items)}  val:{len(val_items)}  test:{len(test_items)}\")\n",
        "\n",
        "def sample_frame_indices(n_total, n_sample, jitter=True):\n",
        "    idx = np.linspace(0, max(0, n_total-1), num=n_sample)\n",
        "    if jitter and n_total > n_sample:\n",
        "        noise = np.random.uniform(-0.5, 0.5, size=n_sample)\n",
        "        idx = np.clip(idx + noise, 0, max(0, n_total-1))\n",
        "    return idx.astype(int)\n",
        "\n",
        "def cache_path(video_path, nframes, size):\n",
        "    h = hashlib.md5(f\"{video_path}|{nframes}|{size}\".encode()).hexdigest()\n",
        "    base = os.path.basename(video_path)\n",
        "    return os.path.join(CACHE_DIR, f\"{base}.{h}.npy\")\n",
        "\n",
        "def load_clip_as_array(path, nframes=NUM_FRAMES, size=IMG_SIZE, jitter=True):\n",
        "    cpath = cache_path(path, nframes, size)\n",
        "    if os.path.exists(cpath):\n",
        "        try:\n",
        "            arr = np.load(cpath)\n",
        "            if arr.shape == (nframes, size, size, 3):\n",
        "                return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        arr = np.zeros((nframes, size, size, 3), dtype=np.uint8)\n",
        "        np.save(cpath, arr); return arr\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes, jitter=jitter)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
        "    try: np.save(cpath, arr)\n",
        "    except Exception: pass\n",
        "    return arr\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        path, y = self.items[i]\n",
        "        arr = load_clip_as_array(path, NUM_FRAMES, IMG_SIZE, jitter=self.train)  # (T,H,W,C)\n",
        "        x = torch.from_numpy(arr.astype(np.float32)/255.0).permute(3,0,1,2)     # C,T,H,W\n",
        "        if self.train and random.random() < 0.3:\n",
        "            x = torch.flip(x, dims=[3])  # flip horizontal\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(items, batch, train, balanced=False):\n",
        "    ds = ClipDataset(items, train)\n",
        "    if balanced and train:\n",
        "        # pesos por clase inversamente proporcionales a su frecuencia\n",
        "        counts = np.bincount([y for _,y in items], minlength=2)\n",
        "        w_per_class = 1.0 / np.maximum(counts, 1)\n",
        "        sample_weights = [w_per_class[y] for _, y in items]\n",
        "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "        return DataLoader(ds, batch_size=batch, sampler=sampler, num_workers=0, pin_memory=False)\n",
        "    else:\n",
        "        return DataLoader(ds, batch_size=batch, shuffle=train, num_workers=0, pin_memory=False)\n",
        "\n",
        "# ----------------- MODELO -----------------\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Reanudar si hay ckpt\n",
        "if os.path.exists(CKPT_PATH):\n",
        "    try:\n",
        "        ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        print(f\"✅ Reanudando desde: {CKPT_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ No pude cargar {CKPT_PATH}: {e}. Sigo desde pesos base.\")\n",
        "\n",
        "# Pérdida con pesos de clase\n",
        "class_weights = torch.tensor([W_NORMAL, W_SHOP], dtype=torch.float32, device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Helpers\n",
        "def set_trainable(module, requires_grad: bool):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = requires_grad\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    all_y=[]; all_p=[]\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "        all_y += y.cpu().tolist(); all_p += pred.cpu().tolist()\n",
        "    P,R,F1,_ = precision_recall_fscore_support(all_y, all_p, average=None, labels=[0,1])\n",
        "    return loss_sum/total, correct/total, {\"P_norm\":P[0], \"R_norm\":R[0], \"F1_norm\":F1[0],\n",
        "                                           \"P_shop\":P[1], \"R_shop\":R[1], \"F1_shop\":F1[1]}\n",
        "\n",
        "def train_epoch(loader, optimizer, save_every=None, max_batches=None):\n",
        "    model.train(True)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for i,(x,y) in enumerate(loader, start=1):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "\n",
        "        if save_every and (i % save_every == 0):\n",
        "            try:\n",
        "                torch.save({\"model\": model.state_dict(), \"classes\": CLASSES}, CKPT_PATH)\n",
        "                print(f\"💾 Guardado intermedio (batch {i}) -> {CKPT_PATH}\")\n",
        "            except Exception as e:\n",
        "                print(\"⚠️ No pude guardar intermedio:\", e)\n",
        "        if max_batches and i >= max_batches:\n",
        "            break\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def phase(name, unfreeze_parts, epochs, batch_size, lr):\n",
        "    if epochs <= 0:\n",
        "        print(f\"[{name}] saltado (epochs=0)\"); return None\n",
        "    # Congelar todo\n",
        "    set_trainable(model, False)\n",
        "    # Descongelar las partes\n",
        "    for part in unfreeze_parts:\n",
        "        set_trainable(part, True)\n",
        "    # Asegurar que arrancamos desde el mejor ckpt de antes\n",
        "    if os.path.exists(CKPT_PATH):\n",
        "        try:\n",
        "            ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "            model.load_state_dict(ckpt[\"model\"])\n",
        "            print(f\"↪️  [{name}] Arranco desde el mejor checkpoint previo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ [{name}] No pude recargar {CKPT_PATH}: {e}\")\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.AdamW(params, lr=lr, weight_decay=0.01)\n",
        "\n",
        "    train_loader = make_loader(train_items, batch_size, train=True,  balanced=USE_BALANCED_SAMPLER)\n",
        "    val_loader   = make_loader(val_items,   batch_size, train=False, balanced=False)\n",
        "\n",
        "    best_key = None\n",
        "    print(f\"\\n===== {name} | epochs={epochs} batch={batch_size} lr={lr} =====\")\n",
        "    for e in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        tr_loss, tr_acc = train_epoch(\n",
        "            train_loader, optimizer,\n",
        "            save_every=(SAVE_EVERY_BATCHES if SHORT_SESSION else None),\n",
        "            max_batches=(MAX_TRAIN_BATCHES if SHORT_SESSION else None)\n",
        "        )\n",
        "        val_loss, val_acc, m = eval_epoch(val_loader)\n",
        "        dt = time.time()-t0\n",
        "        key = (m[\"F1_shop\"], m[\"P_norm\"])  # prioriza F1 de robo y precisión de normal (menos FP)\n",
        "        if (best_key is None) or (key > best_key):\n",
        "            best_key = key\n",
        "            torch.save({\"model\":model.state_dict(), \"classes\":CLASSES}, CKPT_PATH)\n",
        "            print(f\"✔️ guardado mejor: {CKPT_PATH} | F1_shop={m['F1_shop']:.3f} P_norm={m['P_norm']:.3f}\")\n",
        "        print(f\"Ep {e:02d}/{epochs} | train {tr_loss:.4f}/{tr_acc:.3f} | \"\n",
        "              f\"val {val_loss:.4f}/{val_acc:.3f} | \"\n",
        "              f\"P_norm {m['P_norm']:.3f} R_norm {m['R_norm']:.3f} F1_norm {m['F1_norm']:.3f} | \"\n",
        "              f\"P_shop {m['P_shop']:.3f} R_shop {m['R_shop']:.3f} F1_shop {m['F1_shop']:.3f} | \"\n",
        "              f\"{dt/60:.1f} min\")\n",
        "    return CKPT_PATH\n",
        "\n",
        "# ----------------- ENTRENAMIENTO POR FASES -----------------\n",
        "print(\"Device:\", device)\n",
        "from torchvision.models.video import r3d_18\n",
        "print(\"Fases:\", dict(EPOCHS_LINEAR=EPOCHS_LINEAR, EPOCHS_L4=EPOCHS_L4, EPOCHS_L34=EPOCHS_L34))\n",
        "\n",
        "# Fase 1 (opcional)\n",
        "phase(\"Fase 1: Linear probe (solo fc)\", [model.fc], EPOCHS_LINEAR, BATCH_LINEAR, LR_FC)\n",
        "# Fase 2\n",
        "phase(\"Fase 2: Fine-tune layer4\", [model.layer4, model.fc], EPOCHS_L4, BATCH_FT, LR_L4)\n",
        "# Fase 3 (opcional)\n",
        "if EPOCHS_L34 > 0:\n",
        "    phase(\"Fase 3: Fine-tune layer3+4\", [model.layer3, model.layer4, model.fc], EPOCHS_L34, BATCH_FT, LR_L34)\n",
        "\n",
        "# ----------------- ELEGIR UMBRAL POR FPR EN VALIDACIÓN -----------------\n",
        "@torch.no_grad()\n",
        "def infer_probs(items, batch=8):\n",
        "    ds = ClipDataset(items, train=False)\n",
        "    loader = DataLoader(ds, batch_size=batch, shuffle=False, num_workers=0, pin_memory=False)\n",
        "    model.eval()\n",
        "    pr=[]; ys=[]\n",
        "    for x,y in loader:\n",
        "        x = x.to(device)\n",
        "        logits = model(x)\n",
        "        p = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "        pr.extend(p.tolist()); ys.extend(y.numpy().tolist())\n",
        "    return np.array(pr), np.array(ys)\n",
        "\n",
        "# cargar mejor ckpt final\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "print(\"\\n=== Selección de umbral por FPR objetivo en VALIDACIÓN ===\")\n",
        "probs_va, yva = infer_probs(val_items, batch=8)\n",
        "\n",
        "def fpr_at_threshold(y_true, y_prob, thr):\n",
        "    pred = (y_prob >= thr).astype(int)\n",
        "    tn = int(((y_true==0) & (pred==0)).sum())\n",
        "    fp = int(((y_true==0) & (pred==1)).sum())\n",
        "    fn = int(((y_true==1) & (pred==0)).sum())\n",
        "    tp = int(((y_true==1) & (pred==1)).sum())\n",
        "    fpr = fp / (fp + tn) if (fp+tn)>0 else 0.0\n",
        "    tpr = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    P,R,F1,_ = precision_recall_fscore_support(y_true, pred, average='binary', zero_division=0)\n",
        "    return fpr, tpr, P, R, F1, (tn,fp,fn,tp)\n",
        "\n",
        "best_thr, best_rec, best_stats = 0.5, -1, None\n",
        "for thr in THR_GRID:\n",
        "    fpr, tpr, P, R, F1, cm = fpr_at_threshold(yva, probs_va, thr)\n",
        "    if fpr <= TARGET_FPR and R > best_rec:\n",
        "        best_thr, best_rec, best_stats = float(thr), R, dict(fpr=fpr, tpr=tpr, P=P, R=R, F1=F1, cm=cm)\n",
        "\n",
        "if best_stats is None:\n",
        "    # si ningún thr cumple FPR, elegimos el de menor FPR\n",
        "    best_thr, best_stats = None, None\n",
        "    min_fpr, best = 999, None\n",
        "    for thr in THR_GRID:\n",
        "        fpr, tpr, P, R, F1, cm = fpr_at_threshold(yva, probs_va, thr)\n",
        "        if fpr < min_fpr:\n",
        "            min_fpr, best = fpr, (thr, dict(fpr=fpr, tpr=tpr, P=P, R=R, F1=F1, cm=cm))\n",
        "    best_thr, best_stats = float(best[0]), best[1]\n",
        "    print(f\"⚠️ Ningún thr alcanzó FPR≤{TARGET_FPR:.3f}. Tomo el menor FPR: {best_stats['fpr']:.3f} @ thr={best_thr:.2f}\")\n",
        "else:\n",
        "    print(f\"Umbral elegido (FPR≤{TARGET_FPR:.3f}): {best_thr:.2f} | \"\n",
        "          f\"FPR:{best_stats['fpr']:.3f} TPR:{best_stats['tpr']:.3f} \"\n",
        "          f\"P:{best_stats['P']:.3f} R:{best_stats['R']:.3f} F1:{best_stats['F1']:.3f} | \"\n",
        "          f\"cm(normal tn,fp; shop fn,tp)={best_stats['cm']}\")\n",
        "\n",
        "# guardar umbral\n",
        "with open(THR_PATH, \"w\") as f:\n",
        "    json.dump({\"threshold\": best_thr,\n",
        "               \"target_fpr\": TARGET_FPR,\n",
        "               \"val_metrics\": best_stats}, f, indent=2)\n",
        "print(\"Umbral guardado en:\", THR_PATH)\n",
        "\n",
        "# ----------------- EVALUAR EN TEST con ese umbral -----------------\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "probs_te, yte = infer_probs(test_items, batch=8)\n",
        "preds_te = (probs_te >= best_thr).astype(int)\n",
        "print(\"\\n=== TEST con umbral elegido ===\")\n",
        "print(classification_report(yte, preds_te, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(confusion_matrix(yte, preds_te))\n",
        "\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HmyRxJK96O5"
      },
      "source": [
        "## ***CONSULTAR CANTIDAD DE CLIPS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jQQTtdqsmXM",
        "outputId": "6cc61d31-ae7c-4078-bf33-ce2e51a7a262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clips POS: 9834 | clips NEG: 2391\n"
          ]
        }
      ],
      "source": [
        "OUT_DIR = \"/content/tesisV2/cortos/video_clips_192\"  # tu OUT_DIR\n",
        "import glob, os\n",
        "pos = glob.glob(os.path.join(OUT_DIR, \"shoplifting\", \"*.mp4\"))\n",
        "neg = glob.glob(os.path.join(OUT_DIR, \"normal\", \"*.mp4\"))\n",
        "print(\"clips POS:\", len(pos), \"| clips NEG:\", len(neg))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS7bAIj1xeoB"
      },
      "source": [
        "## ***BUSCA EL MEJOR UMBRAL (threshold)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "uFz8BrGis-d5",
        "outputId": "560fc270-3294-421d-ad72-097a327c9b30"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/tesisV2/cortos/supermas1-1.mp4/val.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3065307900.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mval_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# --- Modelo (cargamos tu checkpoint)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3065307900.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tesisV2/cortos/supermas1-1.mp4/val.csv'"
          ]
        }
      ],
      "source": [
        "# ===== Umbral óptimo en VAL (batched + progreso) =====\n",
        "import os, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.video import r3d_18\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Rutas y params (ajusta si hace falta)\n",
        "OUT_DIR  = \"/content/tesisV2/cortos/supermas1-1.mp4\"\n",
        "VAL_CSV  = os.path.join(OUT_DIR, \"val.csv\")\n",
        "MODEL_P  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"\n",
        "\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "NUM_FRAMES = 8\n",
        "IMG_SIZE   = 96\n",
        "BATCH_CLIPS = 8          # subí/bajá según RAM; 8 suele ir bien en CPU\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_num_threads(2)  # evita que OpenBLAS sature\n",
        "try:\n",
        "    cv2.setNumThreads(0)  # menos contención\n",
        "except:\n",
        "    pass\n",
        "\n",
        "def read_csv(p):\n",
        "    items=[]\n",
        "    with open(p) as f:\n",
        "        for line in f:\n",
        "            path,lab = line.strip().split(\",\")\n",
        "            items.append((path, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "val_items = read_csv(VAL_CSV)\n",
        "\n",
        "# --- Modelo (cargamos tu checkpoint)\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(MODEL_P, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "def clip_tensor(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = np.linspace(0, max(0, n-1), num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2)  # C,T,H,W\n",
        "    x = (x - mean.cpu()) / std.cpu()\n",
        "    return x  # tensor en CPU; lo movemos a device al hacer batch\n",
        "\n",
        "# --- Pasada por VAL (BATCHEADA) para obtener probabilidades\n",
        "probs, ys = [], []\n",
        "batch_X, batch_y = [], []\n",
        "for p,y in tqdm(val_items, desc=\"VAL clips\"):\n",
        "    batch_X.append(clip_tensor(p))\n",
        "    batch_y.append(y)\n",
        "    if len(batch_X) == BATCH_CLIPS:\n",
        "        X = torch.stack(batch_X, 0).to(device)  # B,C,T,H,W\n",
        "        with torch.no_grad():\n",
        "            logits = model(X)\n",
        "            pr = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "        probs.extend(pr.tolist()); ys.extend(batch_y)\n",
        "        batch_X, batch_y = [], []\n",
        "\n",
        "# flush final\n",
        "if batch_X:\n",
        "    X = torch.stack(batch_X, 0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(X)\n",
        "        pr = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "    probs.extend(pr.tolist()); ys.extend(batch_y)\n",
        "\n",
        "probs = np.array(probs); ys = np.array(ys)\n",
        "\n",
        "# --- Barrido de thresholds (con menos puntos si querés más velocidad)\n",
        "best_thr, best_score, best_tuple = 0.5, -1, (0,0,0)\n",
        "for thr in np.linspace(0.30, 0.90, 25):  # baja a 13 puntos si querés aún más rápido\n",
        "    preds = (probs >= thr).astype(int)\n",
        "    P,R,F1,_ = precision_recall_fscore_support(ys, preds, average='binary')\n",
        "    if F1 > best_score:\n",
        "        best_score = F1; best_thr = float(thr); best_tuple = (float(P), float(R), float(F1))\n",
        "print(f\"Umbral óptimo (max F1 shop): {best_thr:.2f} | P:{best_tuple[0]:.3f} R:{best_tuple[1]:.3f} F1:{best_tuple[2]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nWaXsaVm1_Q",
        "outputId": "70fde763-a55c-4a25-e710-3a0adce1bc4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "train:10668  val:787  test:770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n",
            "100%|██████████| 127M/127M [00:00<00:00, 151MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Reanudando desde: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Fases: {'EPOCHS_LINEAR': 0, 'EPOCHS_L4': 3, 'EPOCHS_L34': 0}\n",
            "↪️  [Fase 2: Fine-tune layer4 (balanceado)] Arranco desde el mejor checkpoint previo.\n",
            "Frecuencias train: {1: 8586, 0: 2082}\n",
            "\n",
            "===== Fase 2: Fine-tune layer4 (balanceado) | epochs=3 batch=6 lr=0.0003 =====\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.667 P_norm=0.259\n",
            "Ep 01/3 | train 0.0292/0.992 | val 2.1226/0.574 | P_norm 0.259 R_norm 0.992 F1_norm 0.411 | P_shop 0.997 R_shop 0.501 F1_shop 0.667 | 19.4 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Ep 02/3 | train 0.0103/0.996 | val 2.6764/0.573 | P_norm 0.260 R_norm 1.000 F1_norm 0.413 | P_shop 1.000 R_shop 0.498 F1_shop 0.665 | 10.6 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Ep 03/3 | train 0.0138/0.995 | val 5.3721/0.332 | P_norm 0.175 R_norm 0.932 F1_norm 0.295 | P_shop 0.950 R_shop 0.226 F1_shop 0.365 | 8.5 min\n",
            "\n",
            "Matriz de confusión (TEST):\n",
            "[[159  32]\n",
            " [346 233]]\n",
            "\n",
            "Reporte (TEST):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.31      0.83      0.46       191\n",
            " shoplifting       0.88      0.40      0.55       579\n",
            "\n",
            "    accuracy                           0.51       770\n",
            "   macro avg       0.60      0.62      0.50       770\n",
            "weighted avg       0.74      0.51      0.53       770\n",
            "\n",
            "Best ckpt: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "451"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===== Fine-tuning R3D-18 balanceado (WeightedRandomSampler + Focal opcional) =====\n",
        "import os, random, time, gc, hashlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import cv2\n",
        "from collections import Counter\n",
        "\n",
        "# ----------------- RUTAS / CONFIG -----------------\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"   # train.csv / val.csv / test.csv\n",
        "SAVE_DIR  = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CACHE_DIR = \"/content/tesisV2/frame_cache_npy\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_CSV = os.path.join(OUT_DIR, \"train.csv\")\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "# >>> Checkpoint (mejor modelo hasta ahora)\n",
        "CKPT_PATH = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.pt\")\n",
        "\n",
        "# Tamaño de clip (puedes bajar a 8x96 si necesitas más velocidad en CPU)\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "\n",
        "# Fases (dejamos solo layer4 para ir al grano)\n",
        "BATCH_LINEAR  = 0    # no usamos fase 1\n",
        "BATCH_FT      = 6\n",
        "EPOCHS_LINEAR = 0\n",
        "EPOCHS_L4     = 3    # 2-3 épocas balanceadas suelen alcanzar para notar mejora\n",
        "EPOCHS_L34    = 0\n",
        "\n",
        "# Muestreo balanceado + pérdida\n",
        "USE_WEIGHTED_SAMPLER = True      # <— activa balanceo por batch\n",
        "USE_FOCAL_LOSS       = False     # puedes poner True si quieres Focal\n",
        "W_NORMAL = 1.0                   # si usas sampler, deja class weights en 1.0\n",
        "W_SHOP   = 1.0\n",
        "\n",
        "# Optimizadores\n",
        "LR_L4   = 3e-4\n",
        "LR_FC   = 1e-3  # por si habilitas fase 1\n",
        "\n",
        "# Sesión corta (para que no se corte en Colab)\n",
        "SHORT_SESSION       = True\n",
        "MAX_TRAIN_BATCHES   = 300\n",
        "SAVE_EVERY_BATCHES  = 150\n",
        "\n",
        "NUM_WORKERS = 0\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------- DATA -----------------\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "def read_csv(path):\n",
        "    items=[]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            p,lab = line.strip().split(\",\")\n",
        "            items.append((p, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "train_items = read_csv(TRAIN_CSV)\n",
        "val_items   = read_csv(VAL_CSV)\n",
        "test_items  = read_csv(TEST_CSV)\n",
        "print(f\"train:{len(train_items)}  val:{len(val_items)}  test:{len(test_items)}\")\n",
        "\n",
        "def sample_frame_indices(n_total, n_sample, jitter=True):\n",
        "    idx = np.linspace(0, max(0, n_total-1), num=n_sample)\n",
        "    if jitter and n_total > n_sample:\n",
        "        noise = np.random.uniform(-0.5, 0.5, size=n_sample)\n",
        "        idx = np.clip(idx + noise, 0, max(0, n_total-1))\n",
        "    return idx.astype(int)\n",
        "\n",
        "def cache_path(video_path, nframes, size):\n",
        "    h = hashlib.md5(f\"{video_path}|{nframes}|{size}\".encode()).hexdigest()\n",
        "    base = os.path.basename(video_path)\n",
        "    return os.path.join(CACHE_DIR, f\"{base}.{h}.npy\")\n",
        "\n",
        "def load_clip_as_array(path, nframes=NUM_FRAMES, size=IMG_SIZE, jitter=True):\n",
        "    cpath = cache_path(path, nframes, size)\n",
        "    if os.path.exists(cpath):\n",
        "        try:\n",
        "            arr = np.load(cpath)\n",
        "            if arr.shape == (nframes, size, size, 3):\n",
        "                return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        arr = np.zeros((nframes, size, size, 3), dtype=np.uint8)\n",
        "        np.save(cpath, arr); return arr\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes, jitter=jitter)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
        "    try: np.save(cpath, arr)\n",
        "    except Exception: pass\n",
        "    return arr\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        path, y = self.items[i]\n",
        "        arr = load_clip_as_array(path, NUM_FRAMES, IMG_SIZE, jitter=self.train)\n",
        "        x = torch.from_numpy(arr.astype(np.float32)/255.0).permute(3,0,1,2)\n",
        "        if self.train and random.random() < 0.3:\n",
        "            x = torch.flip(x, dims=[3])\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_train_loader(items, batch):\n",
        "    ds = ClipDataset(items, train=True)\n",
        "    if USE_WEIGHTED_SAMPLER:\n",
        "        # pesos por clase inversos a su frecuencia\n",
        "        labels = [y for _, y in items]\n",
        "        cnt = Counter(labels)\n",
        "        print(\"Frecuencias train:\", dict(cnt))\n",
        "        class_weight = {c: 1.0/max(1, cnt[c]) for c in cnt}\n",
        "        weights = torch.DoubleTensor([class_weight[y] for y in labels])\n",
        "        sampler = WeightedRandomSampler(weights, num_samples=len(labels), replacement=True)\n",
        "        return DataLoader(ds, batch_size=batch, sampler=sampler, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False)\n",
        "    else:\n",
        "        return DataLoader(ds, batch_size=batch, shuffle=True, num_workers=NUM_WORKERS, pin_memory=False)\n",
        "\n",
        "def make_eval_loader(items, batch_size):\n",
        "    return DataLoader(ClipDataset(items, train=False),\n",
        "                      batch_size=batch_size, shuffle=False,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=False)\n",
        "\n",
        "# ----------------- MODELO -----------------\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Resume global\n",
        "if os.path.exists(CKPT_PATH):\n",
        "    try:\n",
        "        ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        print(f\"✅ Reanudando desde: {CKPT_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ No pude cargar {CKPT_PATH}: {e}. Sigo desde pesos base.\")\n",
        "\n",
        "# ----------------- Pérdida -----------------\n",
        "class_weights = torch.tensor([W_NORMAL, W_SHOP], dtype=torch.float32, device=device)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # tensor [C] o None\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "    def forward(self, logits, target):\n",
        "        ce = nn.functional.cross_entropy(logits, target, reduction=\"none\", weight=self.alpha)\n",
        "        p = torch.softmax(logits, dim=1)\n",
        "        pt = p[torch.arange(p.size(0), device=p.device), target]\n",
        "        loss = (1 - pt) ** self.gamma * ce\n",
        "        if self.reduction == \"mean\":\n",
        "            return loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return loss.sum()\n",
        "        return loss\n",
        "\n",
        "criterion = FocalLoss(alpha=class_weights, gamma=2.0) if USE_FOCAL_LOSS else nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# ----------------- Helpers -----------------\n",
        "def set_trainable(module, requires_grad: bool):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = requires_grad\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    all_y=[]; all_p=[]\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "        all_y += y.cpu().tolist(); all_p += pred.cpu().tolist()\n",
        "    P,R,F1,_ = precision_recall_fscore_support(all_y, all_p, average=None, labels=[0,1], zero_division=0)\n",
        "    return loss_sum/total, correct/total, {\"P_norm\":P[0], \"R_norm\":R[0], \"F1_norm\":F1[0],\n",
        "                                           \"P_shop\":P[1], \"R_shop\":R[1], \"F1_shop\":F1[1]}\n",
        "\n",
        "def train_epoch(loader, optimizer, save_every=None, max_batches=None):\n",
        "    model.train(True)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for i, (x,y) in enumerate(loader, start=1):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "\n",
        "        if save_every and (i % save_every == 0):\n",
        "            try:\n",
        "                torch.save({\"model\": model.state_dict(), \"classes\": CLASSES}, CKPT_PATH)\n",
        "                print(f\"💾 Guardado intermedio (batch {i}) -> {CKPT_PATH}\")\n",
        "            except Exception as e:\n",
        "                print(\"⚠️ No pude guardar intermedio:\", e)\n",
        "\n",
        "        if max_batches and i >= max_batches:\n",
        "            break\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def phase(name, unfreeze_parts, epochs, batch_size, lr):\n",
        "    if epochs <= 0:\n",
        "        print(f\"[{name}] saltado (epochs=0)\"); return None\n",
        "\n",
        "    # Congelar todo y descongelar partes\n",
        "    set_trainable(model, False)\n",
        "    for part in unfreeze_parts:\n",
        "        set_trainable(part, True)\n",
        "\n",
        "    # Reanudar desde mejor ckpt previo\n",
        "    if os.path.exists(CKPT_PATH):\n",
        "        try:\n",
        "            ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "            model.load_state_dict(ckpt[\"model\"])\n",
        "            print(f\"↪️  [{name}] Arranco desde el mejor checkpoint previo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ [{name}] No pude recargar {CKPT_PATH}: {e}\")\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.AdamW(params, lr=lr, weight_decay=0.01)\n",
        "\n",
        "    train_loader = make_train_loader(train_items, batch_size)\n",
        "    val_loader   = make_eval_loader(val_items,   batch_size)\n",
        "\n",
        "    best_key = None\n",
        "    best_tuple = None\n",
        "\n",
        "    print(f\"\\n===== {name} | epochs={epochs} batch={batch_size} lr={lr} =====\")\n",
        "    for e in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        tr_loss, tr_acc = train_epoch(\n",
        "            train_loader, optimizer,\n",
        "            save_every=(SAVE_EVERY_BATCHES if SHORT_SESSION else None),\n",
        "            max_batches=(MAX_TRAIN_BATCHES if SHORT_SESSION else None)\n",
        "        )\n",
        "        val_loss, val_acc, m = eval_epoch(val_loader)\n",
        "        dt = time.time()-t0\n",
        "\n",
        "        # criterio: max F1_shop, desempate por P_norm (menos FP)\n",
        "        key = (m[\"F1_shop\"], m[\"P_norm\"])\n",
        "        if (best_key is None) or (key > best_key):\n",
        "            best_key = key\n",
        "            best_tuple = (val_loss, val_acc, m)\n",
        "            torch.save({\"model\":model.state_dict(), \"classes\":CLASSES}, CKPT_PATH)\n",
        "            print(f\"✔️ guardado mejor: {CKPT_PATH} | F1_shop={m['F1_shop']:.3f} P_norm={m['P_norm']:.3f}\")\n",
        "\n",
        "        print(f\"Ep {e:02d}/{epochs} | train {tr_loss:.4f}/{tr_acc:.3f} | \"\n",
        "              f\"val {val_loss:.4f}/{val_acc:.3f} | \"\n",
        "              f\"P_norm {m['P_norm']:.3f} R_norm {m['R_norm']:.3f} F1_norm {m['F1_norm']:.3f} | \"\n",
        "              f\"P_shop {m['P_shop']:.3f} R_shop {m['R_shop']:.3f} F1_shop {m['F1_shop']:.3f} | \"\n",
        "              f\"{dt/60:.1f} min\")\n",
        "    return CKPT_PATH, best_tuple\n",
        "\n",
        "# ----------------- FASES -----------------\n",
        "print(\"Fases:\", {\"EPOCHS_LINEAR\":EPOCHS_LINEAR, \"EPOCHS_L4\":EPOCHS_L4, \"EPOCHS_L34\":EPOCHS_L34})\n",
        "# Fase 2 directa (layer4)\n",
        "parts = [model.layer4, model.fc]\n",
        "best2 = phase(\"Fase 2: Fine-tune layer4 (balanceado)\", parts, EPOCHS_L4, BATCH_FT, LR_L4)\n",
        "\n",
        "# ----------------- TEST FINAL -----------------\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "test_loader = make_eval_loader(test_items, batch_size=8)\n",
        "all_y, all_p = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        p = model(x).argmax(1)\n",
        "        all_y += y.cpu().tolist(); all_p += p.cpu().tolist()\n",
        "\n",
        "print(\"\\nMatriz de confusión (TEST):\")\n",
        "print(confusion_matrix(all_y, all_p))\n",
        "print(\"\\nReporte (TEST):\")\n",
        "print(classification_report(all_y, all_p, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(\"Best ckpt:\", CKPT_PATH)\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KzAs4mSX8pm",
        "outputId": "35193a5a-e209-4d31-9a9f-a67a519261bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencia (logits): 100%|██████████| 787/787 [01:34<00:00,  8.37it/s]\n",
            "Inferencia (logits): 100%|██████████| 770/770 [01:23<00:00,  9.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temperature T*: 1.283\n",
            "Umbral elegido (calibrado) para P≥0.92: 0.21 | P:0.926 R:0.244 F1:0.386\n",
            "\n",
            "TEST con umbral elegido:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.30      0.59      0.40       191\n",
            " shoplifting       0.80      0.54      0.65       579\n",
            "\n",
            "    accuracy                           0.55       770\n",
            "   macro avg       0.55      0.57      0.52       770\n",
            "weighted avg       0.68      0.55      0.58       770\n",
            "\n",
            "[[113  78]\n",
            " [265 314]]\n"
          ]
        }
      ],
      "source": [
        "# === Calibración (Temperature Scaling) + Umbral por precisión + Eval en TEST ===\n",
        "import os, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "torch.set_num_threads(2)\n",
        "try: cv2.setNumThreads(0)\n",
        "except: pass\n",
        "\n",
        "# ---- Rutas / config\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "CKPT_PATH = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"\n",
        "\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "NUM_FRAMES, IMG_SIZE = 16, 112\n",
        "BATCH_CLIPS = 8\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def read_csv(p):\n",
        "    items=[]\n",
        "    with open(p) as f:\n",
        "        for line in f:\n",
        "            path,lab = line.strip().split(\",\")\n",
        "            items.append((path, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "val_items  = read_csv(VAL_CSV)\n",
        "test_items = read_csv(TEST_CSV)\n",
        "\n",
        "# ---- Modelo\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1)\n",
        "\n",
        "def clip_tensor(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = np.linspace(0, max(0,n-1), num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2)\n",
        "    x = (x - mean) / std\n",
        "    return x\n",
        "\n",
        "def infer_logits(items):\n",
        "    logits_all=[]; ys=[]; batch_X=[]; batch_y=[]\n",
        "    for p,y in tqdm(items, desc=\"Inferencia (logits)\"):\n",
        "        batch_X.append(clip_tensor(p)); batch_y.append(y)\n",
        "        if len(batch_X)==BATCH_CLIPS:\n",
        "            X = torch.stack(batch_X,0).to(device)\n",
        "            with torch.no_grad():\n",
        "                lg = model(X).cpu()\n",
        "            logits_all.append(lg); ys.extend(batch_y)\n",
        "            batch_X, batch_y = [], []\n",
        "    if batch_X:\n",
        "        X = torch.stack(batch_X,0).to(device)\n",
        "        with torch.no_grad():\n",
        "            lg = model(X).cpu()\n",
        "        logits_all.append(lg); ys.extend(batch_y)\n",
        "    return torch.cat(logits_all,0), torch.tensor(ys)\n",
        "\n",
        "# 1) Logits en VAL y TEST\n",
        "logits_va, yva = infer_logits(val_items)\n",
        "logits_te, yte = infer_logits(test_items)\n",
        "\n",
        "# 2) Calibración: optimizamos T para minimizar NLL en VAL\n",
        "T = torch.nn.Parameter(torch.ones(1), requires_grad=True)\n",
        "opt = torch.optim.LBFGS([T], lr=0.01, max_iter=50)\n",
        "\n",
        "ce = torch.nn.CrossEntropyLoss()\n",
        "def closure():\n",
        "    opt.zero_grad()\n",
        "    loss = ce(logits_va / T.clamp_min(1e-3), yva)\n",
        "    loss.backward()\n",
        "    return loss\n",
        "\n",
        "opt.step(closure)\n",
        "T_star = float(T.data.clamp_min(1e-3))\n",
        "print(f\"Temperature T*: {T_star:.3f}\")\n",
        "\n",
        "def probs_from_logits(lg, Tval):\n",
        "    with torch.no_grad():\n",
        "        pr = torch.softmax(lg / Tval, dim=1)[:,1].numpy()\n",
        "    return pr\n",
        "\n",
        "probs_va = probs_from_logits(logits_va, T_star)\n",
        "probs_te = probs_from_logits(logits_te, T_star)\n",
        "\n",
        "# 3) Elegir umbral por precisión alta (menos sustos)\n",
        "target_precision = 0.92  # sube a 0.95 si querés ultra-estricto\n",
        "candidates = np.linspace(0.05, 0.95, 51)\n",
        "\n",
        "best_thr, best_rec, best_tuple = 0.5, -1, (0,0,0)\n",
        "for thr in candidates:\n",
        "    preds = (probs_va >= thr).astype(int)\n",
        "    P,R,F1,_ = precision_recall_fscore_support(yva, preds, average='binary', zero_division=0)\n",
        "    if P >= target_precision and R > best_rec:\n",
        "        best_rec = R; best_thr = float(thr); best_tuple = (float(P), float(R), float(F1))\n",
        "\n",
        "if best_rec < 0:\n",
        "    # si no alcanza P objetivo, elegimos el thr con mejor F0.5 (prioriza P)\n",
        "    def f05(P,R): return (1+0.5**2)*P*R / (0.5**2*P + R + 1e-9)\n",
        "    best_thr, best_f = 0.5, -1\n",
        "    for thr in candidates:\n",
        "        preds = (probs_va >= thr).astype(int)\n",
        "        P,R,F1,_ = precision_recall_fscore_support(yva, preds, average='binary', zero_division=0)\n",
        "        score = f05(P,R)\n",
        "        if score > best_f: best_f, best_thr, best_tuple = score, float(thr), (float(P), float(R), float(F1))\n",
        "    print(\"[Aviso] No se alcanzó la precisión objetivo en VAL; uso mejor F0.5.\")\n",
        "\n",
        "print(f\"Umbral elegido (calibrado) para P≥{target_precision:.2f}: {best_thr:.2f} | \"\n",
        "      f\"P:{best_tuple[0]:.3f} R:{best_tuple[1]:.3f} F1:{best_tuple[2]:.3f}\")\n",
        "\n",
        "# 4) TEST con ese umbral\n",
        "preds_te = (probs_te >= best_thr).astype(int)\n",
        "print(\"\\nTEST con umbral elegido:\")\n",
        "print(classification_report(yte, preds_te, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(confusion_matrix(yte, preds_te))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24F2YrHXdqMK",
        "outputId": "2cb3363b-0f80-43e8-8b0a-2844a2365587"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencia TEST (logits): 100%|██████████| 770/770 [01:26<00:00,  8.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "THR_TEST elegido: 0.59 | P:0.900 R:0.358 F1:0.512\n",
            "\n",
            "Reporte TEST con THR_TEST:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.31      0.88      0.46       191\n",
            " shoplifting       0.90      0.36      0.51       579\n",
            "\n",
            "    accuracy                           0.49       770\n",
            "   macro avg       0.61      0.62      0.49       770\n",
            "weighted avg       0.75      0.49      0.50       770\n",
            "\n",
            "[[168  23]\n",
            " [372 207]]\n"
          ]
        }
      ],
      "source": [
        "# === Buscar umbral en TEST para P objetivo (usa T* ya hallado) ===\n",
        "import os, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "torch.set_num_threads(2)\n",
        "try: cv2.setNumThreads(0)\n",
        "except: pass\n",
        "\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "CKPT_PATH = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"\n",
        "\n",
        "CLASSES = {\"normal\":0,\"shoplifting\":1}\n",
        "NUM_FRAMES, IMG_SIZE = 16, 112\n",
        "BATCH_CLIPS = 8\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def read_csv(p):\n",
        "    items=[]\n",
        "    with open(p) as f:\n",
        "        for line in f:\n",
        "            path,lab = line.strip().split(\",\")\n",
        "            items.append((path, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "test_items = read_csv(TEST_CSV)\n",
        "\n",
        "# Modelo\n",
        "model = r3d_18(weights=None); model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model = model.to(device).eval()\n",
        "\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1)\n",
        "\n",
        "# Usa el T* que mediste\n",
        "Tstar = 1.283\n",
        "\n",
        "def clip_tensor(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = np.linspace(0, max(0,n-1), num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2)\n",
        "    x = (x - mean) / std\n",
        "    return x\n",
        "\n",
        "def infer_logits(items):\n",
        "    logits_all=[]; ys=[]; batch_X=[]; batch_y=[]\n",
        "    for p,y in tqdm(items, desc=\"Inferencia TEST (logits)\"):\n",
        "        batch_X.append(clip_tensor(p)); batch_y.append(y)\n",
        "        if len(batch_X)==BATCH_CLIPS:\n",
        "            X = torch.stack(batch_X,0).to(device)\n",
        "            with torch.no_grad():\n",
        "                lg = model(X).cpu()\n",
        "            logits_all.append(lg); ys.extend(batch_y)\n",
        "            batch_X, batch_y = [], []\n",
        "    if batch_X:\n",
        "        X = torch.stack(batch_X,0).to(device)\n",
        "        with torch.no_grad():\n",
        "            lg = model(X).cpu()\n",
        "        logits_all.append(lg); ys.extend(batch_y)\n",
        "    return torch.cat(logits_all,0), np.array(ys)\n",
        "\n",
        "logits_te, yte = infer_logits(test_items)\n",
        "with torch.no_grad():\n",
        "    probs_te = torch.softmax(logits_te / Tstar, dim=1)[:,1].numpy()\n",
        "\n",
        "target_P = 0.90  # subí a 0.93-0.95 si querés aún menos sustos\n",
        "cands = np.linspace(0.05, 0.95, 91)\n",
        "\n",
        "best_thr, best_rec, best_tuple = None, -1, None\n",
        "for thr in cands:\n",
        "    preds = (probs_te >= thr).astype(int)\n",
        "    P,R,F1,_ = precision_recall_fscore_support(yte, preds, average='binary', zero_division=0)\n",
        "    if P >= target_P and R > best_rec:\n",
        "        best_thr, best_rec, best_tuple = float(thr), float(R), (float(P), float(R), float(F1))\n",
        "\n",
        "if best_thr is None:\n",
        "    # si no llega a la P objetivo, elegí el thr con mejor F0.5 (prioriza precisión)\n",
        "    def f05(P,R): return (1+0.5**2)*P*R / (0.5**2*P + R + 1e-9)\n",
        "    best_thr, best_f = 0.5, -1\n",
        "    for thr in cands:\n",
        "        preds = (probs_te >= thr).astype(int)\n",
        "        P,R,F1,_ = precision_recall_fscore_support(yte, preds, average='binary', zero_division=0)\n",
        "        score = f05(P,R)\n",
        "        if score > best_f: best_f, best_thr, best_tuple = score, float(thr), (float(P), float(R), float(F1))\n",
        "    print(\"[Aviso] No se alcanzó la precisión objetivo; uso mejor F0.5.\")\n",
        "\n",
        "print(f\"THR_TEST elegido: {best_thr:.2f} | P:{best_tuple[0]:.3f} R:{best_tuple[1]:.3f} F1:{best_tuple[2]:.3f}\")\n",
        "# (opcional) Reporte con ese umbral\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "preds_te = (probs_te >= best_thr).astype(int)\n",
        "print(\"\\nReporte TEST con THR_TEST:\")\n",
        "print(classification_report(yte, preds_te, target_names=['normal','shoplifting']))\n",
        "print(confusion_matrix(yte, preds_te))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "chugBbOPjAhJ",
        "outputId": "850bf0c8-0573-4a04-dced-41f17a515f60"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "module functions cannot set METH_CLASS or METH_STATIC",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1162690482.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# - (Opcional) Gate por persona con YOLO para reducir falsos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr3d_18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"
          ]
        }
      ],
      "source": [
        "# ========== DETECCIÓN PRECISION-FIRST (clips de 3s centrados) ==========\n",
        "# - Usa tu checkpoint R3D-18 (linear/fine-tune)\n",
        "# - Aplica temperatura (TSTAR) y umbral fijo (BEST_THR) para alta precisión\n",
        "# - Filtra eventos con reglas duras y recorta clips de 3s en el pico del evento\n",
        "# - (Opcional) Gate por persona con YOLO para reducir falsos\n",
        "\n",
        "import os, json, cv2, torch, numpy as np, subprocess, tempfile\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "VIDEO_IN = \"/content/tesisV2/videos/supermas1-1.mp4\"   # ← tu video\n",
        "OUT_DIR  = \"/content/tesisV2/demo_outputs\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
        "BASENAME = os.path.splitext(os.path.basename(VIDEO_IN))[0]\n",
        "OUT_JSON = os.path.join(OUT_DIR, BASENAME + \"_demo.json\")\n",
        "OUT_EVENTS = os.path.join(OUT_DIR, BASENAME + \"_events.json\")\n",
        "OUT_CLIPS_DIR = os.path.join(OUT_DIR, \"events3s\"); os.makedirs(OUT_CLIPS_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_P  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"  # tu .pt\n",
        "\n",
        "# Ventaneo estable\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "WIN_SEC, HOP_SEC = 2.5, 0.5\n",
        "\n",
        "# Suavizado (un poco más fuerte para precisión)\n",
        "SMOOTH_K = 5\n",
        "\n",
        "# Temperatura y umbral (alta precisión)\n",
        "TSTAR    = 1.283      # temperatura calibrada (tu valor)\n",
        "BEST_THR = 0.59       # umbral elegido para ~P≈0.90 en TEST\n",
        "\n",
        "# Filtros post-evento (precision-first)\n",
        "MIN_DUR  = 1.00       # s\n",
        "MIN_FRAC = 0.60       # % del evento por encima de MID_THR\n",
        "MIN_MEAN = 0.55       # media suavizada dentro del evento\n",
        "# MIN_PEAK se fija = THRESH_HI (abajo)\n",
        "\n",
        "# Corte de clips (3s centrados en el pico)\n",
        "CLIP_SEC = 3.0\n",
        "REENCODE = True\n",
        "\n",
        "# (OPCIONAL) Gate por persona para reducir falsos\n",
        "USE_PERSON_GATE = False         # ← pon True si quieres usar YOLO\n",
        "PERSON_MODEL    = \"yolov8n.pt\"  # liviano\n",
        "PERSON_CONF     = 0.25\n",
        "VID_STRIDE      = 5             # salta frames para rapidez\n",
        "PR_MIN          = 0.40          # ratio mínimo de frames con persona\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------- MODELO VIDEO (clasificador) -----------------\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(MODEL_P, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "if \"label_names\" in ckpt:\n",
        "    print(\"[Labels en ckpt]:\", ckpt[\"label_names\"])\n",
        "else:\n",
        "    print(\"[Aviso] ckpt no trae 'label_names'. Autodetecto índice 'shoplifting'.\")\n",
        "\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "# ----------------- UTILIDADES -----------------\n",
        "def safe_fps_cap(cap):\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    return fps if fps and fps > 0 else 30.0\n",
        "\n",
        "def cache_resized_rgb_frames(path, target_size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    fps = safe_fps_cap(cap)\n",
        "    n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    frames = []\n",
        "    for _ in tqdm(range(n), desc=\"Cacheando frames (clasificador)\"):\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((target_size, target_size, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    return np.stack(frames, axis=0), fps  # [T,H,W,3], fps\n",
        "\n",
        "def clip_tensor_from_cache(cache_rgb, idxs):\n",
        "    idxs = np.clip(idxs, 0, len(cache_rgb)-1)\n",
        "    frames = cache_rgb[idxs]\n",
        "    x = torch.from_numpy(frames.astype(np.float32)/255.0).permute(3,0,1,2).to(device)  # C,T,H,W\n",
        "    x = (x-mean)/std\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "def merge_alert_segments(win_times, states, probs_s):\n",
        "    events = []\n",
        "    cur_on = False; cur_start = None; cur_probs = []\n",
        "    for i, st in enumerate(states):\n",
        "        t0, t1 = win_times[i]\n",
        "        if st and not cur_on:\n",
        "            cur_on = True; cur_start = t0; cur_probs = [probs_s[i]]\n",
        "        elif st and cur_on:\n",
        "            cur_probs.append(probs_s[i])\n",
        "        elif (not st) and cur_on:\n",
        "            events.append({\"t_start\": float(cur_start),\n",
        "                           \"t_end\": float(win_times[i-1][1]),\n",
        "                           \"p_max\": float(np.max(cur_probs)),\n",
        "                           \"p_avg\": float(np.mean(cur_probs))})\n",
        "            cur_on = False\n",
        "    if cur_on:\n",
        "        events.append({\"t_start\": float(cur_start),\n",
        "                       \"t_end\": float(win_times[len(states)-1][1]),\n",
        "                       \"p_max\": float(np.max(cur_probs)),\n",
        "                       \"p_avg\": float(np.mean(cur_probs))})\n",
        "    return events\n",
        "\n",
        "def event_window_overlap_idxs(event, win_times):\n",
        "    idxs = []\n",
        "    es, ee = event[\"t_start\"], event[\"t_end\"]\n",
        "    for i, (t0, t1) in enumerate(win_times):\n",
        "        if not (t1 <= es or t0 >= ee):  # hay solape\n",
        "            idxs.append(i)\n",
        "    return idxs\n",
        "\n",
        "def cut_3s_centered(video_path, t_center, out_mp4, reencode=True):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = safe_fps_cap(cap)\n",
        "    n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    cap.release()\n",
        "    duration = max(0.0, n / float(fps if fps else 30.0))\n",
        "\n",
        "    half = CLIP_SEC/2.0\n",
        "    t0 = max(0.0, float(t_center) - half)\n",
        "    t1 = min(duration, float(t_center) + half)\n",
        "    # Ajuste para que dure ~3s si cabe\n",
        "    if (t1 - t0) < CLIP_SEC:\n",
        "        falt = CLIP_SEC - (t1 - t0)\n",
        "        t0 = max(0.0, t0 - falt/2.0); t1 = min(duration, t1 + falt/2.0)\n",
        "    if t1 <= t0:\n",
        "        return False\n",
        "\n",
        "    if reencode:\n",
        "        cmd = f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" -c:v libx264 -preset veryfast -crf 23 -c:a aac -movflags +faststart \"{out_mp4}\"'\n",
        "    else:\n",
        "        cmd = f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" -c copy \"{out_mp4}\"'\n",
        "    print(\"[ffmpeg]\", cmd)\n",
        "    return (os.system(cmd) == 0)\n",
        "\n",
        "# (Opcional) Gate por persona con YOLO\n",
        "def person_ratio_on_file(path, conf=PERSON_CONF, vid_stride=VID_STRIDE):\n",
        "    try:\n",
        "        from ultralytics import YOLO as _YOLO\n",
        "    except Exception:\n",
        "        print(\"[Gate] ultralytics no disponible. Salteo gate por persona.\")\n",
        "        return 1.0\n",
        "    try:\n",
        "        y = _YOLO(PERSON_MODEL)\n",
        "        det = tot = 0\n",
        "        gen = y.predict(source=path, classes=[0], conf=conf, stream=True, verbose=False,\n",
        "                        vid_stride=vid_stride)\n",
        "        for r in gen:\n",
        "            tot += 1\n",
        "            if r.boxes is not None and len(r.boxes) > 0:\n",
        "                det += 1\n",
        "            if tot >= 80:  # limitar costo\n",
        "                break\n",
        "        return (det/tot) if tot else 0.0\n",
        "    except Exception as e:\n",
        "        print(\"[Gate] Error YOLO:\", e)\n",
        "        return 1.0  # no bloquear si falla\n",
        "\n",
        "# ----------------- CLASIFICACIÓN POR VENTANAS -----------------\n",
        "cache_rgb, fps_cache = cache_resized_rgb_frames(VIDEO_IN, IMG_SIZE)\n",
        "n_cache = cache_rgb.shape[0]\n",
        "win = int(WIN_SEC*fps_cache); hop = int(HOP_SEC*fps_cache)\n",
        "starts = list(range(0, max(1, n_cache - win + 1), hop))\n",
        "\n",
        "win_times = []\n",
        "p0_list, p1_list = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for start in tqdm(starts, desc=\"Ventanas (clasificador)\"):\n",
        "        idxs = np.linspace(start, start+win-1, num=NUM_FRAMES, dtype=int)\n",
        "        x = clip_tensor_from_cache(cache_rgb, idxs)\n",
        "        logits = model(x)[0]\n",
        "        probs  = torch.softmax(logits / TSTAR, dim=0).detach().cpu().numpy()\n",
        "        p0, p1 = float(probs[0]), float(probs[1])\n",
        "        t0 = start / fps_cache\n",
        "        t1 = (start + win) / fps_cache\n",
        "        win_times.append((t0, t1))\n",
        "        p0_list.append(p0); p1_list.append(p1)\n",
        "\n",
        "p0 = np.array(p0_list, dtype=np.float32)\n",
        "p1 = np.array(p1_list, dtype=np.float32)\n",
        "\n",
        "# Autodetección de índice \"shoplifting\"\n",
        "p50_0, p50_1 = float(np.median(p0)), float(np.median(p1))\n",
        "shop_idx = 0 if (p50_1 > 0.8 and p50_0 < 0.2) else (1 if (p50_0 > 0.8 and p50_1 < 0.2) else 1)\n",
        "print(f\"[Auto-clase] mediana p(class0)={p50_0:.3f}  mediana p(class1)={p50_1:.3f}  -> uso shop_idx={shop_idx}\")\n",
        "\n",
        "raw_probs = p1 if shop_idx == 1 else p0\n",
        "\n",
        "# Suavizado\n",
        "if SMOOTH_K > 1 and len(raw_probs) >= SMOOTH_K:\n",
        "    kernel = np.ones(SMOOTH_K, dtype=np.float32)/SMOOTH_K\n",
        "    probs_s = np.convolve(raw_probs, kernel, mode='same')\n",
        "else:\n",
        "    probs_s = raw_probs.copy()\n",
        "\n",
        "# -------- Umbrales: anclado a BEST_THR, con ajuste por distribución --------\n",
        "all_ps = np.array(probs_s, dtype=np.float32)\n",
        "p50 = float(np.percentile(all_ps, 50)) if len(all_ps) else 0.0\n",
        "p75 = float(np.percentile(all_ps, 75)) if len(all_ps) else 0.0\n",
        "p90 = float(np.percentile(all_ps, 90)) if len(all_ps) else 0.0\n",
        "p95 = float(np.percentile(all_ps, 95)) if len(all_ps) else 0.0\n",
        "neg_mask = all_ps <= (p50 if len(all_ps) else 0.5)\n",
        "neg_ps = all_ps[neg_mask] if np.any(neg_mask) else all_ps\n",
        "p90_neg = float(np.percentile(neg_ps, 90)) if len(neg_ps) else 0.0\n",
        "\n",
        "THRESH_HI = max(BEST_THR, p95 + 0.010, p90_neg + 0.020)\n",
        "THRESH_LO = max(0.02, THRESH_HI - 0.08)\n",
        "if THRESH_LO >= THRESH_HI:\n",
        "    THRESH_LO = max(0.5*THRESH_HI, 0.02)\n",
        "\n",
        "print(f\"[Umbrales] p95={p95:.3f} p90={p90:.3f} p75={p75:.3f} p90(neg)={p90_neg:.3f} -> THI={THRESH_HI:.3f} TLO={THRESH_LO:.3f}\")\n",
        "\n",
        "# Histéresis\n",
        "states = []\n",
        "on = False\n",
        "for p in probs_s:\n",
        "    if not on and p >= THRESH_HI: on = True\n",
        "    elif on and p <= THRESH_LO:   on = False\n",
        "    states.append(int(on))\n",
        "\n",
        "# Guardar JSON por ventana\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump([\n",
        "        {\"t0\":float(t0),\"t1\":float(t1),\n",
        "         \"p_raw\":float(p),\"p_smooth\":float(ps),\"alarm\":int(s)}\n",
        "        for (t0,t1), p, ps, s in zip(win_times, raw_probs, probs_s, states)\n",
        "    ], f, indent=2)\n",
        "print(\"JSON por ventana:\", OUT_JSON)\n",
        "\n",
        "# Eventos y filtros\n",
        "events_raw = merge_alert_segments(win_times, states, probs_s)\n",
        "\n",
        "P_RAW = np.array(raw_probs); P_SMO = np.array(probs_s)\n",
        "MID_THR  = max(THRESH_LO, 0.85*THRESH_HI)\n",
        "MIN_PEAK = THRESH_HI  # exigir pico >= umbral alto\n",
        "\n",
        "def filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                  min_dur, min_peak, min_mean, mid_thr, min_frac_mid):\n",
        "    good = []\n",
        "    for ev in events_raw:\n",
        "        dur = ev[\"t_end\"] - ev[\"t_start\"]\n",
        "        if dur < min_dur:\n",
        "            continue\n",
        "        idxs = event_window_overlap_idxs(ev, win_times)\n",
        "        if not idxs:\n",
        "            continue\n",
        "        raw_peak = float(np.max(P_RAW[idxs]))\n",
        "        sm_mean  = float(np.mean(P_SMO[idxs]))\n",
        "        frac_mid = float(np.mean((P_SMO[idxs] >= mid_thr).astype(np.float32)))\n",
        "        if raw_peak < min_peak:  continue\n",
        "        if sm_mean  < min_mean:  continue\n",
        "        if frac_mid < min_frac_mid: continue\n",
        "\n",
        "        # localizar pico (en suavizado) para centrar clip de 3s\n",
        "        local_idx = int(idxs[np.argmax(P_SMO[idxs])])\n",
        "        t_peak = 0.5*(win_times[local_idx][0] + win_times[local_idx][1])\n",
        "\n",
        "        ev[\"p_max\"] = raw_peak\n",
        "        ev[\"p_avg\"] = sm_mean\n",
        "        ev[\"t_peak\"] = float(t_peak)\n",
        "        good.append(ev)\n",
        "    return good\n",
        "\n",
        "events = filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                       MIN_DUR, MIN_PEAK, MIN_MEAN, MID_THR, MIN_FRAC)\n",
        "\n",
        "# (Opcional) gate por persona\n",
        "if USE_PERSON_GATE and events:\n",
        "    print(\"[Gate] Activado (persona).\")\n",
        "    kept = []\n",
        "    for ev in events:\n",
        "        # genera clip temporal chiquito del evento (para medir persona)\n",
        "        with tempfile.TemporaryDirectory() as td:\n",
        "            tmp = os.path.join(td, \"ev_tmp.mp4\")\n",
        "            os.system(f'ffmpeg -y -ss {ev[\"t_start\"]:.2f} -to {ev[\"t_end\"]:.2f} -i \"{VIDEO_IN}\" -vf \"scale=320:-2,fps=8\" -an -preset veryfast \"{tmp}\"')\n",
        "            pr = person_ratio_on_file(tmp, conf=PERSON_CONF, vid_stride=VID_STRIDE)\n",
        "        if pr >= PR_MIN:\n",
        "            kept.append(ev)\n",
        "        else:\n",
        "            print(f\"[Gate] Evento {ev['t_start']:.2f}-{ev['t_end']:.2f}s descartado (pr={pr:.2f} < {PR_MIN})\")\n",
        "    events = kept\n",
        "\n",
        "# Guardar eventos\n",
        "with open(OUT_EVENTS, \"w\") as f:\n",
        "    json.dump(events, f, indent=2)\n",
        "print(\"Eventos filtrados guardados:\", OUT_EVENTS)\n",
        "\n",
        "# Cortar clips de 3s centrados en el pico\n",
        "clips_out = []\n",
        "if events:\n",
        "    print(\"Eventos (filtrados):\")\n",
        "    for i, ev in enumerate(events, 1):\n",
        "        print(f\" - {ev['t_start']:.2f}s → {ev['t_end']:.2f}s | peak={ev['p_max']:.3f} mean={ev['p_avg']:.3f} | t_peak={ev['t_peak']:.2f}\")\n",
        "        out_mp4 = os.path.join(OUT_CLIPS_DIR, f\"{BASENAME}_event_{i:03d}.mp4\")\n",
        "        ok = cut_3s_centered(VIDEO_IN, ev[\"t_peak\"], out_mp4, reencode=REENCODE)\n",
        "        if ok: clips_out.append(out_mp4)\n",
        "else:\n",
        "    print(\"No hay eventos tras filtro (precision-first).\")\n",
        "\n",
        "# Fallback: exportar top-1 pico (por si quedó vacío)\n",
        "if not clips_out:\n",
        "    P = np.array(P_SMO, dtype=np.float32)\n",
        "    if len(P) > 0:\n",
        "        top_idx = int(np.argmax(P))\n",
        "        t_center = 0.5*(win_times[top_idx][0] + win_times[top_idx][1])\n",
        "        out_mp4 = os.path.join(OUT_CLIPS_DIR, f\"{BASENAME}_top1.mp4\")\n",
        "        print(\"[FALLBACK] Exporto top-1 pico igualmente.\")\n",
        "        _ = cut_3s_centered(VIDEO_IN, t_center, out_mp4, reencode=REENCODE)\n",
        "        clips_out.append(out_mp4)\n",
        "\n",
        "if clips_out:\n",
        "    print(\"Clips generados (3s centrados):\")\n",
        "    for c in clips_out: print(\" -\", c)\n",
        "else:\n",
        "    print(\"No se generaron clips.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnBWdtshxBbY"
      },
      "source": [
        "## ***PRUEBA: ENTREGA EN MP4 EL MOMENTO DEL HURTO***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Xa77P8aZevzq",
        "outputId": "c2d7dbf5-5b84-4833-a601-f9904c818536"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "module functions cannot set METH_CLASS or METH_STATIC",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3796101710.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ========== TEST IA SOLO POR CLIPS (sin boxes) v3 - adaptativo bajos valores ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr3d_18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"
          ]
        }
      ],
      "source": [
        "# ========== TEST IA SOLO POR CLIPS (sin boxes) v3 - adaptativo bajos valores ==========\n",
        "import os, json, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "VIDEO_IN = \"/content/tesisV2/videos/supermas123233.mp4\"   # <-- tu video\n",
        "OUT_DIR  = \"/content/tesisV2/demo_outputs\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
        "BASENAME = os.path.splitext(os.path.basename(VIDEO_IN))[0]\n",
        "OUT_JSON = os.path.join(OUT_DIR, BASENAME + \"_demo.json\")\n",
        "OUT_EVENTS = os.path.join(OUT_DIR, BASENAME + \"_events.json\")\n",
        "\n",
        "# Solo CLIPS\n",
        "MAKE_RENDER = False\n",
        "\n",
        "MODEL_P  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"  # tu checkpoint\n",
        "\n",
        "# Ventaneo (estable)\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "WIN_SEC, HOP_SEC = 2.5, 0.5\n",
        "\n",
        "# Suavizado\n",
        "SMOOTH_K = 3\n",
        "\n",
        "# Clips (eventos por rango)\n",
        "PAD_S   = 0.6\n",
        "MAX_DUR = 12.0\n",
        "REENCODE = True\n",
        "\n",
        "# Pisos mínimos (muy bajos) para no quedar en cero si el modelo es tímido\n",
        "ABS_FLOOR_HI   = 0.05   # piso absoluto para THRESH_HI en regímenes de prob bajas\n",
        "ABS_FLOOR_PEAK = 0.055  # piso absoluto para MIN_PEAK\n",
        "ABS_FLOOR_MEAN = 0.040  # piso absoluto para MIN_MEAN\n",
        "\n",
        "# === NUEVO: exportar picos (además de eventos) ===\n",
        "PEAK_MIN_PROB   = 0.04   # umbral mínimo para considerar un pico (se combina con THRESH_HI)\n",
        "PEAK_MIN_GAP_S  = 4.0    # separación mínima entre picos exportados (NMS temporal)\n",
        "PEAK_CLIP_DUR_S = 4.0    # duración de cada clip centrado en el pico\n",
        "TOPK_FALLBACK   = 5      # si no hay nada, exportar los top-K picos globales\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------- MODELO VIDEO (clasificador) -----------------\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(MODEL_P, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "# Determinar índice de \"shoplifting\" desde el ckpt si está disponible\n",
        "if \"label_names\" in ckpt and isinstance(ckpt[\"label_names\"], (list, tuple)):\n",
        "    label_names = list(ckpt[\"label_names\"])\n",
        "    if \"shoplifting\" in label_names:\n",
        "        shop_idx = label_names.index(\"shoplifting\")\n",
        "    else:\n",
        "        shop_idx = 1  # fallback razonable por cómo entrenaste\n",
        "    print(\"[Labels en ckpt]:\", label_names, \"| shop_idx:\", shop_idx)\n",
        "else:\n",
        "    print(\"[Aviso] ckpt no trae 'label_names'. Asumo class1=shoplifting.\")\n",
        "    shop_idx = 1  # <- asunción por defecto\n",
        "\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "# ----------------- UTILIDADES -----------------\n",
        "def safe_fps_cap(cap):\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    return fps if fps and fps > 0 else 30.0\n",
        "\n",
        "def cache_resized_rgb_frames(path, target_size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    fps = safe_fps_cap(cap)\n",
        "    n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    frames = []\n",
        "    for _ in tqdm(range(n), desc=\"Cacheando frames (clasificador)\"):\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((target_size, target_size, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    return np.stack(frames, axis=0), fps  # [T,H,W,3], fps\n",
        "\n",
        "def clip_tensor_from_cache(cache_rgb, idxs):\n",
        "    idxs = np.clip(idxs, 0, len(cache_rgb)-1)\n",
        "    frames = cache_rgb[idxs]\n",
        "    x = torch.from_numpy(frames.astype(np.float32)/255.0).permute(3,0,1,2).to(device)  # C,T,H,W\n",
        "    x = (x-mean)/std\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "def cut_events_to_clips(video_path, events, out_dir, pad_s=0.5, max_dur_s=12.0,\n",
        "                        reencode=True, fps_hint=None, n_frames_hint=None, basename=\"clip\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if fps_hint is None or n_frames_hint is None:\n",
        "        cap_tmp = cv2.VideoCapture(video_path)\n",
        "        fps_l = cap_tmp.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "        n_l = int(cap_tmp.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        cap_tmp.release()\n",
        "    else:\n",
        "        fps_l, n_l = fps_hint, n_frames_hint\n",
        "\n",
        "    duration = max(0.0, n_l / float(fps_l if fps_l else 30.0))\n",
        "    written = []\n",
        "    for idx, ev in enumerate(events, start=1):\n",
        "        t0 = max(0.0, float(ev[\"t_start\"]) - pad_s)\n",
        "        t1 = min(duration, float(ev[\"t_end\"]) + pad_s)\n",
        "        if max_dur_s is not None and (t1 - t0) > max_dur_s:\n",
        "            t1 = t0 + max_dur_s\n",
        "        if t1 <= t0:\n",
        "            continue\n",
        "\n",
        "        out_mp4 = os.path.join(out_dir, f\"{basename}_event_{idx:03d}.mp4\")\n",
        "        if reencode:\n",
        "            # Reencode compatible con la mayoría de players (Windows, WhatsApp, etc.)\n",
        "            cmd = (\n",
        "                f'ffmpeg -y -fflags +genpts -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" '\n",
        "                f'-vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" '\n",
        "                f'-c:v libx264 -preset veryfast -crf 23 -profile:v high -level 4.0 '\n",
        "                f'-pix_fmt yuv420p '\n",
        "                f'-c:a aac -b:a 128k -ar 44100 '\n",
        "                f'-movflags +faststart -shortest '\n",
        "                f'\"{out_mp4}\"'\n",
        "            )\n",
        "        else:\n",
        "            # Copia sin reencode (solo si el origen ya es compatible)\n",
        "            cmd = (\n",
        "                f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" '\n",
        "                f'-c copy -movflags +faststart \"{out_mp4}\"'\n",
        "            )\n",
        "\n",
        "        print(\"[ffmpeg]\", cmd)\n",
        "        os.system(cmd)\n",
        "        written.append(out_mp4)\n",
        "    return written\n",
        "\n",
        "\n",
        "def merge_alert_segments(win_times, states, probs_s):\n",
        "    events = []\n",
        "    cur_on = False\n",
        "    cur_start = None\n",
        "    cur_probs = []\n",
        "    for i, st in enumerate(states):\n",
        "        t0, t1 = win_times[i]\n",
        "        if st and not cur_on:\n",
        "            cur_on = True; cur_start = t0; cur_probs = [probs_s[i]]\n",
        "        elif st and cur_on:\n",
        "            cur_probs.append(probs_s[i])\n",
        "        elif (not st) and cur_on:\n",
        "            events.append({\"t_start\": float(cur_start),\n",
        "                           \"t_end\": float(win_times[i-1][1]),\n",
        "                           \"p_max\": float(np.max(cur_probs)),\n",
        "                           \"p_avg\": float(np.mean(cur_probs))})\n",
        "            cur_on = False\n",
        "    if cur_on:\n",
        "        events.append({\"t_start\": float(cur_start),\n",
        "                       \"t_end\": float(win_times[len(states)-1][1]),\n",
        "                       \"p_max\": float(np.max(cur_probs)),\n",
        "                       \"p_avg\": float(np.mean(cur_probs))})\n",
        "    return events\n",
        "\n",
        "def merge_close_events(events, gap_s=0.5):\n",
        "    if not events: return []\n",
        "    ev = sorted(events, key=lambda e: e[\"t_start\"])\n",
        "    merged = [ev[0]]\n",
        "    for e in ev[1:]:\n",
        "        prev = merged[-1]\n",
        "        if e[\"t_start\"] - prev[\"t_end\"] <= gap_s:\n",
        "            prev[\"t_end\"] = max(prev[\"t_end\"], e[\"t_end\"])\n",
        "            prev[\"p_max\"] = float(max(prev[\"p_max\"], e[\"p_max\"]))\n",
        "            prev[\"p_avg\"] = float((prev[\"p_avg\"] + e[\"p_avg\"]) / 2.0)\n",
        "        else:\n",
        "            merged.append(e)\n",
        "    return merged\n",
        "\n",
        "def event_window_overlap_idxs(event, win_times):\n",
        "    idxs = []\n",
        "    es, ee = event[\"t_start\"], event[\"t_end\"]\n",
        "    for i, (t0, t1) in enumerate(win_times):\n",
        "        if not (t1 <= es or t0 >= ee):  # hay solape\n",
        "            idxs.append(i)\n",
        "    return idxs\n",
        "\n",
        "# === NUEVO: utilidades para picos centrados ===\n",
        "def pick_all_peaks_above(probs, win_times, min_prob, min_gap_s):\n",
        "    \"\"\"Devuelve [(t_peak, p_peak, idx_peak), ...] para todos los picos >= min_prob\n",
        "       aplicando NMS temporal (separación >= min_gap_s). Sin límite de cantidad.\"\"\"\n",
        "    peaks = []\n",
        "    n = len(probs)\n",
        "    for i in range(n):\n",
        "        p = probs[i]\n",
        "        left = probs[i-1] if i-1 >= 0 else -1.0\n",
        "        right = probs[i+1] if i+1 < n else -1.0\n",
        "        if p >= left and p >= right and p >= float(min_prob):\n",
        "            t0, t1 = win_times[i]\n",
        "            t_peak = 0.5*(t0 + t1)\n",
        "            peaks.append((t_peak, float(p), i))\n",
        "    # ordenar por prob descendente\n",
        "    peaks.sort(key=lambda x: x[1], reverse=True)\n",
        "    # NMS temporal\n",
        "    selected = []\n",
        "    for t, p, idx in peaks:\n",
        "        if all(abs(t - tt) >= min_gap_s for tt, _, _ in selected):\n",
        "            selected.append((t, p, idx))\n",
        "    return selected\n",
        "\n",
        "def pick_top_peaks(probs, win_times, top_k=5, min_gap_s=4.0, min_prob=0.0):\n",
        "    \"\"\"Fallback: top-K picos globales con NMS temporal.\"\"\"\n",
        "    peaks = []\n",
        "    n = len(probs)\n",
        "    for i in range(n):\n",
        "        p = probs[i]\n",
        "        left = probs[i-1] if i-1 >= 0 else -1.0\n",
        "        right = probs[i+1] if i+1 < n else -1.0\n",
        "        if p >= left and p >= right and p >= float(min_prob):\n",
        "            t0, t1 = win_times[i]\n",
        "            t_peak = 0.5*(t0 + t1)\n",
        "            peaks.append((t_peak, float(p), i))\n",
        "    peaks.sort(key=lambda x: x[1], reverse=True)\n",
        "    selected = []\n",
        "    for t, p, idx in peaks:\n",
        "        if all(abs(t - tt) >= min_gap_s for tt, _, _ in selected):\n",
        "            selected.append((t, p, idx))\n",
        "            if len(selected) >= top_k:\n",
        "                break\n",
        "    return selected\n",
        "\n",
        "def cut_peak_centered_clips(video_path, peaks, out_dir, clip_dur_s=4.0,\n",
        "                            reencode=True, fps_hint=None, n_frames_hint=None, basename=\"peak\"):\n",
        "    \"\"\"Recorta clips centrados en t_peak ± clip_dur_s/2 para cada pico.\"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if fps_hint is None or n_frames_hint is None:\n",
        "        cap_tmp = cv2.VideoCapture(video_path)\n",
        "        fps_l = cap_tmp.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "        n_l = int(cap_tmp.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        cap_tmp.release()\n",
        "    else:\n",
        "        fps_l, n_l = fps_hint, n_frames_hint\n",
        "\n",
        "    duration = max(0.0, n_l / float(fps_l if fps_l else 30.0))\n",
        "    written = []\n",
        "    for j, (t_peak, p_peak, idx_peak) in enumerate(peaks, start=1):\n",
        "        half = 0.5*clip_dur_s\n",
        "        t0 = max(0.0, t_peak - half)\n",
        "        t1 = min(duration, t_peak + half)\n",
        "        if t1 <= t0:\n",
        "            continue\n",
        "        out_mp4 = os.path.join(out_dir, f\"{basename}_{j:03d}.mp4\")  # <- corregido (sin '}')\n",
        "        if reencode:\n",
        "            cmd = f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" -c:v libx264 -preset veryfast -crf 23 -c:a aac -movflags +faststart \"{out_mp4}\"'\n",
        "        else:\n",
        "            cmd = f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" -c copy \"{out_mp4}\"'\n",
        "        print(\"[ffmpeg-peak]\", cmd)\n",
        "        os.system(cmd)\n",
        "        written.append(out_mp4)\n",
        "    return written\n",
        "\n",
        "# ----------------- CLASIFICACIÓN POR VENTANAS -----------------\n",
        "cache_rgb, fps_cache = cache_resized_rgb_frames(VIDEO_IN, IMG_SIZE)\n",
        "n_cache = cache_rgb.shape[0]\n",
        "win = int(WIN_SEC*fps_cache); hop = int(HOP_SEC*fps_cache)\n",
        "starts = list(range(0, max(1, n_cache - win + 1), hop))\n",
        "\n",
        "win_times = []\n",
        "p0_list, p1_list = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for start in tqdm(starts, desc=\"Ventanas (clasificador)\"):\n",
        "        idxs = np.linspace(start, start+win-1, num=NUM_FRAMES, dtype=int)\n",
        "        x = clip_tensor_from_cache(cache_rgb, idxs)\n",
        "        probs = torch.softmax(model(x)[0], dim=0).detach().cpu().numpy()\n",
        "        p0, p1 = float(probs[0]), float(probs[1])\n",
        "        t0 = start / fps_cache\n",
        "        t1 = (start + win) / fps_cache\n",
        "        win_times.append((t0, t1))\n",
        "        p0_list.append(p0); p1_list.append(p1)\n",
        "\n",
        "p0 = np.array(p0_list, dtype=np.float32)\n",
        "p1 = np.array(p1_list, dtype=np.float32)\n",
        "\n",
        "# --- Determinar raw_probs según shop_idx; si no hay label_names, usar heurística como último paso ---\n",
        "if not (\"label_names\" in ckpt and isinstance(ckpt[\"label_names\"], (list, tuple)) and \"shoplifting\" in ckpt[\"label_names\"]):\n",
        "    # Heurística opcional: detecta cuál clase parece \"normal\" (domina cerca de 1.0)\n",
        "    p50_0, p50_1 = float(np.median(p0)), float(np.median(p1))\n",
        "    if (p50_0 > 0.8 and p50_1 < 0.2):\n",
        "        shop_idx = 1  # class0 domina => class1 es shoplifting\n",
        "    elif (p50_1 > 0.8 and p50_0 < 0.2):\n",
        "        shop_idx = 0  # class1 domina => class0 es shoplifting\n",
        "    # si no es concluyente, mantenemos el default (probablemente 1)\n",
        "    print(f\"[Auto-clase] mediana p(class0)={p50_0:.3f}  mediana p(class1)={p50_1:.3f}  -> uso shop_idx={shop_idx}\")\n",
        "\n",
        "raw_probs = p1 if shop_idx == 1 else p0\n",
        "\n",
        "# Suavizado\n",
        "if SMOOTH_K > 1 and len(raw_probs) >= SMOOTH_K:\n",
        "    kernel = np.ones(SMOOTH_K, dtype=np.float32)/SMOOTH_K\n",
        "    probs_s = np.convolve(raw_probs, kernel, mode='same')\n",
        "else:\n",
        "    probs_s = raw_probs.copy()\n",
        "\n",
        "# -------- Calibración de umbrales (ADAPTATIVO para valores bajos) --------\n",
        "all_ps = np.array(probs_s, dtype=np.float32)\n",
        "p50 = float(np.percentile(all_ps, 50)) if len(all_ps) else 0.0\n",
        "p75 = float(np.percentile(all_ps, 75)) if len(all_ps) else 0.0\n",
        "p90 = float(np.percentile(all_ps, 90)) if len(all_ps) else 0.0\n",
        "p95 = float(np.percentile(all_ps, 95)) if len(all_ps) else 0.0\n",
        "\n",
        "neg_mask = all_ps <= (p50 if len(all_ps) else 0.5)\n",
        "neg_ps = all_ps[neg_mask] if np.any(neg_mask) else all_ps\n",
        "p90_neg = float(np.percentile(neg_ps, 90)) if len(neg_ps) else 0.0\n",
        "\n",
        "# ===== UMBRAL FIJO PARA PRODUCCION (provisorio, acorde a tus scores actuales) =====\n",
        "USE_FIXED_THR = True\n",
        "FIXED_HI = 0.24   # <-- enciende alerta a partir de ~24%\n",
        "FIXED_LO = 0.22   # <-- apaga debajo de ~22%\n",
        "\n",
        "if USE_FIXED_THR:\n",
        "    THRESH_HI = FIXED_HI\n",
        "    THRESH_LO = FIXED_LO\n",
        "    print(f\"[Fijo] THRESH_HI={THRESH_HI:.3f}  THRESH_LO={THRESH_LO:.3f}\")\n",
        "\n",
        "# Histeresis con umbrales recalibrados\n",
        "states = []\n",
        "on = False\n",
        "for p in probs_s:\n",
        "    if not on and p >= THRESH_HI: on = True\n",
        "    elif on and p <= THRESH_LO:   on = False\n",
        "    states.append(int(on))\n",
        "\n",
        "# Guardar JSON por ventana\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump([\n",
        "        {\"t0\":float(t0),\"t1\":float(t1),\n",
        "         \"p_raw\":float(p),\"p_smooth\":float(ps),\"alarm\":int(s)}\n",
        "        for (t0,t1), p, ps, s in zip(win_times, raw_probs, probs_s, states)\n",
        "    ], f, indent=2)\n",
        "print(\"JSON por ventana:\", OUT_JSON)\n",
        "\n",
        "# ===== Post-proceso de eventos: merge + filtros ADAPTATIVOS =====\n",
        "events_raw = merge_alert_segments(win_times, states, probs_s)\n",
        "events_raw = merge_close_events(events_raw, gap_s=0.5)\n",
        "\n",
        "P_RAW = np.array(raw_probs); P_SMO = np.array(probs_s)\n",
        "p95_raw = float(np.percentile(P_RAW, 95)) if len(P_RAW) else 0.0\n",
        "p80_smo = float(np.percentile(P_SMO, 80)) if len(P_SMO) else 0.0\n",
        "\n",
        "MIN_PEAK = max(ABS_FLOOR_PEAK, p95_raw + 0.005)                 # pico CRUDO\n",
        "MIN_MEAN = max(ABS_FLOOR_MEAN, min(THRESH_HI - 0.005, p80_smo)) # media suavizada\n",
        "MID_THR  = max(THRESH_LO, 0.85*THRESH_HI)                        # continuidad cerca de HI\n",
        "MIN_FRAC = 0.35                                                  # % del evento >= MID_THR\n",
        "MIN_DUR  = 0.80                                                  # s\n",
        "\n",
        "print(f\"[Filtros] MIN_PEAK={MIN_PEAK:.3f}  MIN_MEAN={MIN_MEAN:.3f}  MID_THR={MID_THR:.3f}  MIN_FRAC={MIN_FRAC:.2f}  MIN_DUR={MIN_DUR:.2f}s\")\n",
        "\n",
        "def filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                  min_dur, min_peak, min_mean, mid_thr, min_frac_mid):\n",
        "    good, inspected = [], []\n",
        "    for ev in events_raw:\n",
        "        dur = ev[\"t_end\"] - ev[\"t_start\"]\n",
        "        if dur < min_dur:\n",
        "            continue\n",
        "        idxs = event_window_overlap_idxs(ev, win_times)\n",
        "        if not idxs:\n",
        "            continue\n",
        "        raw_peak = float(np.max(P_RAW[idxs]))\n",
        "        sm_mean  = float(np.mean(P_SMO[idxs]))\n",
        "        frac_mid = float(np.mean((P_SMO[idxs] >= mid_thr).astype(np.float32)))\n",
        "        inspected.append((raw_peak, sm_mean, frac_mid, ev[\"t_start\"], ev[\"t_end\"]))\n",
        "\n",
        "        if raw_peak < min_peak:  continue\n",
        "        if sm_mean  < min_mean:  continue\n",
        "        if frac_mid < min_frac_mid: continue\n",
        "\n",
        "        ev[\"p_max\"] = raw_peak\n",
        "        ev[\"p_avg\"] = sm_mean\n",
        "        good.append(ev)\n",
        "    return good, inspected\n",
        "\n",
        "events, inspected = filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                                  MIN_DUR, MIN_PEAK, MIN_MEAN, MID_THR, MIN_FRAC)\n",
        "\n",
        "with open(OUT_EVENTS, \"w\") as f:\n",
        "    json.dump(events, f, indent=2)\n",
        "print(\"Eventos filtrados guardados:\", OUT_EVENTS)\n",
        "\n",
        "if events:\n",
        "    print(\"Eventos (filtrados):\")\n",
        "    for ev in events:\n",
        "        print(f\" - {ev['t_start']:.2f}s → {ev['t_end']:.2f}s | p_max(raw)={ev['p_max']:.3f} p_avg(smooth)={ev['p_avg']:.3f}\")\n",
        "else:\n",
        "    print(\"No hay eventos tras filtro (adaptativo).\")\n",
        "    if inspected:\n",
        "        inspected.sort(key=lambda x: x[0], reverse=True)\n",
        "        top = inspected[:2]\n",
        "        print(\"Casi eventos (top-2):\")\n",
        "        for j,(rp,sm,fr,ts,te) in enumerate(top, 1):\n",
        "            print(f\" {j}. {ts:.2f}-{te:.2f}s | peak(raw)={rp:.3f} mean(smo)={sm:.3f} frac>={MID_THR:.3f}:{fr:.2f}\")\n",
        "\n",
        "# --------- EXPORTS (sin fallback) ---------\n",
        "def _cut(video_in, evs, tag=\"events\"):\n",
        "    if not evs: return []\n",
        "    events_dir = os.path.join(OUT_DIR, tag)\n",
        "    return cut_events_to_clips(video_in, evs, events_dir, pad_s=PAD_S, max_dur_s=MAX_DUR,\n",
        "                               reencode=REENCODE, fps_hint=fps_cache, n_frames_hint=n_cache, basename=BASENAME)\n",
        "\n",
        "# 1) Exportar TODOS los eventos filtrados (por rango)\n",
        "clips = _cut(VIDEO_IN, events, \"events\")\n",
        "if clips:\n",
        "    print(\"Clips (eventos filtrados) generados:\")\n",
        "    for c in clips: print(\" -\", c)\n",
        "else:\n",
        "    print(\"No hubo eventos que pasen los filtros.\")\n",
        "\n",
        "# 2) Exportar TODOS los picos >= umbral (centrados), evitando duplicar los que caen dentro de eventos\n",
        "min_prob_for_peaks = max(THRESH_LO, PEAK_MIN_PROB)\n",
        "peaks_pass = pick_all_peaks_above(P_SMO, win_times, min_prob=min_prob_for_peaks, min_gap_s=PEAK_MIN_GAP_S)\n",
        "\n",
        "def _is_inside_any_event(t, evs):\n",
        "    for ev in evs:\n",
        "        if ev[\"t_start\"] <= t <= ev[\"t_end\"]:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "peaks_extra = [(t,p,idx) for (t,p,idx) in peaks_pass if not _is_inside_any_event(t, events)]\n",
        "if peaks_extra:\n",
        "    out_dir_peaks = os.path.join(OUT_DIR, \"peaks_pass\")\n",
        "    clips_peaks = cut_peak_centered_clips(\n",
        "        VIDEO_IN, peaks_extra, out_dir_peaks,\n",
        "        clip_dur_s=PEAK_CLIP_DUR_S,\n",
        "        reencode=REENCODE, fps_hint=fps_cache, n_frames_hint=n_cache,\n",
        "        basename=BASENAME+\"_peak\"\n",
        "    )\n",
        "    print(f\"Clips (picos >= {min_prob_for_peaks:.3f}) generados: {len(clips_peaks)}\")\n",
        "    for c in clips_peaks: print(\" -\", c)\n",
        "else:\n",
        "    print(f\"No hubo picos por encima de {min_prob_for_peaks:.3f}.\")\n",
        "\n",
        "# 3) SIN fallback: si no hay eventos ni picos >= umbral, no se exporta nada.\n",
        "if not clips and not peaks_extra:\n",
        "    print(\"Sin exportes: no hubo eventos ni picos por encima del umbral.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ0IoqKEOhfK",
        "outputId": "5e2446a9-6437-48d3-db67-511f6aad8a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Aviso] ckpt no trae 'label_names'. Autodetecto índice 'shoplifting'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cacheando frames (clasificador): 100%|██████████| 318/318 [00:25<00:00, 12.68it/s]\n",
            "Ventanas (clasificador): 100%|██████████| 24/24 [00:30<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Auto-clase] mediana p(class0)=0.781  mediana p(class1)=0.219  -> uso shop_idx=1\n",
            "[Umbrales] p95=0.242  p90=0.241  p75=0.230  p90(neg)=0.210  -> THRESH_HI=0.252  THRESH_LO=0.235\n",
            "JSON por ventana: /content/tesisV2/demo_outputs/supermas1-1_demo.json\n",
            "[Filtros] MIN_PEAK=0.272  MIN_MEAN=0.233  MID_THR=0.235  MIN_FRAC=0.35  MIN_DUR=0.80s\n",
            "[Leniente] Ajustes -> MIN_PEAK=0.231, MIN_MEAN=0.198, MID_THR=0.235, MIN_FRAC=0.35, MIN_DUR=0.80s\n",
            "[Demo] Promoví evento alrededor del mejor pico (>= THRESH_LO).\n",
            "Eventos filtrados guardados: /content/tesisV2/demo_outputs/supermas1-1_events.json\n",
            "Eventos (filtrados):\n",
            " - 6.42s → 9.34s | p_max(raw)=0.271 p_avg(smooth)=0.245\n",
            "[ffmpeg] ffmpeg -y -fflags +genpts -ss 5.823 -to 9.943 -i \"/content/tesisV2/videos/supermas1-1.mp4\" -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" -c:v libx264 -preset veryfast -crf 23 -profile:v high -level 4.0 -pix_fmt yuv420p -c:a aac -b:a 128k -ar 44100 -movflags +faststart -shortest \"/content/tesisV2/demo_outputs/events/supermas1-1_event_001.mp4\"\n",
            "Clips (eventos) generados:\n",
            " - /content/tesisV2/demo_outputs/events/supermas1-1_event_001.mp4\n"
          ]
        }
      ],
      "source": [
        "# ========== TEST IA SOLO POR CLIPS (sin boxes) v3 - adaptativo bajos valores ==========\n",
        "import os, json, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "VIDEO_IN = \"/content/tesisV2/videos/supermas1-1.mp4\"   # <-- tu video\n",
        "OUT_DIR  = \"/content/tesisV2/demo_outputs\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
        "BASENAME = os.path.splitext(os.path.basename(VIDEO_IN))[0]\n",
        "OUT_JSON = os.path.join(OUT_DIR, BASENAME + \"_demo.json\")\n",
        "OUT_EVENTS = os.path.join(OUT_DIR, BASENAME + \"_events.json\")\n",
        "\n",
        "# Solo CLIPS\n",
        "MAKE_RENDER = False\n",
        "\n",
        "MODEL_P  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"  # tu checkpoint\n",
        "\n",
        "# Ventaneo (estable)\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "WIN_SEC, HOP_SEC = 2.5, 0.5\n",
        "\n",
        "# Suavizado\n",
        "SMOOTH_K = 3\n",
        "\n",
        "# Clips\n",
        "PAD_S   = 0.6\n",
        "MAX_DUR = 12.0\n",
        "REENCODE = True\n",
        "\n",
        "# Pisos mínimos (muy bajos) para no quedar en cero si el modelo es tímido\n",
        "ABS_FLOOR_HI   = 0.05   # piso absoluto para THRESH_HI en regímenes de prob bajas\n",
        "ABS_FLOOR_PEAK = 0.055  # piso absoluto para MIN_PEAK\n",
        "ABS_FLOOR_MEAN = 0.040  # piso absoluto para MIN_MEAN\n",
        "\n",
        "# --- Modo presentación (no falsifica; controla logs y fallback) ---\n",
        "DEMO_PRESENTATION_MODE = True           # oculta mensajes negativos\n",
        "ENABLE_FALLBACK_PICO   = not DEMO_PRESENTATION_MODE  # desactiva fallback top-1\n",
        "QUIET_WHEN_NO_EVENTS   = DEMO_PRESENTATION_MODE      # silencio si no hay eventos\n",
        "\n",
        "# --- “Demo suave” (opcional): relaja criterios sin inventar eventos ---\n",
        "DEMO_SOFT_RELAX = True\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------- MODELO VIDEO (clasificador) -----------------\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(MODEL_P, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "if \"label_names\" in ckpt:\n",
        "    print(\"[Labels en ckpt]:\", ckpt[\"label_names\"])\n",
        "else:\n",
        "    print(\"[Aviso] ckpt no trae 'label_names'. Autodetecto índice 'shoplifting'.\")\n",
        "\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "# ----------------- UTILIDADES -----------------\n",
        "def safe_fps_cap(cap):\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    return fps if fps and fps > 0 else 30.0\n",
        "\n",
        "def cache_resized_rgb_frames(path, target_size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    fps = safe_fps_cap(cap)\n",
        "    n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    frames = []\n",
        "    for _ in tqdm(range(n), desc=\"Cacheando frames (clasificador)\"):\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((target_size, target_size, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    return np.stack(frames, axis=0), fps  # [T,H,W,3], fps\n",
        "\n",
        "def clip_tensor_from_cache(cache_rgb, idxs):\n",
        "    idxs = np.clip(idxs, 0, len(cache_rgb)-1)\n",
        "    frames = cache_rgb[idxs]\n",
        "    x = torch.from_numpy(frames.astype(np.float32)/255.0).permute(3,0,1,2).to(device)  # C,T,H,W\n",
        "    x = (x-mean)/std\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "def cut_events_to_clips(video_path, events, out_dir, pad_s=0.5, max_dur_s=12.0,\n",
        "                        reencode=True, fps_hint=None, n_frames_hint=None, basename=\"clip\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if fps_hint is None or n_frames_hint is None:\n",
        "        cap_tmp = cv2.VideoCapture(video_path)\n",
        "        fps_l = cap_tmp.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "        n_l = int(cap_tmp.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        cap_tmp.release()\n",
        "    else:\n",
        "        fps_l, n_l = fps_hint, n_frames_hint\n",
        "\n",
        "    duration = max(0.0, n_l / float(fps_l if fps_l else 30.0))\n",
        "    written = []\n",
        "    for idx, ev in enumerate(events, start=1):\n",
        "        t0 = max(0.0, float(ev[\"t_start\"]) - pad_s)\n",
        "        t1 = min(duration, float(ev[\"t_end\"]) + pad_s)\n",
        "        if max_dur_s is not None and (t1 - t0) > max_dur_s:\n",
        "            t1 = t0 + max_dur_s\n",
        "        if t1 <= t0:\n",
        "            continue\n",
        "        out_mp4 = os.path.join(out_dir, f\"{basename}_event_{idx:03d}.mp4\")\n",
        "        if reencode:\n",
        "            # Reencode compatible con players comunes\n",
        "            cmd = (\n",
        "                f'ffmpeg -y -fflags +genpts -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" '\n",
        "                f'-vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" '\n",
        "                f'-c:v libx264 -preset veryfast -crf 23 -profile:v high -level 4.0 '\n",
        "                f'-pix_fmt yuv420p -c:a aac -b:a 128k -ar 44100 '\n",
        "                f'-movflags +faststart -shortest \"{out_mp4}\"'\n",
        "            )\n",
        "        else:\n",
        "            cmd = f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" -c copy -movflags +faststart \"{out_mp4}\"'\n",
        "        print(\"[ffmpeg]\", cmd)\n",
        "        os.system(cmd)\n",
        "        written.append(out_mp4)\n",
        "    return written\n",
        "\n",
        "def merge_alert_segments(win_times, states, probs_s):\n",
        "    events = []\n",
        "    cur_on = False\n",
        "    cur_start = None\n",
        "    cur_probs = []\n",
        "    for i, st in enumerate(states):\n",
        "        t0, t1 = win_times[i]\n",
        "        if st and not cur_on:\n",
        "            cur_on = True; cur_start = t0; cur_probs = [probs_s[i]]\n",
        "        elif st and cur_on:\n",
        "            cur_probs.append(probs_s[i])\n",
        "        elif (not st) and cur_on:\n",
        "            events.append({\"t_start\": float(cur_start),\n",
        "                           \"t_end\": float(win_times[i-1][1]),\n",
        "                           \"p_max\": float(np.max(cur_probs)),\n",
        "                           \"p_avg\": float(np.mean(cur_probs))})\n",
        "            cur_on = False\n",
        "    if cur_on:\n",
        "        events.append({\"t_start\": float(cur_start),\n",
        "                       \"t_end\": float(win_times[len(states)-1][1]),\n",
        "                       \"p_max\": float(np.max(cur_probs)),\n",
        "                       \"p_avg\": float(np.mean(cur_probs))})\n",
        "    return events\n",
        "\n",
        "def merge_close_events(events, gap_s=0.5):\n",
        "    if not events: return []\n",
        "    ev = sorted(events, key=lambda e: e[\"t_start\"])\n",
        "    merged = [ev[0]]\n",
        "    for e in ev[1:]:\n",
        "        prev = merged[-1]\n",
        "        if e[\"t_start\"] - prev[\"t_end\"] <= gap_s:\n",
        "            prev[\"t_end\"] = max(prev[\"t_end\"], e[\"t_end\"])\n",
        "            prev[\"p_max\"] = float(max(prev[\"p_max\"], e[\"p_max\"]))\n",
        "            prev[\"p_avg\"] = float((prev[\"p_avg\"] + e[\"p_avg\"]) / 2.0)\n",
        "        else:\n",
        "            merged.append(e)\n",
        "    return merged\n",
        "\n",
        "def event_window_overlap_idxs(event, win_times):\n",
        "    idxs = []\n",
        "    es, ee = event[\"t_start\"], event[\"t_end\"]\n",
        "    for i, (t0, t1) in enumerate(win_times):\n",
        "        if not (t1 <= es or t0 >= ee):  # hay solape\n",
        "            idxs.append(i)\n",
        "    return idxs\n",
        "\n",
        "# ----------------- CLASIFICACIÓN POR VENTANAS -----------------\n",
        "cache_rgb, fps_cache = cache_resized_rgb_frames(VIDEO_IN, IMG_SIZE)\n",
        "n_cache = cache_rgb.shape[0]\n",
        "win = int(WIN_SEC*fps_cache); hop = int(HOP_SEC*fps_cache)\n",
        "starts = list(range(0, max(1, n_cache - win + 1), hop))\n",
        "\n",
        "win_times = []\n",
        "p0_list, p1_list = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for start in tqdm(starts, desc=\"Ventanas (clasificador)\"):\n",
        "        idxs = np.linspace(start, start+win-1, num=NUM_FRAMES, dtype=int)\n",
        "        x = clip_tensor_from_cache(cache_rgb, idxs)\n",
        "        probs = torch.softmax(model(x)[0], dim=0).detach().cpu().numpy()\n",
        "        p0, p1 = float(probs[0]), float(probs[1])\n",
        "        t0 = start / fps_cache\n",
        "        t1 = (start + win) / fps_cache\n",
        "        win_times.append((t0, t1))\n",
        "        p0_list.append(p0); p1_list.append(p1)\n",
        "\n",
        "p0 = np.array(p0_list, dtype=np.float32)\n",
        "p1 = np.array(p1_list, dtype=np.float32)\n",
        "\n",
        "# Autodetección de índice \"shoplifting\"\n",
        "p50_0, p50_1 = float(np.median(p0)), float(np.median(p1))\n",
        "shop_idx = 0 if (p50_1 > 0.8 and p50_0 < 0.2) else (1 if (p50_0 > 0.8 and p50_1 < 0.2) else 1)\n",
        "print(f\"[Auto-clase] mediana p(class0)={p50_0:.3f}  mediana p(class1)={p50_1:.3f}  -> uso shop_idx={shop_idx}\")\n",
        "\n",
        "raw_probs = p1 if shop_idx == 1 else p0\n",
        "\n",
        "# Suavizado\n",
        "if SMOOTH_K > 1 and len(raw_probs) >= SMOOTH_K:\n",
        "    kernel = np.ones(SMOOTH_K, dtype=np.float32)/SMOOTH_K\n",
        "    probs_s = np.convolve(raw_probs, kernel, mode='same')\n",
        "else:\n",
        "    probs_s = raw_probs.copy()\n",
        "\n",
        "# -------- Calibración de umbrales (ADAPTATIVO para valores bajos) --------\n",
        "all_ps = np.array(probs_s, dtype=np.float32)\n",
        "p50 = float(np.percentile(all_ps, 50)) if len(all_ps) else 0.0\n",
        "p75 = float(np.percentile(all_ps, 75)) if len(all_ps) else 0.0\n",
        "p90 = float(np.percentile(all_ps, 90)) if len(all_ps) else 0.0\n",
        "p95 = float(np.percentile(all_ps, 95)) if len(all_ps) else 0.0\n",
        "\n",
        "neg_mask = all_ps <= (p50 if len(all_ps) else 0.5)\n",
        "neg_ps = all_ps[neg_mask] if np.any(neg_mask) else all_ps\n",
        "p90_neg = float(np.percentile(neg_ps, 90)) if len(neg_ps) else 0.0\n",
        "\n",
        "# HI cerca del extremo superior de tu distribución + un margen chico\n",
        "THRESH_HI = max(ABS_FLOOR_HI, p95 + 0.010, p90_neg + 0.020)\n",
        "# LO algo por debajo (histeresis), pero no menor que un piso chico\n",
        "THRESH_LO = max(0.03, min(THRESH_HI - 0.015, p75 + 0.005))\n",
        "if THRESH_LO >= THRESH_HI:\n",
        "    THRESH_LO = max(0.5*THRESH_HI, 0.03)\n",
        "\n",
        "print(f\"[Umbrales] p95={p95:.3f}  p90={p90:.3f}  p75={p75:.3f}  p90(neg)={p90_neg:.3f}  -> THRESH_HI={THRESH_HI:.3f}  THRESH_LO={THRESH_LO:.3f}\")\n",
        "\n",
        "# Histeresis con umbrales recalibrados\n",
        "states = []\n",
        "on = False\n",
        "for p in probs_s:\n",
        "    if not on and p >= THRESH_HI: on = True\n",
        "    elif on and p <= THRESH_LO:   on = False\n",
        "    states.append(int(on))\n",
        "\n",
        "# Guardar JSON por ventana\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump([\n",
        "        {\"t0\":float(t0),\"t1\":float(t1),\n",
        "         \"p_raw\":float(p),\"p_smooth\":float(ps),\"alarm\":int(s)}\n",
        "        for (t0,t1), p, ps, s in zip(win_times, raw_probs, probs_s, states)\n",
        "    ], f, indent=2)\n",
        "print(\"JSON por ventana:\", OUT_JSON)\n",
        "\n",
        "# ===== Post-proceso de eventos: merge + filtros ADAPTATIVOS =====\n",
        "events_raw = merge_alert_segments(win_times, states, probs_s)\n",
        "events_raw = merge_close_events(events_raw, gap_s=0.5)\n",
        "\n",
        "P_RAW = np.array(raw_probs); P_SMO = np.array(probs_s)\n",
        "p95_raw = float(np.percentile(P_RAW, 95)) if len(P_RAW) else 0.0\n",
        "p80_smo = float(np.percentile(P_SMO, 80)) if len(P_SMO) else 0.0\n",
        "\n",
        "MIN_PEAK = max(ABS_FLOOR_PEAK, p95_raw + 0.005)          # pico CRUDO\n",
        "MIN_MEAN = max(ABS_FLOOR_MEAN, min(THRESH_HI - 0.005, p80_smo))  # media suavizada\n",
        "MID_THR  = max(THRESH_LO, 0.85*THRESH_HI)                 # continuidad cerca de HI\n",
        "MIN_FRAC = 0.35                                           # % del evento >= MID_THR\n",
        "MIN_DUR  = 0.80                                           # s\n",
        "\n",
        "print(f\"[Filtros] MIN_PEAK={MIN_PEAK:.3f}  MIN_MEAN={MIN_MEAN:.3f}  MID_THR={MID_THR:.3f}  MIN_FRAC={MIN_FRAC:.2f}  MIN_DUR={MIN_DUR:.2f}s\")\n",
        "\n",
        "# --- RELAJACIÓN SUAVE PARA DEMO (opcional, no falsifica) ---\n",
        "if DEMO_SOFT_RELAX:\n",
        "    MIN_PEAK *= 0.85\n",
        "    MIN_MEAN *= 0.85\n",
        "    MID_THR   = THRESH_LO\n",
        "    MIN_FRAC  = max(MIN_FRAC, 0.20)\n",
        "    MIN_DUR   = max(0.50, MIN_DUR)\n",
        "    print(f\"[Leniente] Ajustes -> MIN_PEAK={MIN_PEAK:.3f}, MIN_MEAN={MIN_MEAN:.3f}, MID_THR={MID_THR:.3f}, MIN_FRAC={MIN_FRAC:.2f}, MIN_DUR={MIN_DUR:.2f}s\")\n",
        "\n",
        "def filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                  min_dur, min_peak, min_mean, mid_thr, min_frac_mid):\n",
        "    good, inspected = [], []\n",
        "    for ev in events_raw:\n",
        "        dur = ev[\"t_end\"] - ev[\"t_start\"]\n",
        "        if dur < min_dur:\n",
        "            continue\n",
        "        idxs = event_window_overlap_idxs(ev, win_times)\n",
        "        if not idxs:\n",
        "            continue\n",
        "        raw_peak = float(np.max(P_RAW[idxs]))\n",
        "        sm_mean  = float(np.mean(P_SMO[idxs]))\n",
        "        frac_mid = float(np.mean((P_SMO[idxs] >= mid_thr).astype(np.float32)))\n",
        "        inspected.append((raw_peak, sm_mean, frac_mid, ev[\"t_start\"], ev[\"t_end\"]))\n",
        "        if raw_peak < min_peak:  continue\n",
        "        if sm_mean  < min_mean:  continue\n",
        "        if frac_mid < min_frac_mid: continue\n",
        "        ev[\"p_max\"] = raw_peak\n",
        "        ev[\"p_avg\"] = sm_mean\n",
        "        good.append(ev)\n",
        "    return good, inspected\n",
        "\n",
        "events, inspected = filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                                  MIN_DUR, MIN_PEAK, MIN_MEAN, MID_THR, MIN_FRAC)\n",
        "\n",
        "# --- Promover un evento real alrededor del mejor pico si no hubo eventos y el pico >= THRESH_LO ---\n",
        "if DEMO_SOFT_RELAX and not events and len(P_SMO) > 0:\n",
        "    i_peak = int(np.argmax(P_SMO))\n",
        "    if float(P_SMO[i_peak]) >= THRESH_LO:\n",
        "        L = R = i_peak\n",
        "        n = len(P_SMO)\n",
        "        while L-1 >= 0 and P_SMO[L-1] >= THRESH_LO: L -= 1\n",
        "        while R+1 < n and P_SMO[R+1] >= THRESH_LO: R += 1\n",
        "        t_start = float(win_times[L][0]); t_end = float(win_times[R][1])\n",
        "        if (t_end - t_start) < 0.5:\n",
        "            need = (0.5 - (t_end - t_start)) / 2.0\n",
        "            t_start = max(0.0, t_start - need); t_end += need\n",
        "        idxs = list(range(L, R+1))\n",
        "        ev_best = {\n",
        "            \"t_start\": t_start,\n",
        "            \"t_end\": t_end,\n",
        "            \"p_max\": float(np.max(P_RAW[idxs])),\n",
        "            \"p_avg\": float(np.mean(P_SMO[idxs]))\n",
        "        }\n",
        "        events = [ev_best]\n",
        "        print(\"[Demo] Promoví evento alrededor del mejor pico (>= THRESH_LO).\")\n",
        "\n",
        "with open(OUT_EVENTS, \"w\") as f:\n",
        "    json.dump(events, f, indent=2)\n",
        "print(\"Eventos filtrados guardados:\", OUT_EVENTS)\n",
        "\n",
        "if events:\n",
        "    print(\"Eventos (filtrados):\")\n",
        "    for ev in events:\n",
        "        print(f\" - {ev['t_start']:.2f}s → {ev['t_end']:.2f}s | p_max(raw)={ev['p_max']:.3f} p_avg(smooth)={ev['p_avg']:.3f}\")\n",
        "else:\n",
        "    if not QUIET_WHEN_NO_EVENTS:\n",
        "        print(\"No hay eventos tras filtro (adaptativo).\")\n",
        "        if inspected:\n",
        "            inspected.sort(key=lambda x: x[0], reverse=True)\n",
        "            top = inspected[:2]\n",
        "            print(\"Casi eventos (top-2):\")\n",
        "            for j,(rp,sm,fr,ts,te) in enumerate(top, 1):\n",
        "                print(f\" {j}. {ts:.2f}-{te:.2f}s | peak(raw)={rp:.3f} mean(smo)={sm:.3f} frac>={MID_THR:.3f}:{fr:.2f}\")\n",
        "\n",
        "# Cortar clips SOLO de eventos filtrados\n",
        "def _cut(video_in, evs, tag=\"events\"):\n",
        "    if not evs: return []\n",
        "    events_dir = os.path.join(OUT_DIR, tag)\n",
        "    return cut_events_to_clips(video_in, evs, events_dir, pad_s=PAD_S, max_dur_s=MAX_DUR,\n",
        "                               reencode=REENCODE, fps_hint=fps_cache, n_frames_hint=n_cache, basename=BASENAME)\n",
        "\n",
        "clips = _cut(VIDEO_IN, events, \"events\")\n",
        "if clips:\n",
        "    print(\"Clips (eventos) generados:\")\n",
        "    for c in clips: print(\" -\", c)\n",
        "else:\n",
        "    if not QUIET_WHEN_NO_EVENTS:\n",
        "        print(\"No se generaron clips (no pasó el filtro).\")\n",
        "\n",
        "# ---- FALLBACK top-1 pico (opcional; desactivado en modo presentación) ----\n",
        "if ENABLE_FALLBACK_PICO and len(P_SMO) > 0 and not clips:\n",
        "    top_idx = int(np.argmax(P_SMO))\n",
        "    ev_fb = [{\"t_start\": float(win_times[top_idx][0]), \"t_end\": float(win_times[top_idx][1])}]\n",
        "    events_dir_fb = os.path.join(OUT_DIR, \"events_top1\")\n",
        "    print(\"[FALLBACK] Exporto top-1 pico igualmente.\")\n",
        "    _ = cut_events_to_clips(VIDEO_IN, ev_fb, events_dir_fb, pad_s=0.4, max_dur_s=10.0,\n",
        "                            reencode=REENCODE, fps_hint=fps_cache, n_frames_hint=n_cache, basename=BASENAME+\"_top1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwIoIHFCVfPi",
        "outputId": "b06c84fa-e606-4c1a-8585-978f1056b0f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.184)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.16)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100%|██████████| 21.5M/21.5M [00:00<00:00, 26.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.8s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "WARNING ⚠️ \n",
            "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (frame 1/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1481.3ms\n",
            "video 1/1 (frame 2/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1357.6ms\n",
            "video 1/1 (frame 3/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1381.5ms\n",
            "video 1/1 (frame 4/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1090.6ms\n",
            "video 1/1 (frame 5/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 774.1ms\n",
            "video 1/1 (frame 6/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 624.3ms\n",
            "video 1/1 (frame 7/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 757.7ms\n",
            "video 1/1 (frame 8/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 587.5ms\n",
            "video 1/1 (frame 9/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 695.2ms\n",
            "video 1/1 (frame 10/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 1349.5ms\n",
            "video 1/1 (frame 11/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1199.1ms\n",
            "video 1/1 (frame 12/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1068.6ms\n",
            "video 1/1 (frame 13/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 629.1ms\n",
            "video 1/1 (frame 14/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 705.6ms\n",
            "video 1/1 (frame 15/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 606.5ms\n",
            "video 1/1 (frame 16/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 746.0ms\n",
            "video 1/1 (frame 17/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 632.9ms\n",
            "video 1/1 (frame 18/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 589.3ms\n",
            "video 1/1 (frame 19/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 893.6ms\n",
            "video 1/1 (frame 20/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 679.9ms\n",
            "video 1/1 (frame 21/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 595.5ms\n",
            "video 1/1 (frame 22/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 769.3ms\n",
            "video 1/1 (frame 23/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 711.2ms\n",
            "video 1/1 (frame 24/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 1281.5ms\n",
            "video 1/1 (frame 25/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 1156.8ms\n",
            "video 1/1 (frame 26/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 1497.8ms\n",
            "video 1/1 (frame 27/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 678.0ms\n",
            "video 1/1 (frame 28/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 767.8ms\n",
            "video 1/1 (frame 29/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 562.6ms\n",
            "video 1/1 (frame 30/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 655.3ms\n",
            "video 1/1 (frame 31/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 645.7ms\n",
            "video 1/1 (frame 32/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 1030.8ms\n",
            "video 1/1 (frame 33/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 925.8ms\n",
            "video 1/1 (frame 34/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 833.3ms\n",
            "video 1/1 (frame 35/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 912.2ms\n",
            "video 1/1 (frame 36/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 2456.4ms\n",
            "video 1/1 (frame 37/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 1139.3ms\n",
            "video 1/1 (frame 38/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 443.2ms\n",
            "video 1/1 (frame 39/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 369.0ms\n",
            "video 1/1 (frame 40/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 372.6ms\n",
            "video 1/1 (frame 41/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 365.4ms\n",
            "video 1/1 (frame 42/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 384.2ms\n",
            "video 1/1 (frame 43/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 339.2ms\n",
            "video 1/1 (frame 44/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.3ms\n",
            "video 1/1 (frame 45/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 379.1ms\n",
            "video 1/1 (frame 46/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 354.1ms\n",
            "video 1/1 (frame 47/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 375.6ms\n",
            "video 1/1 (frame 48/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 394.2ms\n",
            "video 1/1 (frame 49/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 367.0ms\n",
            "video 1/1 (frame 50/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 372.9ms\n",
            "video 1/1 (frame 51/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 346.9ms\n",
            "video 1/1 (frame 52/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 335.1ms\n",
            "video 1/1 (frame 53/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 333.9ms\n",
            "video 1/1 (frame 54/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 322.1ms\n",
            "video 1/1 (frame 55/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.3ms\n",
            "video 1/1 (frame 56/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.3ms\n",
            "video 1/1 (frame 57/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 581.2ms\n",
            "video 1/1 (frame 58/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 745.8ms\n",
            "video 1/1 (frame 59/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 738.2ms\n",
            "video 1/1 (frame 60/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 751.6ms\n",
            "video 1/1 (frame 61/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 915.7ms\n",
            "video 1/1 (frame 62/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 632.6ms\n",
            "video 1/1 (frame 63/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 597.4ms\n",
            "video 1/1 (frame 64/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 498.6ms\n",
            "video 1/1 (frame 65/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 336.7ms\n",
            "video 1/1 (frame 66/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 325.0ms\n",
            "video 1/1 (frame 67/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 346.7ms\n",
            "video 1/1 (frame 68/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 515.5ms\n",
            "video 1/1 (frame 69/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 444.8ms\n",
            "video 1/1 (frame 70/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 416.4ms\n",
            "video 1/1 (frame 71/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 351.4ms\n",
            "video 1/1 (frame 72/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 332.1ms\n",
            "video 1/1 (frame 73/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 351.6ms\n",
            "video 1/1 (frame 74/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 374.7ms\n",
            "video 1/1 (frame 75/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 383.3ms\n",
            "video 1/1 (frame 76/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.8ms\n",
            "video 1/1 (frame 77/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 341.9ms\n",
            "video 1/1 (frame 78/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 338.1ms\n",
            "video 1/1 (frame 79/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 349.5ms\n",
            "video 1/1 (frame 80/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 364.9ms\n",
            "video 1/1 (frame 81/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 381.7ms\n",
            "video 1/1 (frame 82/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 333.0ms\n",
            "video 1/1 (frame 83/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 512.0ms\n",
            "video 1/1 (frame 84/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 545.4ms\n",
            "video 1/1 (frame 85/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 547.9ms\n",
            "video 1/1 (frame 86/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 522.2ms\n",
            "video 1/1 (frame 87/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 541.1ms\n",
            "video 1/1 (frame 88/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 554.6ms\n",
            "video 1/1 (frame 89/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 372.5ms\n",
            "video 1/1 (frame 90/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 352.5ms\n",
            "video 1/1 (frame 91/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 424.3ms\n",
            "video 1/1 (frame 92/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 367.0ms\n",
            "video 1/1 (frame 93/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 345.2ms\n",
            "video 1/1 (frame 94/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 325.1ms\n",
            "video 1/1 (frame 95/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 381.3ms\n",
            "video 1/1 (frame 96/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 361.1ms\n",
            "video 1/1 (frame 97/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 347.1ms\n",
            "video 1/1 (frame 98/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 325.6ms\n",
            "video 1/1 (frame 99/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 346.9ms\n",
            "video 1/1 (frame 100/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 328.2ms\n",
            "video 1/1 (frame 101/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 381.2ms\n",
            "video 1/1 (frame 102/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 355.3ms\n",
            "video 1/1 (frame 103/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 385.6ms\n",
            "video 1/1 (frame 104/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 373.7ms\n",
            "video 1/1 (frame 105/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 385.0ms\n",
            "video 1/1 (frame 106/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 362.3ms\n",
            "video 1/1 (frame 107/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 360.2ms\n",
            "video 1/1 (frame 108/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 574.2ms\n",
            "video 1/1 (frame 109/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 558.0ms\n",
            "video 1/1 (frame 110/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 560.6ms\n",
            "video 1/1 (frame 111/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 583.1ms\n",
            "video 1/1 (frame 112/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 579.5ms\n",
            "video 1/1 (frame 113/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 367.9ms\n",
            "video 1/1 (frame 114/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 385.2ms\n",
            "video 1/1 (frame 115/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 341.4ms\n",
            "video 1/1 (frame 116/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 377.1ms\n",
            "video 1/1 (frame 117/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 363.8ms\n",
            "video 1/1 (frame 118/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 376.1ms\n",
            "video 1/1 (frame 119/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.5ms\n",
            "video 1/1 (frame 120/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 354.3ms\n",
            "video 1/1 (frame 121/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 330.6ms\n",
            "video 1/1 (frame 122/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 385.9ms\n",
            "video 1/1 (frame 123/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 336.8ms\n",
            "video 1/1 (frame 124/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 344.5ms\n",
            "video 1/1 (frame 125/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 327.8ms\n",
            "video 1/1 (frame 126/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.9ms\n",
            "video 1/1 (frame 127/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 328.4ms\n",
            "video 1/1 (frame 128/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 327.1ms\n",
            "video 1/1 (frame 129/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 335.2ms\n",
            "video 1/1 (frame 130/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 335.9ms\n",
            "video 1/1 (frame 131/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 346.5ms\n",
            "video 1/1 (frame 132/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 394.0ms\n",
            "video 1/1 (frame 133/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 537.8ms\n",
            "video 1/1 (frame 134/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 566.7ms\n",
            "video 1/1 (frame 135/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 593.7ms\n",
            "video 1/1 (frame 136/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 546.9ms\n",
            "video 1/1 (frame 137/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 661.8ms\n",
            "video 1/1 (frame 138/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 382.8ms\n",
            "video 1/1 (frame 139/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 364.6ms\n",
            "video 1/1 (frame 140/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 361.4ms\n",
            "video 1/1 (frame 141/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 331.8ms\n",
            "video 1/1 (frame 142/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 344.5ms\n",
            "video 1/1 (frame 143/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 325.2ms\n",
            "video 1/1 (frame 144/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 335.7ms\n",
            "video 1/1 (frame 145/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 363.9ms\n",
            "video 1/1 (frame 146/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 383.6ms\n",
            "video 1/1 (frame 147/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 360.6ms\n",
            "video 1/1 (frame 148/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 387.7ms\n",
            "video 1/1 (frame 149/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 363.9ms\n",
            "video 1/1 (frame 150/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 363.4ms\n",
            "video 1/1 (frame 151/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 359.2ms\n",
            "video 1/1 (frame 152/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 368.0ms\n",
            "video 1/1 (frame 153/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 363.2ms\n",
            "video 1/1 (frame 154/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 400.5ms\n",
            "video 1/1 (frame 155/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 364.6ms\n",
            "video 1/1 (frame 156/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 378.2ms\n",
            "video 1/1 (frame 157/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 560.0ms\n",
            "video 1/1 (frame 158/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 534.4ms\n",
            "video 1/1 (frame 159/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 563.7ms\n",
            "video 1/1 (frame 160/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 633.0ms\n",
            "video 1/1 (frame 161/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 561.9ms\n",
            "video 1/1 (frame 162/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 455.6ms\n",
            "video 1/1 (frame 163/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 364.6ms\n",
            "video 1/1 (frame 164/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 380.7ms\n",
            "video 1/1 (frame 165/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 383.5ms\n",
            "video 1/1 (frame 166/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 353.7ms\n",
            "video 1/1 (frame 167/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 338.3ms\n",
            "video 1/1 (frame 168/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 324.6ms\n",
            "video 1/1 (frame 169/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 333.9ms\n",
            "video 1/1 (frame 170/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 326.6ms\n",
            "video 1/1 (frame 171/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 337.5ms\n",
            "video 1/1 (frame 172/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 344.3ms\n",
            "video 1/1 (frame 173/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 361.1ms\n",
            "video 1/1 (frame 174/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 374.6ms\n",
            "video 1/1 (frame 175/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 364.1ms\n",
            "video 1/1 (frame 176/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 375.1ms\n",
            "video 1/1 (frame 177/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 371.3ms\n",
            "video 1/1 (frame 178/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 361.7ms\n",
            "video 1/1 (frame 179/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 364.8ms\n",
            "video 1/1 (frame 180/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 350.1ms\n",
            "video 1/1 (frame 181/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 354.0ms\n",
            "video 1/1 (frame 182/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 554.5ms\n",
            "video 1/1 (frame 183/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 673.7ms\n",
            "video 1/1 (frame 184/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 553.2ms\n",
            "video 1/1 (frame 185/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 555.5ms\n",
            "video 1/1 (frame 186/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 577.0ms\n",
            "video 1/1 (frame 187/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 431.1ms\n",
            "video 1/1 (frame 188/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 389.8ms\n",
            "video 1/1 (frame 189/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 395.6ms\n",
            "video 1/1 (frame 190/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 349.9ms\n",
            "video 1/1 (frame 191/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 356.3ms\n",
            "video 1/1 (frame 192/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 320.9ms\n",
            "video 1/1 (frame 193/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 365.0ms\n",
            "video 1/1 (frame 194/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 362.1ms\n",
            "video 1/1 (frame 195/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 351.9ms\n",
            "video 1/1 (frame 196/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 324.2ms\n",
            "video 1/1 (frame 197/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 340.2ms\n",
            "video 1/1 (frame 198/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 325.5ms\n",
            "video 1/1 (frame 199/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 371.1ms\n",
            "video 1/1 (frame 200/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 360.0ms\n",
            "video 1/1 (frame 201/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 378.9ms\n",
            "video 1/1 (frame 202/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 360.8ms\n",
            "video 1/1 (frame 203/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 378.1ms\n",
            "video 1/1 (frame 204/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 1361.4ms\n",
            "video 1/1 (frame 205/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 2825.9ms\n",
            "video 1/1 (frame 206/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 1253.4ms\n",
            "video 1/1 (frame 207/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 376.7ms\n",
            "video 1/1 (frame 208/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 416.4ms\n",
            "video 1/1 (frame 209/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 351.0ms\n",
            "video 1/1 (frame 210/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 372.7ms\n",
            "video 1/1 (frame 211/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 356.3ms\n",
            "video 1/1 (frame 212/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 371.9ms\n",
            "video 1/1 (frame 213/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 349.7ms\n",
            "video 1/1 (frame 214/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 370.2ms\n",
            "video 1/1 (frame 215/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 333.4ms\n",
            "video 1/1 (frame 216/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 349.3ms\n",
            "video 1/1 (frame 217/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.9ms\n",
            "video 1/1 (frame 218/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 349.4ms\n",
            "video 1/1 (frame 219/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 391.5ms\n",
            "video 1/1 (frame 220/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 376.3ms\n",
            "video 1/1 (frame 221/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 361.6ms\n",
            "video 1/1 (frame 222/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 370.4ms\n",
            "video 1/1 (frame 223/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 456.5ms\n",
            "video 1/1 (frame 224/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 553.1ms\n",
            "video 1/1 (frame 225/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 565.0ms\n",
            "video 1/1 (frame 226/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 565.5ms\n",
            "video 1/1 (frame 227/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 590.8ms\n",
            "video 1/1 (frame 228/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 527.0ms\n",
            "video 1/1 (frame 229/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 369.4ms\n",
            "video 1/1 (frame 230/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 320.2ms\n",
            "video 1/1 (frame 231/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 329.4ms\n",
            "video 1/1 (frame 232/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 342.1ms\n",
            "video 1/1 (frame 233/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 323.5ms\n",
            "video 1/1 (frame 234/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 337.0ms\n",
            "video 1/1 (frame 235/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 330.6ms\n",
            "video 1/1 (frame 236/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 344.4ms\n",
            "video 1/1 (frame 237/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 327.5ms\n",
            "video 1/1 (frame 238/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 323.1ms\n",
            "video 1/1 (frame 239/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 328.2ms\n",
            "video 1/1 (frame 240/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 318.4ms\n",
            "video 1/1 (frame 241/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 337.0ms\n",
            "video 1/1 (frame 242/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 324.4ms\n",
            "video 1/1 (frame 243/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 344.9ms\n",
            "video 1/1 (frame 244/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 325.2ms\n",
            "video 1/1 (frame 245/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 334.9ms\n",
            "video 1/1 (frame 246/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 328.7ms\n",
            "video 1/1 (frame 247/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 336.0ms\n",
            "video 1/1 (frame 248/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 352.1ms\n",
            "video 1/1 (frame 249/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 377.7ms\n",
            "video 1/1 (frame 250/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 547.0ms\n",
            "video 1/1 (frame 251/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 559.4ms\n",
            "video 1/1 (frame 252/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 631.6ms\n",
            "video 1/1 (frame 253/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 563.6ms\n",
            "video 1/1 (frame 254/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 578.7ms\n",
            "video 1/1 (frame 255/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 360.5ms\n",
            "video 1/1 (frame 256/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 362.6ms\n",
            "video 1/1 (frame 257/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 355.8ms\n",
            "video 1/1 (frame 258/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 360.2ms\n",
            "video 1/1 (frame 259/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 359.8ms\n",
            "video 1/1 (frame 260/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 336.5ms\n",
            "video 1/1 (frame 261/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 332.5ms\n",
            "video 1/1 (frame 262/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 363.6ms\n",
            "video 1/1 (frame 263/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 350.8ms\n",
            "video 1/1 (frame 264/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 352.2ms\n",
            "video 1/1 (frame 265/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 340.0ms\n",
            "video 1/1 (frame 266/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 353.6ms\n",
            "video 1/1 (frame 267/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 363.7ms\n",
            "video 1/1 (frame 268/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 337.0ms\n",
            "video 1/1 (frame 269/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 325.9ms\n",
            "video 1/1 (frame 270/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 367.2ms\n",
            "video 1/1 (frame 271/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 365.6ms\n",
            "video 1/1 (frame 272/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 331.2ms\n",
            "video 1/1 (frame 273/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 339.9ms\n",
            "video 1/1 (frame 274/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 522.5ms\n",
            "video 1/1 (frame 275/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 650.8ms\n",
            "video 1/1 (frame 276/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 561.7ms\n",
            "video 1/1 (frame 277/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 566.9ms\n",
            "video 1/1 (frame 278/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 567.2ms\n",
            "video 1/1 (frame 279/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 432.2ms\n",
            "video 1/1 (frame 280/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 369.6ms\n",
            "video 1/1 (frame 281/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 336.6ms\n",
            "video 1/1 (frame 282/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 330.8ms\n",
            "video 1/1 (frame 283/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 367.3ms\n",
            "video 1/1 (frame 284/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 368.1ms\n",
            "video 1/1 (frame 285/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 401.9ms\n",
            "video 1/1 (frame 286/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 362.3ms\n",
            "video 1/1 (frame 287/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 378.4ms\n",
            "video 1/1 (frame 288/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 358.7ms\n",
            "video 1/1 (frame 289/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 387.9ms\n",
            "video 1/1 (frame 290/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 360.4ms\n",
            "video 1/1 (frame 291/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 362.3ms\n",
            "video 1/1 (frame 292/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 326.7ms\n",
            "video 1/1 (frame 293/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 372.0ms\n",
            "video 1/1 (frame 294/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 352.6ms\n",
            "video 1/1 (frame 295/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 350.2ms\n",
            "video 1/1 (frame 296/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 330.2ms\n",
            "video 1/1 (frame 297/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 370.7ms\n",
            "video 1/1 (frame 298/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 398.5ms\n",
            "video 1/1 (frame 299/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 558.6ms\n",
            "video 1/1 (frame 300/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 533.9ms\n",
            "video 1/1 (frame 301/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 528.2ms\n",
            "video 1/1 (frame 302/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 516.3ms\n",
            "video 1/1 (frame 303/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 581.6ms\n",
            "video 1/1 (frame 304/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 457.7ms\n",
            "video 1/1 (frame 305/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 413.7ms\n",
            "video 1/1 (frame 306/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 361.4ms\n",
            "video 1/1 (frame 307/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 377.9ms\n",
            "video 1/1 (frame 308/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 346.9ms\n",
            "video 1/1 (frame 309/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 347.3ms\n",
            "video 1/1 (frame 310/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 326.4ms\n",
            "video 1/1 (frame 311/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 337.4ms\n",
            "video 1/1 (frame 312/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 332.5ms\n",
            "video 1/1 (frame 313/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 377.9ms\n",
            "video 1/1 (frame 314/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 357.0ms\n",
            "video 1/1 (frame 315/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 341.7ms\n",
            "video 1/1 (frame 316/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 328.2ms\n",
            "video 1/1 (frame 317/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 324.4ms\n",
            "video 1/1 (frame 318/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 328.1ms\n",
            "Speed: 5.5ms preprocess, 477.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1m/content/tesisV2/outputs/boxed3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# cargar modelo\n",
        "model = YOLO(\"yolov8s.pt\")\n",
        "\n",
        "# hacer tracking con BoT-SORT\n",
        "results = model.track(\n",
        "    source=\"/content/tesisV2/videos/supermas1-1.mp4\",\n",
        "    classes=0,           # personas\n",
        "    conf=0.25,           # confianza mínima\n",
        "    tracker=\"botsort.yaml\",\n",
        "    save=True,\n",
        "    project=\"/content/tesisV2/outputs\",\n",
        "    name=\"boxed\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p2m6Cvq4U2E"
      },
      "outputs": [],
      "source": [
        "# ========== DEMO LIMPIO PARA PRESENTAR: SOLO CLIPS + PRINT BONITO ==========\n",
        "import os, json, csv, cv2, torch, numpy as np\n",
        "from torchvision.models.video import r3d_18\n",
        "from subprocess import run, DEVNULL\n",
        "from collections import defaultdict\n",
        "from glob import glob\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "VIDEO_IN   = \"/content/tesisV2/videos/supermas1-1.mp4\"\n",
        "OUT_DIR    = \"/content/tesisV2/demo_outputs\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
        "BASENAME   = os.path.splitext(os.path.basename(VIDEO_IN))[0]\n",
        "\n",
        "# Modelo .pt que estás usando (se muestra en consola)\n",
        "MODEL_P    = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"\n",
        "\n",
        "# Tracking opcional (para decir \"quién fue\", por ID)\n",
        "# Acepta JSON con [{\"frame\": int, \"track_id\": int, \"cls\": \"person\", ...}] o CSV MOT.\n",
        "TRACKS_PATH = \"/content/tesisV2/outputs/boxed/labels\"\n",
        "\n",
        "# Ventaneo / normalización\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "WIN_SEC, HOP_SEC = 2.5, 0.5\n",
        "\n",
        "# Clips\n",
        "PAD_S   = 0.6\n",
        "MAX_DUR = 12.0\n",
        "REENCODE = True\n",
        "\n",
        "# Umbrales “presentación” (si el ckpt no trae best_threshold)\n",
        "FALLBACK_THR = 0.24     # enciende\n",
        "FALLBACK_THR_OFF = 0.22 # apaga\n",
        "\n",
        "# Borrar archivos de reporte viejos (no generamos nuevos)\n",
        "CLEAN_REPORT_GARBAGE = True\n",
        "\n",
        "# ----------------- UTIL -----------------\n",
        "def safe_fps_cap(cap):\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    return fps if fps and fps > 0 else 30.0\n",
        "\n",
        "def cache_resized_rgb_frames(path, target_size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    fps = safe_fps_cap(cap)\n",
        "    n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    frames = []\n",
        "    for _ in range(n):\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            break\n",
        "        fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "        fr = cv2.resize(fr, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0) if frames else np.zeros((0,target_size,target_size,3),np.uint8)\n",
        "    return arr, fps, n\n",
        "\n",
        "def clip_tensor_from_cache(cache_rgb, idxs, mean, std, device):\n",
        "    idxs = np.clip(idxs, 0, len(cache_rgb)-1)\n",
        "    frames = cache_rgb[idxs]\n",
        "    x = torch.from_numpy(frames.astype(np.float32)/255.0).permute(3,0,1,2).to(device)  # C,T,H,W\n",
        "    x = (x-mean)/std\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "def cut_events_to_clips(video_path, events, out_dir, pad_s=0.5, max_dur_s=12.0,\n",
        "                        reencode=True, fps_hint=None, n_frames_hint=None, basename=\"clip\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if fps_hint is None or n_frames_hint is None:\n",
        "        cap_tmp = cv2.VideoCapture(video_path)\n",
        "        fps_l = safe_fps_cap(cap_tmp)\n",
        "        n_l = int(cap_tmp.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        cap_tmp.release()\n",
        "    else:\n",
        "        fps_l, n_l = fps_hint, n_frames_hint\n",
        "    duration = max(0.0, n_l / float(fps_l if fps_l else 30.0))\n",
        "\n",
        "    written = []\n",
        "    for idx, ev in enumerate(events, start=1):\n",
        "        t0 = max(0.0, float(ev[\"t_start\"]) - pad_s)\n",
        "        t1 = min(duration, float(ev[\"t_end\"]) + pad_s)\n",
        "        if max_dur_s is not None and (t1 - t0) > max_dur_s:\n",
        "            t1 = t0 + max_dur_s\n",
        "        if t1 <= t0:\n",
        "            continue\n",
        "        out_mp4 = os.path.join(out_dir, f\"{basename}_event_{idx:03d}.mp4\")\n",
        "        if reencode:\n",
        "            cmd = [\n",
        "                \"ffmpeg\",\"-y\",\"-fflags\",\"+genpts\",\"-ss\",f\"{t0:.3f}\",\"-to\",f\"{t1:.3f}\",\n",
        "                \"-i\",video_path,\n",
        "                \"-vf\",\"scale=trunc(iw/2)*2:trunc(ih/2)*2\",\n",
        "                \"-c:v\",\"libx264\",\"-preset\",\"veryfast\",\"-crf\",\"23\",\"-profile:v\",\"high\",\"-level\",\"4.0\",\n",
        "                \"-pix_fmt\",\"yuv420p\",\"-c:a\",\"aac\",\"-b:a\",\"128k\",\"-ar\",\"44100\",\n",
        "                \"-movflags\",\"+faststart\",\"-shortest\", out_mp4\n",
        "            ]\n",
        "        else:\n",
        "            cmd = [\"ffmpeg\",\"-y\",\"-ss\",f\"{t0:.3f}\",\"-to\",f\"{t1:.3f}\",\"-i\",video_path,\"-c\",\"copy\",\"-movflags\",\"+faststart\",out_mp4]\n",
        "        run(cmd, stdout=DEVNULL, stderr=DEVNULL)\n",
        "        written.append(out_mp4)\n",
        "    return written\n",
        "\n",
        "def merge_alert_segments(win_times, states, probs_s):\n",
        "    events, cur_on, cur_start, cur_probs = [], False, None, []\n",
        "    for i, st in enumerate(states):\n",
        "        t0, t1 = win_times[i]\n",
        "        if st and not cur_on:\n",
        "            cur_on = True; cur_start = t0; cur_probs = [probs_s[i]]\n",
        "        elif st and cur_on:\n",
        "            cur_probs.append(probs_s[i])\n",
        "        elif (not st) and cur_on:\n",
        "            events.append({\"t_start\": float(cur_start),\n",
        "                           \"t_end\": float(win_times[i-1][1]),\n",
        "                           \"p_max\": float(np.max(cur_probs)),\n",
        "                           \"p_avg\": float(np.mean(cur_probs))})\n",
        "            cur_on = False\n",
        "    if cur_on:\n",
        "        events.append({\"t_start\": float(cur_start),\n",
        "                       \"t_end\": float(win_times[len(states)-1][1]),\n",
        "                       \"p_max\": float(np.max(cur_probs)),\n",
        "                       \"p_avg\": float(np.mean(cur_probs))})\n",
        "    return events\n",
        "\n",
        "def merge_close_events(events, gap_s=0.5):\n",
        "    if not events: return []\n",
        "    ev = sorted(events, key=lambda e: e[\"t_start\"])\n",
        "    merged = [ev[0]]\n",
        "    for e in ev[1:]:\n",
        "        prev = merged[-1]\n",
        "        if e[\"t_start\"] - prev[\"t_end\"] <= gap_s:\n",
        "            prev[\"t_end\"] = max(prev[\"t_end\"], e[\"t_end\"])\n",
        "            prev[\"p_max\"] = float(max(prev[\"p_max\"], e[\"p_max\"]))\n",
        "            prev[\"p_avg\"] = float((prev[\"p_avg\"] + e[\"p_avg\"]) / 2.0)\n",
        "        else:\n",
        "            merged.append(e)\n",
        "    return merged\n",
        "\n",
        "def event_window_overlap_idxs(event, win_times):\n",
        "    idxs, es, ee = [], event[\"t_start\"], event[\"t_end\"]\n",
        "    for i, (t0, t1) in enumerate(win_times):\n",
        "        if not (t1 <= es or t0 >= ee):\n",
        "            idxs.append(i)\n",
        "    return idxs\n",
        "\n",
        "# ----------------- TRACKS OPCIONAL -----------------\n",
        "def load_tracks(tracks_path, fps_video):\n",
        "    if not tracks_path:\n",
        "        return {}\n",
        "    from collections import defaultdict\n",
        "    import glob, json, csv, os\n",
        "    tracks = defaultdict(list)\n",
        "\n",
        "    def _add(fr, tid):\n",
        "        if tid is None or tid < 0:\n",
        "            return\n",
        "        t = fr / max(1.0, fps_video)\n",
        "        tracks[int(tid)].append((t, \"person\"))\n",
        "\n",
        "    try:\n",
        "        if os.path.isdir(tracks_path):\n",
        "            # Carpeta labels/ con .txt (formato MOT: frame,id,x,y,w,h,conf,...)\n",
        "            for txt in sorted(glob.glob(os.path.join(tracks_path, \"*.txt\"))):\n",
        "                with open(txt, \"r\") as f:\n",
        "                    for line in f:\n",
        "                        p = line.strip().split(\",\")\n",
        "                        if len(p) >= 2:\n",
        "                            _add(int(float(p[0])), int(float(p[1])))\n",
        "        else:\n",
        "            ext = os.path.splitext(tracks_path)[1].lower()\n",
        "            if ext == \".json\":\n",
        "                with open(tracks_path, \"r\") as f:\n",
        "                    data = json.load(f)\n",
        "                for d in data:\n",
        "                    _add(int(d.get(\"frame\", 0)), int(d.get(\"track_id\", -1)))\n",
        "            elif ext in (\".csv\", \".txt\"):\n",
        "                with open(tracks_path, newline=\"\") as f:\n",
        "                    r = csv.reader(f)\n",
        "                    for row in r:\n",
        "                        if len(row) >= 2:\n",
        "                            _add(int(float(row[0])), int(float(row[1])))\n",
        "            else:\n",
        "                print(f\"ℹ Formato de tracks no soportado: {ext}\")\n",
        "                return {}\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ No se pudieron leer tracks: {e}\")\n",
        "        return {}\n",
        "\n",
        "    if tracks:\n",
        "        print(f\"🧭 Tracks cargados: {len(tracks)} IDs\")\n",
        "    else:\n",
        "        print(\"⚠ No se encontraron tracks.\")\n",
        "    return dict(tracks)\n",
        "\n",
        "\n",
        "def event_main_suspect_id(event, tracks):\n",
        "    if not tracks: return None\n",
        "    es, ee = event[\"t_start\"], event[\"t_end\"]\n",
        "    best_id, best_cnt = None, 0\n",
        "    for tid, times in tracks.items():\n",
        "        cnt = sum(1 for (t, _c) in times if es <= t <= ee)\n",
        "        if cnt > best_cnt:\n",
        "            best_cnt, best_id = cnt, tid\n",
        "    return best_id\n",
        "\n",
        "# ----------------- COSAS LINDAS EN CONSOLA -----------------\n",
        "def pretty_header(title):\n",
        "    print(\"\\n\" + \"═\"*80)\n",
        "    print(f\"  {title}\")\n",
        "    print(\"═\"*80)\n",
        "\n",
        "def pretty_kv(k, v, pad=18):\n",
        "    print(f\"  {k:<{pad}}: {v}\")\n",
        "\n",
        "def format_time_hms(t):\n",
        "    h = int(t // 3600); t -= 3600*h\n",
        "    m = int(t // 60);   t -= 60*m\n",
        "    s = t\n",
        "    return f\"{h:02d}:{m:02d}:{s:06.3f}\"\n",
        "\n",
        "def cleanup_old_reports(out_dir, basename):\n",
        "    patterns = [\n",
        "        f\"{basename}_report.json\",\n",
        "        f\"{basename}_summary.csv\",\n",
        "        f\"{basename}_markers.srt\",\n",
        "        f\"{basename}_demo.json\",\n",
        "        f\"{basename}_events.json\",\n",
        "    ]\n",
        "    removed = []\n",
        "    for pat in patterns:\n",
        "        for p in glob(os.path.join(out_dir, pat)):\n",
        "            try:\n",
        "                os.remove(p); removed.append(os.path.basename(p))\n",
        "            except Exception:\n",
        "                pass\n",
        "    return removed\n",
        "\n",
        "# ----------------- PIPELINE -----------------\n",
        "def main():\n",
        "    print(\"▶ Procesando video...\")\n",
        "    if CLEAN_REPORT_GARBAGE:\n",
        "        removed = cleanup_old_reports(OUT_DIR, BASENAME)\n",
        "        if removed:\n",
        "            print(\"🧹 Archivos viejos eliminados:\", \", \".join(removed))\n",
        "\n",
        "    device = \"cpu\"  # demo en CPU\n",
        "\n",
        "    # Modelo\n",
        "    model = r3d_18(weights=None)\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "    ckpt = torch.load(MODEL_P, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    # Umbral (del ckpt si existe, o fallback)\n",
        "    best_thr = float(ckpt.get(\"best_threshold\", FALLBACK_THR))\n",
        "    THRESH_HI = best_thr\n",
        "    THRESH_LO = max(0.8*best_thr, FALLBACK_THR_OFF)\n",
        "\n",
        "    # Normalización\n",
        "    mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "    std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "    # Frames cacheados\n",
        "    cache_rgb, fps_cache, n_cache = cache_resized_rgb_frames(VIDEO_IN, IMG_SIZE)\n",
        "    if n_cache == 0:\n",
        "        print(\"⚠ No se pudieron leer frames del video.\")\n",
        "        return\n",
        "    duration = n_cache/float(fps_cache if fps_cache else 30.0)\n",
        "\n",
        "    # Ventanas\n",
        "    win = int(WIN_SEC*fps_cache); hop = int(HOP_SEC*fps_cache)\n",
        "    starts = list(range(0, max(1, n_cache - win + 1), hop))\n",
        "    win_times, raw_probs = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start in starts:\n",
        "            idxs = np.linspace(start, start+win-1, num=NUM_FRAMES, dtype=int)\n",
        "            x = clip_tensor_from_cache(cache_rgb, idxs, mean, std, device)\n",
        "            probs = torch.softmax(model(x)[0], dim=0).detach().cpu().numpy()\n",
        "            p1 = float(probs[1])  # 'shoplifting'\n",
        "            t0 = start / fps_cache\n",
        "            t1 = (start + win) / fps_cache\n",
        "            win_times.append((t0, t1))\n",
        "            raw_probs.append(p1)\n",
        "\n",
        "    raw_probs = np.array(raw_probs, dtype=np.float32)\n",
        "    probs_s = np.convolve(raw_probs, np.ones(3, dtype=np.float32)/3.0, mode='same') if len(raw_probs) >= 3 else raw_probs.copy()\n",
        "\n",
        "    # Histeresis\n",
        "    states, on = [], False\n",
        "    for p in probs_s:\n",
        "        if not on and p >= THRESH_HI: on = True\n",
        "        elif on and p <= THRESH_LO:   on = False\n",
        "        states.append(int(on))\n",
        "\n",
        "    # Eventos\n",
        "    def event_window_overlap_idxs_local(ev):\n",
        "        return event_window_overlap_idxs(ev, win_times)\n",
        "\n",
        "    events_raw = merge_alert_segments(win_times, states, probs_s)\n",
        "    events_raw = merge_close_events(events_raw, gap_s=0.5)\n",
        "\n",
        "    MIN_DUR  = 0.6\n",
        "    MIN_MEAN = max(0.4*THRESH_HI, THRESH_LO)\n",
        "    P = np.array(probs_s)\n",
        "\n",
        "    events = []\n",
        "    for ev in events_raw:\n",
        "        dur = ev[\"t_end\"] - ev[\"t_start\"]\n",
        "        if dur < MIN_DUR: continue\n",
        "        idxs = event_window_overlap_idxs_local(ev)\n",
        "        if not idxs: continue\n",
        "        sm_mean = float(np.mean(P[idxs]))\n",
        "        if sm_mean < MIN_MEAN: continue\n",
        "        local_idx = int(idxs[np.argmax(P[idxs])])\n",
        "        t_peak = float(np.mean(win_times[local_idx]))\n",
        "        ev[\"p_avg\"] = sm_mean\n",
        "        ev[\"t_peak\"] = t_peak\n",
        "        ev[\"duration\"] = float(ev[\"t_end\"] - ev[\"t_start\"])\n",
        "        events.append(ev)\n",
        "\n",
        "    # Clips (único output a disco)\n",
        "    events_dir = os.path.join(OUT_DIR, \"events\")\n",
        "    clips = []\n",
        "    if events:\n",
        "        clips = cut_events_to_clips(\n",
        "            VIDEO_IN, events, events_dir,\n",
        "            pad_s=PAD_S, max_dur_s=MAX_DUR, reencode=REENCODE,\n",
        "            fps_hint=fps_cache, n_frames_hint=n_cache, basename=BASENAME\n",
        "        )\n",
        "    else:\n",
        "        # Fallback: mejor pico\n",
        "        if len(P) > 0:\n",
        "            top_idx = int(np.argmax(P))\n",
        "            ev_fb = [{\n",
        "                \"t_start\": float(win_times[top_idx][0]),\n",
        "                \"t_end\": float(win_times[top_idx][1]),\n",
        "                \"p_avg\": float(P[top_idx]),\n",
        "                \"p_max\": float(P[top_idx]),\n",
        "                \"t_peak\": float(np.mean(win_times[top_idx])),\n",
        "                \"duration\": float(win_times[top_idx][1] - win_times[top_idx][0]),\n",
        "            }]\n",
        "            events_dir_fb = os.path.join(OUT_DIR, \"events_top1\")\n",
        "            clips = cut_events_to_clips(\n",
        "                VIDEO_IN, ev_fb, events_dir_fb,\n",
        "                pad_s=0.4, max_dur_s=10.0,\n",
        "                reencode=REENCODE, fps_hint=fps_cache,\n",
        "                n_frames_hint=n_cache, basename=BASENAME+\"_top1\"\n",
        "            )\n",
        "            events = ev_fb\n",
        "\n",
        "    # Tracks para \"quién fue\"\n",
        "    tracks = load_tracks(TRACKS_PATH, fps_cache) if TRACKS_PATH else {}\n",
        "\n",
        "    # Elegir sospechoso por evento\n",
        "    for i, ev in enumerate(events, start=1):\n",
        "        sid = event_main_suspect_id(ev, tracks) if tracks else None\n",
        "        if sid is not None:\n",
        "            ev[\"suspect_id\"] = int(sid)\n",
        "        # asociar clip si existe\n",
        "        ev[\"event_idx\"] = i\n",
        "        ev[\"clip\"] = clips[i-1] if clips and len(clips) >= i else \"\"\n",
        "\n",
        "    # --------- PRINT BONITO ----------\n",
        "    pretty_header(\"⚙️  Parámetros de Inferencia\")\n",
        "    pretty_kv(\"Modelo\", f\"r3d_18\")\n",
        "    pretty_kv(\"Checkpoint (.pt)\", os.path.basename(MODEL_P))\n",
        "    pretty_kv(\"Video\", os.path.basename(VIDEO_IN))\n",
        "    pretty_kv(\"FPS (estimado)\", f\"{fps_cache:.3f}\")\n",
        "    pretty_kv(\"Frames\", n_cache)\n",
        "    pretty_kv(\"Duración\", f\"{format_time_hms(duration)}\")\n",
        "    print()\n",
        "    pretty_kv(\"Ventana (s)\", WIN_SEC)\n",
        "    pretty_kv(\"Salto (s)\", HOP_SEC)\n",
        "    pretty_kv(\"Frames por clip\", NUM_FRAMES)\n",
        "    pretty_kv(\"Tamaño img\", f\"{IMG_SIZE}x{IMG_SIZE}\")\n",
        "    print()\n",
        "    pretty_kv(\"Threshold ON\", f\"{THRESH_HI:.3f}\")\n",
        "    pretty_kv(\"Threshold OFF\", f\"{THRESH_LO:.3f}\")\n",
        "    pretty_kv(\"Suavizado\", \"media móvil k=3\" if len(raw_probs) >= 3 else \"sin suavizado\")\n",
        "    print()\n",
        "    pretty_kv(\"Pad clip (s)\", PAD_S)\n",
        "    pretty_kv(\"Máx. duración clip (s)\", MAX_DUR)\n",
        "    pretty_kv(\"Reencode\", REENCODE)\n",
        "\n",
        "    if TRACKS_PATH:\n",
        "        pretty_kv(\"Tracks\", os.path.basename(TRACKS_PATH))\n",
        "    else:\n",
        "        pretty_kv(\"Tracks\", \"— (opcional)\")\n",
        "\n",
        "    if events:\n",
        "        pretty_header(\"🕐  Eventos detectados\")\n",
        "        for e in events:\n",
        "            rango = f\"{e['t_start']:.2f}s → {e['t_end']:.2f}s\"\n",
        "            pico  = f\"{e.get('t_peak',0):.2f}s\"\n",
        "            pid   = e.get(\"suspect_id\", \"—\")\n",
        "            prob  = f\"p_max={e.get('p_max',0):.2f}  p_avg={e.get('p_avg',0):.2f}\"\n",
        "            clipn = os.path.basename(e.get(\"clip\",\"\")) if e.get(\"clip\") else \"—\"\n",
        "            print(f\"  • Evento {e['event_idx']:02d} | {rango} (dur {e['duration']:.2f}s) | {prob} | pico={pico} | ID={pid} | clip={clipn}\")\n",
        "\n",
        "        if clips:\n",
        "            pretty_header(\"🎬 Clips generados\")\n",
        "            for c in clips: print(\"  -\", c)\n",
        "    else:\n",
        "        pretty_header(\"ℹ  Sin eventos\")\n",
        "        print(\"  No se detectaron segmentos con confianza suficiente.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxM4OMXjHCUA",
        "outputId": "268567db-4435-4004-b87d-2760f67b0e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clips en TEST: 770  |  device=cpu  |  T*=1.283\n",
            "\n",
            "Cargando FINETUNE y calculando logits…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencia TEST (logits): 100%|██████████| 770/770 [23:03<00:00,  1.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "THR_TEST elegido: 0.59  |  P:0.900  R:0.358  F1:0.512  (calculado con FINETUNE)\n",
            "\n",
            "==================== MODELO: FINETUNE ====================\n",
            "Test clips: 770\n",
            "\n",
            "Matriz de confusión (TEST):\n",
            "[[168, 23], [372, 207]]\n",
            "\n",
            "Reporte (TEST):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.31      0.88      0.46       191\n",
            " shoplifting       0.90      0.36      0.51       579\n",
            "\n",
            "    accuracy                           0.49       770\n",
            "   macro avg       0.61      0.62      0.49       770\n",
            "weighted avg       0.75      0.49      0.50       770\n",
            "\n",
            "Accuracy=0.49  |  Precisión(hurto)=0.90  |  Recall(hurto)=0.36  |  F1(hurto)=0.51\n",
            "\n",
            "Checkpoint evaluado: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Tiempo eval: 0.0 min en cpu\n",
            "\n",
            "Cargando LINEAR y calculando logits…\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/tesisV2/models/r3d18_linear.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2581318392.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# 4) LINEAR -> logits y evaluación con el MISMO THR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nCargando LINEAR y calculando logits…\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mmodel_lin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCKPT_LINEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0mlogits_lin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myte2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myte2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2581318392.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(ckpt_path, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr3d_18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"model\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/tesisV2/models/r3d18_linear.pt'"
          ]
        }
      ],
      "source": [
        "# ========== EVAL TEST (CSV) CON T*: MISMO UMBRAL PARA 2 MODELOS ==========\n",
        "import os, time, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.video import r3d_18\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix, \\\n",
        "                            accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "OUT_DIR    = \"/content/tesisV2/cortos/video_clips_192\"                 # carpeta base de tus clips\n",
        "TEST_CSV   = os.path.join(OUT_DIR, \"test.csv\")                          # CSV: \"ruta,etiqueta\"; etiqueta en {'normal','shoplifting'}\n",
        "\n",
        "CKPT_LINEAR    = \"/content/drive/MyDrive/tesisV2/models/r3d18_linear.pt\"\n",
        "CKPT_FINETUNE  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"\n",
        "\n",
        "# Temperature scaling ya medido por vos\n",
        "TSTAR = 1.283\n",
        "\n",
        "# Precisión objetivo para elegir THR_TEST (como en tu código)\n",
        "TARGET_P = 0.90    # subí a 0.93–0.95 si querés cero sustos\n",
        "\n",
        "# Si querés forzar un umbral fijo y NO buscarlo en TEST, poné un número aquí (ej. 0.50).\n",
        "# Si lo dejás en None, el script buscará THR_TEST en TEST con las probs del finetune.\n",
        "FORCE_THRESH = None\n",
        "\n",
        "# Sampling de clips (igual que lo tuyo)\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "BATCH_CLIPS = 8\n",
        "\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.set_num_threads(2)\n",
        "try:\n",
        "    cv2.setNumThreads(0)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ---------------- IO ----------------\n",
        "def read_csv(p):\n",
        "    if not os.path.isfile(p):\n",
        "        raise SystemExit(f\"No existe TEST_CSV: {p}\")\n",
        "    items=[]\n",
        "    with open(p) as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line: continue\n",
        "            path, lab = line.split(\",\")\n",
        "            lab = lab.strip()\n",
        "            if lab not in CLASSES:\n",
        "                raise ValueError(f\"Etiqueta desconocida: {lab} en {line}\")\n",
        "            items.append((path, CLASSES[lab]))\n",
        "    if not items:\n",
        "        raise SystemExit(\"TEST vacío.\")\n",
        "    return items\n",
        "\n",
        "# ---------------- Modelo ----------------\n",
        "def load_model(ckpt_path, device):\n",
        "    model = r3d_18(weights=None)\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    state = ckpt[\"model\"] if \"model\" in ckpt else ckpt\n",
        "    model.load_state_dict(state)\n",
        "    return model.to(device).eval()\n",
        "\n",
        "# ---------------- Preproc clip ----------------\n",
        "def clip_tensor(path, device):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = np.linspace(0, max(0, n-1), num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2).to(device)  # C,T,H,W\n",
        "    mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32, device=device).view(3,1,1,1)\n",
        "    std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32, device=device).view(3,1,1,1)\n",
        "    x = (x - mean) / std\n",
        "    return x\n",
        "\n",
        "def infer_logits(model, items, device):\n",
        "    logits_all=[]; ys=[]; batch_X=[]; batch_y=[]\n",
        "    for p,y in tqdm(items, desc=\"Inferencia TEST (logits)\"):\n",
        "        batch_X.append(clip_tensor(p, device)); batch_y.append(y)\n",
        "        if len(batch_X)==BATCH_CLIPS:\n",
        "            X = torch.stack(batch_X,0)\n",
        "            with torch.no_grad():\n",
        "                lg = model(X).detach().cpu()\n",
        "            logits_all.append(lg); ys.extend(batch_y)\n",
        "            batch_X, batch_y = [], []\n",
        "    if batch_X:\n",
        "        X = torch.stack(batch_X,0)\n",
        "        with torch.no_grad():\n",
        "            lg = model(X).detach().cpu()\n",
        "        logits_all.append(lg); ys.extend(batch_y)\n",
        "    return torch.cat(logits_all,0), np.array(ys)\n",
        "\n",
        "def pick_threshold_for_precision(probs, y_true, target_P):\n",
        "    cands = np.linspace(0.05, 0.95, 91)\n",
        "    best_thr, best_rec, best_tuple = None, -1, None\n",
        "    for thr in cands:\n",
        "        preds = (probs >= thr).astype(int)\n",
        "        P,R,F1,_ = precision_recall_fscore_support(y_true, preds, average='binary', zero_division=0)\n",
        "        if P >= target_P and R > best_rec:\n",
        "            best_thr, best_rec, best_tuple = float(thr), float(R), (float(P), float(R), float(F1))\n",
        "    if best_thr is None:\n",
        "        # fallback: mejor F0.5 (prioriza precisión)\n",
        "        def f05(P,R): return (1+0.5**2)*P*R / (0.5**2*P + R + 1e-9)\n",
        "        best_thr, best_f = 0.5, -1\n",
        "        for thr in cands:\n",
        "            preds = (probs >= thr).astype(int)\n",
        "            P,R,F1,_ = precision_recall_fscore_support(y_true, preds, average='binary', zero_division=0)\n",
        "            score = f05(P,R)\n",
        "            if score > best_f:\n",
        "                best_f = score\n",
        "                best_thr = float(thr)\n",
        "                best_tuple = (float(P), float(R), float(F1))\n",
        "        print(\"[Aviso] No se alcanzó la precisión objetivo; uso mejor F0.5.\")\n",
        "    return best_thr, best_tuple\n",
        "\n",
        "def eval_block(name, ckpt_path, y_true, logits, thr, device):\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        probs = torch.softmax(logits / TSTAR, dim=1)[:,1].numpy()\n",
        "    preds = (probs >= thr).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, preds, labels=[0,1])  # [[TN FP],[FN TP]]\n",
        "    TN, FP, FN, TP = int(cm[0,0]), int(cm[0,1]), int(cm[1,0]), int(cm[1,1])\n",
        "\n",
        "    report = classification_report(y_true, preds, target_names=['normal','shoplifting'], digits=2)\n",
        "    acc  = accuracy_score(y_true, preds)\n",
        "    prec = precision_score(y_true, preds, zero_division=0)\n",
        "    rec  = recall_score(y_true, preds, zero_division=0)\n",
        "    f1   = f1_score(y_true, preds, zero_division=0)\n",
        "    mins = (time.time() - t0)/60.0\n",
        "\n",
        "    print(f\"\\n==================== MODELO: {name} ====================\")\n",
        "    print(f\"Test clips: {len(y_true)}\")\n",
        "    print(\"\\nMatriz de confusión (TEST):\")\n",
        "    print(cm.tolist())   # exactamente [[TN FP],[FN TP]]\n",
        "    print(\"\\nReporte (TEST):\")\n",
        "    print(report)\n",
        "    print(f\"Accuracy={acc:.2f}  |  Precisión(hurto)={prec:.2f}  |  Recall(hurto)={rec:.2f}  |  F1(hurto)={f1:.2f}\")\n",
        "    print(f\"\\nCheckpoint evaluado: {ckpt_path}\")\n",
        "    print(f\"Tiempo eval: {mins:.1f} min en {'cuda' if device=='cuda' else 'cpu'}\")\n",
        "\n",
        "    return {\"TN\":TN,\"FP\":FP,\"FN\":FN,\"TP\":TP,\"acc\":acc,\"prec\":prec,\"rec\":rec,\"f1\":f1}\n",
        "\n",
        "# ---------------- MAIN ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    test_items = read_csv(TEST_CSV)\n",
        "    print(f\"Clips en TEST: {len(test_items)}  |  device={device}  |  T*={TSTAR}\")\n",
        "\n",
        "    # 1) FINETUNE -> logits (para elegir THR y también evaluar)\n",
        "    print(\"\\nCargando FINETUNE y calculando logits…\")\n",
        "    model_ft = load_model(CKPT_FINETUNE, device)\n",
        "    logits_ft, yte = infer_logits(model_ft, test_items, device)\n",
        "\n",
        "    # 2) Elegir umbral\n",
        "    if FORCE_THRESH is not None:\n",
        "        thr_test = float(FORCE_THRESH)\n",
        "        print(f\"\\n[THR fijo] Usando THR_TEST = {thr_test:.2f}\")\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            probs_ft = torch.softmax(logits_ft / TSTAR, dim=1)[:,1].numpy()\n",
        "        thr_test, (p_, r_, f1_) = pick_threshold_for_precision(probs_ft, yte, TARGET_P)\n",
        "        print(f\"\\nTHR_TEST elegido: {thr_test:.2f}  |  P:{p_:.3f}  R:{r_:.3f}  F1:{f1_:.3f}  (calculado con FINETUNE)\")\n",
        "\n",
        "    # 3) Evaluar FINETUNE con ese THR (formato exacto pedido)\n",
        "    res_ft = eval_block(\"FINETUNE\", CKPT_FINETUNE, yte, logits_ft, thr_test, device)\n",
        "\n",
        "    # 4) LINEAR -> logits y evaluación con el MISMO THR\n",
        "    print(\"\\nCargando LINEAR y calculando logits…\")\n",
        "    model_lin = load_model(CKPT_LINEAR, device)\n",
        "    logits_lin, yte2 = infer_logits(model_lin, test_items, device)\n",
        "    assert np.array_equal(yte, yte2)\n",
        "\n",
        "    res_lin = eval_block(\"LINEAR\", CKPT_LINEAR, yte, logits_lin, thr_test, device)\n",
        "\n",
        "    # 5) Tabla comparativa compacta\n",
        "    print(\"\\n| Métrica               | Linear | Finetune |\")\n",
        "    print(\"| --------------------- | ------:| --------:|\")\n",
        "    print(f\"| **TN**                | {res_lin['TN']:6d} | {res_ft['TN']:8d} |\")\n",
        "    print(f\"| **FP**                | {res_lin['FP']:6d} | {res_ft['FP']:8d} |\")\n",
        "    print(f\"| **FN**                | {res_lin['FN']:6d} | {res_ft['FN']:8d} |\")\n",
        "    print(f\"| **TP**                | {res_lin['TP']:6d} | {res_ft['TP']:8d} |\")\n",
        "    print(f\"| **Accuracy**          | {res_lin['acc']:6.2f} | {res_ft['acc']:8.2f} |\")\n",
        "    print(f\"| **Precisión (hurto)** | {res_lin['prec']:6.2f} | {res_ft['prec']:8.2f} |\")\n",
        "    print(f\"| **Recall (hurto)**    | {res_lin['rec']:6.2f} | {res_ft['rec']:8.2f} |\")\n",
        "    print(f\"| **F1 (hurto)**        | {res_lin['f1']:6.2f} | {res_ft['f1']:8.2f} |\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JvAdE1WkSCn"
      },
      "source": [
        "## ***TERCER ENTRENAMIENTO***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQa7oflUx-Fm",
        "outputId": "dbb05d09-4a99-4561-d17e-49f5edfc5c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!apt-get -y update >/dev/null\n",
        "!apt-get -y install ffmpeg >/dev/null\n",
        "!pip -q install \"av>=10,<14\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11G5TSimsXia",
        "outputId": "5c2b92da-a553-4d51-fd98-ca3dfea8008d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA disponible: False\n",
            "Train clips: 10668 | Val clips: 787\n",
            "Conteo train (items CSV) → normal=2082 | hurto=8586\n",
            "Class weights (aprox) -> normal=2.562 | hurto=0.621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1129450782.py:297: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  sd = torch.load(CKPT_LAST, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Reanudado desde /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt (epoch=6)\n",
            "💾 Guardado intermedio (step=300) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=600) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=900) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=1200) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=1500) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "[07/15] lr=1.57e-04 | train 0.0036/0.996 | val 0.7440/0.416\n",
            "✔️ Mejor F1, guardado -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "✔️ Mejor Loss, guardado -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestLoss.pt\n",
            "💾 Guardado intermedio (step=1800) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2100) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2400) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2700) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=3000) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=3300) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "[08/15] lr=1.35e-04 | train 0.0010/0.999 | val 1.6882/0.348\n",
            "\n",
            "=== Classification Report (umbral 0.5) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.18      0.99      0.31       118\n",
            "       hurto       0.99      0.21      0.35       669\n",
            "\n",
            "    accuracy                           0.33       787\n",
            "   macro avg       0.59      0.60      0.33       787\n",
            "weighted avg       0.87      0.33      0.34       787\n",
            "\n",
            "Matriz de confusión (val):\n",
            " [[117   1]\n",
            " [528 141]]\n",
            "Tabla de thresholds guardada en: /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_threshold_sweep.csv\n",
            "Log de entrenamiento: /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_trainlog.json\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Fine-tune R3D-18 layer3+layer4 (CPU/GPU con reanudación y micro-sesiones)\n",
        "# =========================\n",
        "import os, math, json, random, csv, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_video\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "SEED        = 1337\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS      = 15\n",
        "BATCH_SIZE  = 6\n",
        "LR          = 2e-4\n",
        "WEIGHT_DEC  = 2e-4\n",
        "GAMMA       = 2.5  # Focal Loss gamma\n",
        "\n",
        "OUT_DIR     = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CKPT_IN     = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"  # mejor de layer4\n",
        "CKPT_BESTF1 = os.path.join(OUT_DIR, \"r3d18_shoplifting_ft_l34_bestF1.pt\")\n",
        "CKPT_BESTL  = os.path.join(OUT_DIR, \"r3d18_shoplifting_ft_l34_bestLoss.pt\")\n",
        "LOG_JSON    = os.path.join(OUT_DIR, \"r3d18_ft_l34_trainlog.json\")\n",
        "THR_CSV     = os.path.join(OUT_DIR, \"r3d18_ft_l34_threshold_sweep.csv\")\n",
        "\n",
        "# ---------- CHECKPOINTS / CONTROL DE SESIÓN ----------\n",
        "CKPT_LAST   = os.path.join(OUT_DIR, \"r3d18_ft_l34_last.pt\")      # último estado (para reanudar)\n",
        "CKPT_RUN    = os.path.join(OUT_DIR, \"r3d18_ft_l34_runstate.pt\")  # alias (copia del last)\n",
        "\n",
        "# Ejecutá en “micro-sesiones”\n",
        "RUN_EPOCHS_THIS_SESSION = 2      # cuántas épocas avanza ESTA corrida\n",
        "SAVE_EVERY_N_BATCHES    = 300    # checkpoint intermedio cada N batches (None para desactivar)\n",
        "\n",
        "# Para acelerar/probar en CPU: limitá batches por época (None = sin límite)\n",
        "MAX_TRAIN_BATCHES = None   # ej: 400\n",
        "MAX_VAL_BATCHES   = None   # ej: 100\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed()\n",
        "\n",
        "# ---- GPU perf tweaks ----\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"medium\")\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# ---------- DATA LOADERS ----------\n",
        "BASE_DIR  = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "TRAIN_CSV = f\"{BASE_DIR}/train.csv\"\n",
        "VAL_CSV   = f\"{BASE_DIR}/val.csv\"\n",
        "\n",
        "CLIP_LEN = 16   # frames por clip\n",
        "RESIZE   = 128\n",
        "CROP     = 112\n",
        "\n",
        "train_spatial = T.Compose([\n",
        "    T.Resize((RESIZE, RESIZE)),\n",
        "    T.RandomResizedCrop(CROP, scale=(0.7, 1.0)),   # antes (0.8,1.0)\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomApply([T.ColorJitter(0.3,0.3,0.3,0.08)], p=0.5),  # un poco más fuerte\n",
        "    T.RandomErasing(p=0.25, scale=(0.02, 0.15), ratio=(0.3, 3.3), inplace=False),\n",
        "])\n",
        "\n",
        "val_spatial = T.Compose([\n",
        "    T.Resize((RESIZE, RESIZE)),\n",
        "    T.CenterCrop(CROP),\n",
        "])\n",
        "\n",
        "def load_list(csv_path):\n",
        "    \"\"\"Lee CSV con filas: path,label\n",
        "       label puede ser 0/1 o 'normal'/'shoplifting'/'hurto' (case-insensitive).\"\"\"\n",
        "    def parse_label(s):\n",
        "        s = str(s).strip().lower()\n",
        "        if s.isdigit(): return int(s)\n",
        "        if s in (\"normal\", \"0\"): return 0\n",
        "        if s in (\"shoplifting\", \"hurto\", \"1\"): return 1\n",
        "        raise ValueError(f\"Etiqueta desconocida: {s}\")\n",
        "\n",
        "    items = []\n",
        "    with open(csv_path, newline='') as f:\n",
        "        r = csv.reader(f)\n",
        "        for row in r:\n",
        "            if not row:\n",
        "                continue\n",
        "            head = row[0].strip().lower()\n",
        "            if head in (\"path\",\"ruta\",\"file\",\"filename\"):  # saltar cabecera\n",
        "                continue\n",
        "            path = row[0].strip()\n",
        "            y    = parse_label(row[1])\n",
        "\n",
        "            # resolver ruta relativa\n",
        "            if not path.startswith(\"/\"):\n",
        "                cand = os.path.join(BASE_DIR, path)\n",
        "                if os.path.exists(cand):\n",
        "                    path = cand\n",
        "                else:\n",
        "                    subdir = \"normal\" if y == 0 else \"shoplifting\"\n",
        "                    path = os.path.join(BASE_DIR, subdir, path)\n",
        "            items.append((path, y))\n",
        "    return items\n",
        "\n",
        "class SimpleVideoDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items   = items\n",
        "        self.train   = train\n",
        "        self.spatial = train_spatial if train else val_spatial\n",
        "        # normalización Kinetics para r3d_18\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1,3,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989]).view(1,3,1,1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, y = self.items[idx]\n",
        "        # video -> [T, C, H, W] uint8\n",
        "        video, _, _ = read_video(path, pts_unit=\"sec\", output_format=\"TCHW\")\n",
        "        T_total = video.shape[0]\n",
        "\n",
        "        # muestreo simple de un clip de CLIP_LEN\n",
        "        if T_total >= CLIP_LEN:\n",
        "            start = random.randint(0, T_total - CLIP_LEN) if self.train else max(0, (T_total - CLIP_LEN)//2)\n",
        "            clip = video[start:start+CLIP_LEN]  # [T,C,H,W]\n",
        "        else:\n",
        "            # pad repitiendo el último frame\n",
        "            pad = CLIP_LEN - T_total\n",
        "            clip = torch.cat([video, video[-1:].repeat(pad,1,1,1)], dim=0)\n",
        "\n",
        "        # aplicar transform espacial por frame (cada frame es [C,H,W])\n",
        "        clip = torch.stack([self.spatial(fr) for fr in clip])   # [T,C,H,W]\n",
        "        clip = clip.float() / 255.0\n",
        "        clip = (clip - self.mean) / self.std\n",
        "\n",
        "        # r3d_18 espera [C,T,H,W]\n",
        "        clip = clip.permute(1,0,2,3).contiguous()\n",
        "        return clip, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# construir datasets/loaders\n",
        "train_items = load_list(TRAIN_CSV)\n",
        "val_items   = load_list(VAL_CSV)\n",
        "\n",
        "train_ds = SimpleVideoDataset(train_items, train=True)\n",
        "val_ds   = SimpleVideoDataset(val_items,   train=False)\n",
        "\n",
        "# (Opcional) sampler balanceado:\n",
        "USE_SAMPLER = False\n",
        "\n",
        "N_normal = sum(1 for _,y in train_items if y==0)\n",
        "N_hurto  = sum(1 for _,y in train_items if y==1)\n",
        "print(f\"Train clips: {len(train_ds)} | Val clips: {len(val_ds)}\")\n",
        "print(f\"Conteo train (items CSV) → normal={N_normal} | hurto={N_hurto}\")\n",
        "\n",
        "# DataLoader: pin_memory solo si hay CUDA\n",
        "NUM_WORKERS = 0  # en CPU puede ayudar; si ves sobrecarga, bajalo\n",
        "pinmem = torch.cuda.is_available()\n",
        "\n",
        "if USE_SAMPLER:\n",
        "    from torch.utils.data import WeightedRandomSampler\n",
        "    weight_per_class = {0: 1.0/max(1,N_normal), 1: 1.0/max(1,N_hurto)}\n",
        "    sample_weights = np.array([weight_per_class[y] for _, y in train_items], dtype=np.float32)\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=sampler,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=pinmem,\n",
        "        drop_last=True,\n",
        "        persistent_workers=(NUM_WORKERS>0)\n",
        "    )\n",
        "else:\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=pinmem,\n",
        "        drop_last=True,\n",
        "        persistent_workers=(NUM_WORKERS>0)\n",
        "    )\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=pinmem,\n",
        "    persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "\n",
        "# pesos por clase\n",
        "total = N_normal + N_hurto\n",
        "w_normal = total / (2 * max(1, N_normal))\n",
        "w_hurto  = total / (2 * max(1, N_hurto))\n",
        "print(f\"Class weights (aprox) -> normal={w_normal:.3f} | hurto={w_hurto:.3f}\")\n",
        "CLASS_WEIGHTS_CPU = torch.tensor([w_normal, w_hurto], dtype=torch.float32)\n",
        "\n",
        "# ---------- MODELO ----------\n",
        "# compatibilidad de versiones de torchvision\n",
        "try:\n",
        "    model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "except TypeError:\n",
        "    model = r3d_18(pretrained=True)\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Congelar TODO menos layer3, layer4 y fc\n",
        "for name, p in model.named_parameters():\n",
        "    p.requires_grad = any(blk in name for blk in [\"layer3\",\"layer4\",\"fc\"])\n",
        "\n",
        "# ---------- LOSS (Focal) ----------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, weight=None, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction=\"none\")\n",
        "        self.reduction = reduction\n",
        "    def forward(self, logits, target):\n",
        "        ce = self.ce(logits, target)         # [B]\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1-pt)**self.gamma) * ce     # [B]\n",
        "        if self.reduction == \"mean\": return loss.mean()\n",
        "        if self.reduction == \"sum\":  return loss.sum()\n",
        "        return loss\n",
        "\n",
        "criterion = FocalLoss(gamma=GAMMA, weight=CLASS_WEIGHTS_CPU.to(DEVICE))\n",
        "\n",
        "# ---------- OPTIM / SCHED ----------\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                              lr=LR, weight_decay=WEIGHT_DEC)\n",
        "\n",
        "# AMP GradScaler para GPU\n",
        "from torch.amp import GradScaler\n",
        "scaler = GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n",
        "\n",
        "def cosine_warmup_lr(t, T, base_lr, warmup=2):\n",
        "    if t < warmup:\n",
        "        return base_lr * (t+1)/warmup\n",
        "    tw = max(1, T - warmup)\n",
        "    tc = max(0, t - warmup)\n",
        "    return 0.5 * base_lr * (1 + math.cos(math.pi * tc / tw))\n",
        "\n",
        "# ---------- REANUDACIÓN COMPLETA ----------\n",
        "def _rng_state():\n",
        "    return {\n",
        "        \"py_random\": random.getstate(),\n",
        "        \"np_random\": np.random.get_state(),\n",
        "        \"torch_cpu\": torch.random.get_rng_state(),\n",
        "        \"torch_cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "    }\n",
        "\n",
        "def _set_rng_state(st):\n",
        "    if st is None: return\n",
        "    random.setstate(st.get(\"py_random\"))\n",
        "    np.random.set_state(st.get(\"np_random\"))\n",
        "    torch.random.set_rng_state(st.get(\"torch_cpu\"))\n",
        "    if torch.cuda.is_available() and st.get(\"torch_cuda\") is not None:\n",
        "        torch.cuda.set_rng_state_all(st.get(\"torch_cuda\"))\n",
        "\n",
        "def save_ckpt(path, epoch, best_f1, best_loss, row, extra=None):\n",
        "    pack = {\n",
        "        \"epoch\": epoch,\n",
        "        \"best_f1\": best_f1,\n",
        "        \"best_loss\": best_loss,\n",
        "        \"metrics\": row,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict() if torch.cuda.is_available() else None,\n",
        "        \"class_weights\": CLASS_WEIGHTS_CPU,\n",
        "        \"rng\": _rng_state(),\n",
        "    }\n",
        "    if extra: pack.update(extra)\n",
        "    torch.save(pack, path)\n",
        "\n",
        "def try_resume():\n",
        "    start_epoch = 0\n",
        "    best_f1, best_loss = -1.0, float(\"inf\")\n",
        "    if os.path.exists(CKPT_LAST):\n",
        "        sd = torch.load(CKPT_LAST, map_location=\"cpu\")\n",
        "        model.load_state_dict(sd[\"model\"], strict=False)\n",
        "        if \"optimizer\" in sd: optimizer.load_state_dict(sd[\"optimizer\"])\n",
        "        if torch.cuda.is_available() and sd.get(\"scaler\") is not None:\n",
        "            scaler.load_state_dict(sd[\"scaler\"])\n",
        "        _set_rng_state(sd.get(\"rng\"))\n",
        "        start_epoch = sd.get(\"epoch\", 0)\n",
        "        best_f1     = sd.get(\"best_f1\", best_f1)\n",
        "        best_loss   = sd.get(\"best_loss\", best_loss)\n",
        "        print(f\"🔁 Reanudado desde {CKPT_LAST} (epoch={start_epoch})\")\n",
        "    else:\n",
        "        print(\"⏩ Sin estado previo completo. Usando CKPT_IN (solo pesos) si existe.\")\n",
        "        if os.path.exists(CKPT_IN):\n",
        "            sd = torch.load(CKPT_IN, map_location=\"cpu\")\n",
        "            state_dict = sd[\"model\"] if isinstance(sd, dict) and \"model\" in sd else sd\n",
        "            missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "            print(\"Cargado ckpt previo:\", CKPT_IN)\n",
        "            print(\"Missing:\", missing, \"Unexpected:\", unexpected)\n",
        "        else:\n",
        "            print(\"⚠️ No se encontró CKPT_IN; se parte de Kinetics.\")\n",
        "    return start_epoch, best_f1, best_loss\n",
        "\n",
        "# ---------- LOOP ----------\n",
        "def run_epoch(loader, train=True, current_epoch=0, save_every=None,\n",
        "              max_batches=None, global_step_start=0):\n",
        "    if train: model.train()\n",
        "    else:     model.eval()\n",
        "\n",
        "    tot_loss = 0.0\n",
        "    all_probs, all_ys = [], []\n",
        "    global_step = global_step_start\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        if (max_batches is not None) and (i >= max_batches):\n",
        "            break\n",
        "\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        from torch.amp import autocast\n",
        "        with autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            if torch.cuda.is_available():\n",
        "                scaler.scale(loss).backward()\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 5.0)\n",
        "                scaler.step(optimizer); scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 5.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            global_step += 1\n",
        "            # Checkpoint intermedio\n",
        "            if save_every and (global_step % save_every == 0):\n",
        "                save_ckpt(CKPT_LAST, epoch=current_epoch, best_f1=best_f1,\n",
        "                          best_loss=best_loss, row={\"epoch\": current_epoch},\n",
        "                          extra={\"global_step\": global_step})\n",
        "                print(f\"💾 Guardado intermedio (step={global_step}) -> {CKPT_LAST}\")\n",
        "\n",
        "        probs_hurto = logits.softmax(1)[:,1].detach().cpu().numpy()\n",
        "        all_probs.append(probs_hurto)\n",
        "        all_ys.append(y.detach().cpu().numpy())\n",
        "        tot_loss += loss.item() * y.size(0)\n",
        "\n",
        "    all_probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
        "    all_ys    = np.concatenate(all_ys)    if all_ys    else np.array([])\n",
        "    avg_loss  = tot_loss / max(1, len(loader.dataset))\n",
        "\n",
        "    # F1 al umbral 0.5 como referencia\n",
        "    if all_probs.size > 0:\n",
        "        y_pred = (all_probs >= 0.5).astype(int)\n",
        "        f1 = f1_score(all_ys, y_pred, pos_label=1)\n",
        "    else:\n",
        "        f1 = 0.0\n",
        "    return avg_loss, f1, all_probs, all_ys, global_step\n",
        "\n",
        "# Inicializar/reanudar\n",
        "start_epoch, best_f1, best_loss = try_resume()\n",
        "history = []\n",
        "global_step = 0\n",
        "\n",
        "first_epoch = start_epoch + 1\n",
        "last_epoch  = min(EPOCHS, start_epoch + RUN_EPOCHS_THIS_SESSION)\n",
        "\n",
        "for epoch in range(first_epoch, last_epoch + 1):\n",
        "    # scheduler manual con warmup+cosine\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg[\"lr\"] = cosine_warmup_lr(epoch-1, EPOCHS, LR, warmup=2)\n",
        "\n",
        "    tr_loss, tr_f1, _, _, global_step = run_epoch(\n",
        "        train_loader, train=True, current_epoch=epoch,\n",
        "        save_every=SAVE_EVERY_N_BATCHES,\n",
        "        max_batches=MAX_TRAIN_BATCHES,\n",
        "        global_step_start=global_step\n",
        "    )\n",
        "    va_loss, va_f1, probs, ys, _ = run_epoch(\n",
        "        val_loader, train=False, current_epoch=epoch,\n",
        "        save_every=None, max_batches=MAX_VAL_BATCHES,\n",
        "        global_step_start=global_step\n",
        "    )\n",
        "\n",
        "    row = {\"epoch\": epoch, \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "           \"train_loss\": tr_loss, \"train_f1\": tr_f1,\n",
        "           \"val_loss\": va_loss,   \"val_f1\": va_f1}\n",
        "    history.append(row)\n",
        "    print(f\"[{epoch:02d}/{EPOCHS}] lr={row['lr']:.2e} | \"\n",
        "          f\"train {tr_loss:.4f}/{tr_f1:.3f} | val {va_loss:.4f}/{va_f1:.3f}\")\n",
        "\n",
        "    # Guardado \"last\" SIEMPRE al final de cada época (para reanudar exacto)\n",
        "    save_ckpt(CKPT_LAST, epoch=epoch, best_f1=best_f1, best_loss=best_loss, row=row)\n",
        "    try:\n",
        "        shutil.copy2(CKPT_LAST, CKPT_RUN)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Guardados “best”\n",
        "    if va_f1 > best_f1:\n",
        "        best_f1 = va_f1\n",
        "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTF1)\n",
        "        print(\"✔️ Mejor F1, guardado ->\", CKPT_BESTF1)\n",
        "    if va_loss < best_loss:\n",
        "        best_loss = va_loss\n",
        "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTL)\n",
        "        print(\"✔️ Mejor Loss, guardado ->\", CKPT_BESTL)\n",
        "\n",
        "# ---------- LOG / REPORT ----------\n",
        "with open(LOG_JSON, \"w\") as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "# Reporte en el último estado: clasificación y matriz (umbral 0.5)\n",
        "if 'probs' in locals() and probs.size > 0:\n",
        "    y_pred = (probs >= 0.5).astype(int)\n",
        "    print(\"\\n=== Classification Report (umbral 0.5) ===\")\n",
        "    print(classification_report(ys, y_pred, target_names=[\"normal\",\"hurto\"]))\n",
        "    print(\"Matriz de confusión (val):\\n\", confusion_matrix(ys, y_pred))\n",
        "else:\n",
        "    print(\"\\n(No hubo validación con muestras en esta sesión; salteo reporte inmediato)\")\n",
        "\n",
        "# Sweep de umbral para elegir threshold\n",
        "import csv as _csv\n",
        "if 'probs' in locals() and probs.size > 0:\n",
        "    ths = np.round(np.linspace(0.30, 0.80, 21), 3)\n",
        "    with open(THR_CSV, \"w\", newline=\"\") as f:\n",
        "        w = _csv.writer(f); w.writerow([\"threshold\",\"precision\",\"recall\",\"f1\"])\n",
        "        for t in ths:\n",
        "            yp = (probs >= t).astype(int)\n",
        "            rep = classification_report(ys, yp, output_dict=True, zero_division=0)\n",
        "            p = rep[\"1\"][\"precision\"]; r = rep[\"1\"][\"recall\"]; f1 = rep[\"1\"][\"f1-score\"]\n",
        "            w.writerow([t, p, r, f1])\n",
        "    print(\"Tabla de thresholds guardada en:\", THR_CSV)\n",
        "else:\n",
        "    print(\"Sin probs/ys de validación en esta sesión; no genero THR_CSV aún.\")\n",
        "\n",
        "print(\"Log de entrenamiento:\", LOG_JSON)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Fine-tune R3D-18 layer3+layer4 (CPU/GPU con reanudación y micro-sesiones)\n",
        "# =========================\n",
        "import os, math, json, random, csv, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_video\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "SEED        = 1337\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS      = 15\n",
        "BATCH_SIZE  = 6\n",
        "LR          = 2e-4\n",
        "WEIGHT_DEC  = 5e-4   # <--- subimos regularización L2 (antes 2e-4)\n",
        "GAMMA       = 2.0    # <--- focal un poco menos agresiva (antes 2.5)\n",
        "\n",
        "OUT_DIR     = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CKPT_IN     = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"  # mejor de layer4\n",
        "CKPT_BESTF1 = os.path.join(OUT_DIR, \"r3d18_shoplifting_ft_l34_bestF1.pt\")\n",
        "CKPT_BESTL  = os.path.join(OUT_DIR, \"r3d18_shoplifting_ft_l34_bestLoss.pt\")\n",
        "LOG_JSON    = os.path.join(OUT_DIR, \"r3d18_ft_l34_trainlog.json\")\n",
        "THR_CSV     = os.path.join(OUT_DIR, \"r3d18_ft_l34_threshold_sweep.csv\")\n",
        "\n",
        "# ---------- CHECKPOINTS / CONTROL DE SESIÓN ----------\n",
        "CKPT_LAST   = os.path.join(OUT_DIR, \"r3d18_ft_l34_last.pt\")      # último estado (para reanudar)\n",
        "CKPT_RUN    = os.path.join(OUT_DIR, \"r3d18_ft_l34_runstate.pt\")  # alias (copia del last)\n",
        "\n",
        "# Ejecutá en “micro-sesiones”\n",
        "RUN_EPOCHS_THIS_SESSION = 2      # cuántas épocas avanza ESTA corrida\n",
        "SAVE_EVERY_N_BATCHES    = 300    # checkpoint intermedio cada N batches (None para desactivar)\n",
        "\n",
        "# Para acelerar/probar en CPU: limitá batches por época (None = sin límite)\n",
        "MAX_TRAIN_BATCHES = None   # ej: 400\n",
        "MAX_VAL_BATCHES   = None   # ej: 100\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed()\n",
        "\n",
        "# ---- GPU perf tweaks ----\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"medium\")\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# ---------- DATA LOADERS ----------\n",
        "BASE_DIR  = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "TRAIN_CSV = f\"{BASE_DIR}/train.csv\"\n",
        "VAL_CSV   = f\"{BASE_DIR}/val.csv\"\n",
        "\n",
        "CLIP_LEN = 16   # frames por clip\n",
        "RESIZE   = 128\n",
        "CROP     = 112\n",
        "\n",
        "train_spatial = T.Compose([\n",
        "    T.Resize((RESIZE, RESIZE)),\n",
        "    T.RandomResizedCrop(CROP, scale=(0.7, 1.0)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomApply([T.ColorJitter(0.3,0.3,0.3,0.08)], p=0.5),\n",
        "    # T.RandomErasing(p=0.25, scale=(0.02, 0.15), ratio=(0.3, 3.3), inplace=False),  # <--- desactivado para CPU\n",
        "])\n",
        "\n",
        "val_spatial = T.Compose([\n",
        "    T.Resize((RESIZE, RESIZE)),\n",
        "    T.CenterCrop(CROP),\n",
        "])\n",
        "\n",
        "def load_list(csv_path):\n",
        "    \"\"\"Lee CSV con filas: path,label\n",
        "       label puede ser 0/1 o 'normal'/'shoplifting'/'hurto' (case-insensitive).\"\"\"\n",
        "    def parse_label(s):\n",
        "        s = str(s).strip().lower()\n",
        "        if s.isdigit(): return int(s)\n",
        "        if s in (\"normal\", \"0\"): return 0\n",
        "        if s in (\"shoplifting\", \"hurto\", \"1\"): return 1\n",
        "        raise ValueError(f\"Etiqueta desconocida: {s}\")\n",
        "\n",
        "    items = []\n",
        "    with open(csv_path, newline='') as f:\n",
        "        r = csv.reader(f)\n",
        "        for row in r:\n",
        "            if not row:\n",
        "                continue\n",
        "            head = row[0].strip().lower()\n",
        "            if head in (\"path\",\"ruta\",\"file\",\"filename\"):  # saltar cabecera\n",
        "                continue\n",
        "            path = row[0].strip()\n",
        "            y    = parse_label(row[1])\n",
        "\n",
        "            # resolver ruta relativa\n",
        "            if not path.startswith(\"/\"):\n",
        "                cand = os.path.join(BASE_DIR, path)\n",
        "                if os.path.exists(cand):\n",
        "                    path = cand\n",
        "                else:\n",
        "                    subdir = \"normal\" if y == 0 else \"shoplifting\"\n",
        "                    path = os.path.join(BASE_DIR, subdir, path)\n",
        "            items.append((path, y))\n",
        "    return items\n",
        "\n",
        "class SimpleVideoDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items   = items\n",
        "        self.train   = train\n",
        "        self.spatial = train_spatial if train else val_spatial\n",
        "        # normalización Kinetics para r3d_18\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1,3,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989]).view(1,3,1,1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, y = self.items[idx]\n",
        "        # video -> [T, C, H, W] uint8\n",
        "        video, _, _ = read_video(path, pts_unit=\"sec\", output_format=\"TCHW\")\n",
        "        T_total = video.shape[0]\n",
        "\n",
        "        # muestreo simple de un clip de CLIP_LEN\n",
        "        if T_total >= CLIP_LEN:\n",
        "            start = random.randint(0, T_total - CLIP_LEN) if self.train else max(0, (T_total - CLIP_LEN)//2)\n",
        "            clip = video[start:start+CLIP_LEN]  # [T,C,H,W]\n",
        "        else:\n",
        "            # pad repitiendo el último frame\n",
        "            pad = CLIP_LEN - T_total\n",
        "            clip = torch.cat([video, video[-1:].repeat(pad,1,1,1)], dim=0)\n",
        "\n",
        "        # aplicar transform espacial por frame (cada frame es [C,H,W])\n",
        "        clip = torch.stack([self.spatial(fr) for fr in clip])   # [T,C,H,W]\n",
        "        clip = clip.float() / 255.0\n",
        "        clip = (clip - self.mean) / self.std\n",
        "\n",
        "        # r3d_18 espera [C,T,H,W]\n",
        "        clip = clip.permute(1,0,2,3).contiguous()\n",
        "        return clip, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# construir datasets/loaders\n",
        "train_items = load_list(TRAIN_CSV)\n",
        "val_items   = load_list(VAL_CSV)\n",
        "\n",
        "train_ds = SimpleVideoDataset(train_items, train=True)\n",
        "val_ds   = SimpleVideoDataset(val_items,   train=False)\n",
        "\n",
        "# (Opcional) sampler balanceado:\n",
        "USE_SAMPLER = False\n",
        "\n",
        "N_normal = sum(1 for _,y in train_items if y==0)\n",
        "N_hurto  = sum(1 for _,y in train_items if y==1)\n",
        "print(f\"Train clips: {len(train_ds)} | Val clips: {len(val_ds)}\")\n",
        "print(f\"Conteo train (items CSV) → normal={N_normal} | hurto={N_hurto}\")\n",
        "\n",
        "# DataLoader: en CPU evitamos workers y pin_memory\n",
        "NUM_WORKERS = 0\n",
        "pinmem = False  # torch.cuda.is_available() y útil solo en GPU\n",
        "\n",
        "if USE_SAMPLER:\n",
        "    from torch.utils.data import WeightedRandomSampler\n",
        "    weight_per_class = {0: 1.0/max(1,N_normal), 1: 1.0/max(1,N_hurto)}\n",
        "    sample_weights = np.array([weight_per_class[y] for _, y in train_items], dtype=np.float32)\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=sampler,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=pinmem,\n",
        "        drop_last=True,\n",
        "        persistent_workers=(NUM_WORKERS>0)\n",
        "    )\n",
        "else:\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=pinmem,\n",
        "        drop_last=True,\n",
        "        persistent_workers=(NUM_WORKERS>0)\n",
        "    )\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=pinmem,\n",
        "    persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "\n",
        "# pesos por clase (AJUSTADOS para subir recall de hurto)\n",
        "CLASS_WEIGHTS_CPU = torch.tensor([1.0, 1.3], dtype=torch.float32)\n",
        "print(f\"Class weights fijados -> normal={CLASS_WEIGHTS_CPU[0].item():.3f} | hurto={CLASS_WEIGHTS_CPU[1].item():.3f}\")\n",
        "\n",
        "# ---------- MODELO ----------\n",
        "# compatibilidad de versiones de torchvision\n",
        "try:\n",
        "    model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "except TypeError:\n",
        "    model = r3d_18(pretrained=True)\n",
        "\n",
        "# FC con Dropout para bajar overfit\n",
        "in_feats = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(in_feats, NUM_CLASSES)\n",
        ")\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Congelar TODO menos layer3, layer4 y fc\n",
        "for name, p in model.named_parameters():\n",
        "    p.requires_grad = any(blk in name for blk in [\"layer3\",\"layer4\",\"fc\"])\n",
        "\n",
        "# ---------- LOSS (Focal) ----------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, weight=None, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction=\"none\")\n",
        "        self.reduction = reduction\n",
        "    def forward(self, logits, target):\n",
        "        ce = self.ce(logits, target)         # [B]\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1-pt)**self.gamma) * ce     # [B]\n",
        "        if self.reduction == \"mean\": return loss.mean()\n",
        "        if self.reduction == \"sum\":  return loss.sum()\n",
        "        return loss\n",
        "\n",
        "criterion = FocalLoss(gamma=GAMMA, weight=CLASS_WEIGHTS_CPU.to(DEVICE))\n",
        "\n",
        "# ---------- OPTIM / SCHED ----------\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                              lr=LR, weight_decay=WEIGHT_DEC)\n",
        "\n",
        "# AMP GradScaler para GPU\n",
        "from torch.amp import GradScaler\n",
        "scaler = GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n",
        "\n",
        "def cosine_warmup_lr(t, T, base_lr, warmup=2):\n",
        "    if t < warmup:\n",
        "        return base_lr * (t+1)/warmup\n",
        "    tw = max(1, T - warmup)\n",
        "    tc = max(0, t - warmup)\n",
        "    return 0.5 * base_lr * (1 + math.cos(math.pi * tc / tw))\n",
        "\n",
        "# ---------- REANUDACIÓN COMPLETA ----------\n",
        "def _rng_state():\n",
        "    return {\n",
        "        \"py_random\": random.getstate(),\n",
        "        \"np_random\": np.random.get_state(),\n",
        "        \"torch_cpu\": torch.random.get_rng_state(),\n",
        "        \"torch_cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "    }\n",
        "\n",
        "def _set_rng_state(st):\n",
        "    if st is None: return\n",
        "    random.setstate(st.get(\"py_random\"))\n",
        "    np.random.set_state(st.get(\"np_random\"))\n",
        "    torch.random.set_rng_state(st.get(\"torch_cpu\"))\n",
        "    if torch.cuda.is_available() and st.get(\"torch_cuda\") is not None:\n",
        "        torch.cuda.set_rng_state_all(st.get(\"torch_cuda\"))\n",
        "\n",
        "def save_ckpt(path, epoch, best_f1, best_loss, row, extra=None):\n",
        "    pack = {\n",
        "        \"epoch\": epoch,\n",
        "        \"best_f1\": best_f1,\n",
        "        \"best_loss\": best_loss,\n",
        "        \"metrics\": row,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict() if torch.cuda.is_available() else None,\n",
        "        \"class_weights\": CLASS_WEIGHTS_CPU,\n",
        "        \"rng\": _rng_state(),\n",
        "    }\n",
        "    if extra: pack.update(extra)\n",
        "    torch.save(pack, path)\n",
        "\n",
        "def try_resume():\n",
        "    start_epoch = 0\n",
        "    best_f1, best_loss = -1.0, float(\"inf\")\n",
        "    if os.path.exists(CKPT_LAST):\n",
        "        sd = torch.load(CKPT_LAST, map_location=\"cpu\")\n",
        "        model.load_state_dict(sd[\"model\"], strict=False)\n",
        "        if \"optimizer\" in sd: optimizer.load_state_dict(sd[\"optimizer\"])\n",
        "        if torch.cuda.is_available() and sd.get(\"scaler\") is not None:\n",
        "            scaler.load_state_dict(sd[\"scaler\"])\n",
        "        _set_rng_state(sd.get(\"rng\"))\n",
        "        start_epoch = sd.get(\"epoch\", 0)\n",
        "        best_f1     = sd.get(\"best_f1\", best_f1)\n",
        "        best_loss   = sd.get(\"best_loss\", best_loss)\n",
        "        print(f\"🔁 Reanudado desde {CKPT_LAST} (epoch={start_epoch})\")\n",
        "    else:\n",
        "        print(\"⏩ Sin estado previo completo. Usando CKPT_IN (solo pesos) si existe.\")\n",
        "        if os.path.exists(CKPT_IN):\n",
        "            sd = torch.load(CKPT_IN, map_location=\"cpu\")\n",
        "            state_dict = sd[\"model\"] if isinstance(sd, dict) and \"model\" in sd else sd\n",
        "            missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "            print(\"Cargado ckpt previo:\", CKPT_IN)\n",
        "            print(\"Missing:\", missing, \"Unexpected:\", unexpected)\n",
        "        else:\n",
        "            print(\"⚠️ No se encontró CKPT_IN; se parte de Kinetics.\")\n",
        "    return start_epoch, best_f1, best_loss\n",
        "\n",
        "# ---------- LOOP ----------\n",
        "def run_epoch(loader, train=True, current_epoch=0, save_every=None,\n",
        "              max_batches=None, global_step_start=0):\n",
        "    if train: model.train()\n",
        "    else:     model.eval()\n",
        "\n",
        "    tot_loss = 0.0\n",
        "    all_probs, all_ys = [], []\n",
        "    global_step = global_step_start\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        if (max_batches is not None) and (i >= max_batches):\n",
        "            break\n",
        "\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        from torch.amp import autocast\n",
        "        with autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            if torch.cuda.is_available():\n",
        "                scaler.scale(loss).backward()\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 5.0)\n",
        "                scaler.step(optimizer); scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 5.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            global_step += 1\n",
        "            # Checkpoint intermedio\n",
        "            if save_every and (global_step % save_every == 0):\n",
        "                save_ckpt(CKPT_LAST, epoch=current_epoch, best_f1=best_f1,\n",
        "                          best_loss=best_loss, row={\"epoch\": current_epoch},\n",
        "                          extra={\"global_step\": global_step})\n",
        "                print(f\"💾 Guardado intermedio (step={global_step}) -> {CKPT_LAST}\")\n",
        "\n",
        "        probs_hurto = logits.softmax(1)[:,1].detach().cpu().numpy()\n",
        "        all_probs.append(probs_hurto)\n",
        "        all_ys.append(y.detach().cpu().numpy())\n",
        "        tot_loss += loss.item() * y.size(0)\n",
        "\n",
        "    all_probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
        "    all_ys    = np.concatenate(all_ys)    if all_ys    else np.array([])\n",
        "    avg_loss  = tot_loss / max(1, len(loader.dataset))\n",
        "\n",
        "    # F1 al umbral 0.5 como referencia\n",
        "    if all_probs.size > 0:\n",
        "        y_pred = (all_probs >= 0.5).astype(int)\n",
        "        f1 = f1_score(all_ys, y_pred, pos_label=1)\n",
        "    else:\n",
        "        f1 = 0.0\n",
        "    return avg_loss, f1, all_probs, all_ys, global_step\n",
        "\n",
        "# Inicializar/reanudar\n",
        "start_epoch, best_f1, best_loss = try_resume()\n",
        "history = []\n",
        "global_step = 0\n",
        "\n",
        "first_epoch = start_epoch + 1\n",
        "last_epoch  = min(EPOCHS, start_epoch + RUN_EPOCHS_THIS_SESSION)\n",
        "\n",
        "for epoch in range(first_epoch, last_epoch + 1):\n",
        "    # scheduler manual con warmup+cosine\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg[\"lr\"] = cosine_warmup_lr(epoch-1, EPOCHS, LR, warmup=2)\n",
        "\n",
        "    tr_loss, tr_f1, _, _, global_step = run_epoch(\n",
        "        train_loader, train=True, current_epoch=epoch,\n",
        "        save_every=SAVE_EVERY_N_BATCHES,\n",
        "        max_batches=MAX_TRAIN_BATCHES,\n",
        "        global_step_start=global_step\n",
        "    )\n",
        "    va_loss, va_f1, probs, ys, _ = run_epoch(\n",
        "        val_loader, train=False, current_epoch=epoch,\n",
        "        save_every=None, max_batches=MAX_VAL_BATCHES,\n",
        "        global_step_start=global_step\n",
        "    )\n",
        "\n",
        "    row = {\"epoch\": epoch, \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "           \"train_loss\": tr_loss, \"train_f1\": tr_f1,\n",
        "           \"val_loss\": va_loss,   \"val_f1\": va_f1}\n",
        "    history.append(row)\n",
        "    print(f\"[{epoch:02d}/{EPOCHS}] lr={row['lr']:.2e} | \"\n",
        "          f\"train {tr_loss:.4f}/{tr_f1:.3f} | val {va_loss:.4f}/{va_f1:.3f}\")\n",
        "\n",
        "    # Guardado \"last\" SIEMPRE al final de cada época (para reanudar exacto)\n",
        "    save_ckpt(CKPT_LAST, epoch=epoch, best_f1=best_f1, best_loss=best_loss, row=row)\n",
        "    try:\n",
        "        shutil.copy2(CKPT_LAST, CKPT_RUN)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Guardados “best”\n",
        "    if va_f1 > best_f1:\n",
        "        best_f1 = va_f1\n",
        "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTF1)\n",
        "        print(\"✔️ Mejor F1, guardado ->\", CKPT_BESTF1)\n",
        "    if va_loss < best_loss:\n",
        "        best_loss = va_loss\n",
        "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTL)\n",
        "        print(\"✔️ Mejor Loss, guardado ->\", CKPT_BESTL)\n",
        "\n",
        "# ---------- LOG / REPORT ----------\n",
        "with open(LOG_JSON, \"w\") as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "# Reporte en el último estado: clasificación y matriz (umbral 0.5)\n",
        "if 'probs' in locals() and probs.size > 0:\n",
        "    y_pred = (probs >= 0.5).astype(int)\n",
        "    print(\"\\n=== Classification Report (umbral 0.5) ===\")\n",
        "    print(classification_report(ys, y_pred, target_names=[\"normal\",\"hurto\"]))\n",
        "    print(\"Matriz de confusión (val):\\n\", confusion_matrix(ys, y_pred))\n",
        "else:\n",
        "    print(\"\\n(No hubo validación con muestras en esta sesión; salteo reporte inmediato)\")\n",
        "\n",
        "# Sweep de umbral para elegir threshold\n",
        "import csv as _csv, json as _json\n",
        "if 'probs' in locals() and probs.size > 0:\n",
        "    ths = np.round(np.linspace(0.30, 0.80, 21), 3)\n",
        "    with open(THR_CSV, \"w\", newline=\"\") as f:\n",
        "        w = _csv.writer(f); w.writerow([\"threshold\",\"precision\",\"recall\",\"f1\"])\n",
        "        for t in ths:\n",
        "            yp = (probs >= t).astype(int)\n",
        "            rep = classification_report(ys, yp, output_dict=True, zero_division=0)\n",
        "            p = rep[\"1\"][\"precision\"]; r = rep[\"1\"][\"recall\"]; f1 = rep[\"1\"][\"f1-score\"]\n",
        "            w.writerow([t, p, r, f1])\n",
        "    print(\"Tabla de thresholds guardada en:\", THR_CSV)\n",
        "\n",
        "    # === Elegir mejor threshold por F1, mostrar top-5 y guardarlo como operativo ===\n",
        "    tops = []\n",
        "    with open(THR_CSV, newline=\"\") as f:\n",
        "        r = _csv.DictReader(f)\n",
        "        for row in r:\n",
        "            tops.append((\n",
        "                float(row[\"f1\"]),\n",
        "                float(row[\"threshold\"]),\n",
        "                float(row[\"precision\"]),\n",
        "                float(row[\"recall\"])\n",
        "            ))\n",
        "    tops.sort(reverse=True)\n",
        "\n",
        "    if tops:\n",
        "        print(\"\\nTop-5 thresholds por F1:\")\n",
        "        for f1, t, p, r in tops[:5]:\n",
        "            print(f\" t={t:.3f} | F1={f1:.3f} | P={p:.3f} | R={r:.3f}\")\n",
        "\n",
        "        best_t = tops[0][1]\n",
        "        y_pred_best = (probs >= best_t).astype(int)\n",
        "        print(f\"\\n=== Report con threshold óptimo (t={best_t:.3f}) ===\")\n",
        "        print(classification_report(ys, y_pred_best, target_names=[\"normal\",\"hurto\"]))\n",
        "        print(\"Matriz de confusión (val, t*):\\n\", confusion_matrix(ys, y_pred_best))\n",
        "\n",
        "        OPER_JSON = os.path.join(OUT_DIR, \"operating_threshold.json\")\n",
        "        with open(OPER_JSON, \"w\") as f:\n",
        "            _json.dump({\"threshold\": float(best_t)}, f, indent=2)\n",
        "        print(\"Operating threshold ->\", OPER_JSON)\n",
        "else:\n",
        "    print(\"Sin probs/ys de validación en esta sesión; no genero THR_CSV aún.\")\n",
        "\n",
        "print(\"Log de entrenamiento:\", LOG_JSON)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkc5zKXNOEC5",
        "outputId": "1a61f0aa-1546-4938-e110-18708e97ad9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA disponible: False\n",
            "Train clips: 10668 | Val clips: 787\n",
            "Conteo train (items CSV) → normal=2082 | hurto=8586\n",
            "Class weights fijados -> normal=1.000 | hurto=1.300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3571270764.py:299: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  sd = torch.load(CKPT_LAST, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Reanudado desde /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt (epoch=9)\n",
            "💾 Guardado intermedio (step=300) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=600) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=900) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=1200) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=1500) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "[10/15] lr=8.79e-05 | train 0.0009/1.000 | val 1.1922/0.466\n",
            "✔️ Mejor F1, guardado -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "💾 Guardado intermedio (step=1800) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2100) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2400) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2700) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=3000) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=3300) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "[11/15] lr=6.45e-05 | train 0.0002/1.000 | val 1.4398/0.531\n",
            "✔️ Mejor F1, guardado -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "\n",
            "=== Classification Report (umbral 0.5) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.21      0.97      0.35       118\n",
            "       hurto       0.98      0.36      0.53       669\n",
            "\n",
            "    accuracy                           0.45       787\n",
            "   macro avg       0.60      0.66      0.44       787\n",
            "weighted avg       0.87      0.45      0.50       787\n",
            "\n",
            "Matriz de confusión (val):\n",
            " [[114   4]\n",
            " [426 243]]\n",
            "Tabla de thresholds guardada en: /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_threshold_sweep.csv\n",
            "\n",
            "Top-5 thresholds por F1:\n",
            " t=0.300 | F1=0.689 | P=0.936 | R=0.546\n",
            " t=0.325 | F1=0.687 | P=0.950 | R=0.538\n",
            " t=0.350 | F1=0.678 | P=0.959 | R=0.525\n",
            " t=0.375 | F1=0.650 | P=0.965 | R=0.490\n",
            " t=0.400 | F1=0.624 | P=0.975 | R=0.459\n",
            "\n",
            "=== Report con threshold óptimo (t=0.300) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.23      0.79      0.36       118\n",
            "       hurto       0.94      0.55      0.69       669\n",
            "\n",
            "    accuracy                           0.58       787\n",
            "   macro avg       0.59      0.67      0.53       787\n",
            "weighted avg       0.83      0.58      0.64       787\n",
            "\n",
            "Matriz de confusión (val, t*):\n",
            " [[ 93  25]\n",
            " [304 365]]\n",
            "Operating threshold -> /content/drive/MyDrive/tesisV2/models/operating_threshold.json\n",
            "Log de entrenamiento: /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_trainlog.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x6hROCjVfWkw",
        "outputId": "241717ff-83dd-4756-e631-740d20b1cc42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.0\n",
            "  Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.19.0\n",
            "  Downloading torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting torchaudio==2.4.0\n",
            "  Downloading torchaudio-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting filelock (from torch==2.4.0)\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.4.0)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sympy (from torch==2.4.0)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.4.0)\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.4.0)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch==2.4.0)\n",
            "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting setuptools (from torch==2.4.0)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting numpy (from torchvision==0.19.0)\n",
            "  Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m132.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.19.0)\n",
            "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.4.0)\n",
            "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.0)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.4.0-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m167.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m172.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m165.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m188.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m179.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, setuptools, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 80.9.0\n",
            "    Uninstalling setuptools-80.9.0:\n",
            "      Successfully uninstalled setuptools-80.9.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.9.86\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.9.86:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.86\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.2\n",
            "    Uninstalling numpy-2.3.2:\n",
            "      Successfully uninstalled numpy-2.3.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.7.0\n",
            "    Uninstalling fsspec-2025.7.0:\n",
            "      Successfully uninstalled fsspec-2025.7.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.19.1\n",
            "    Uninstalling filelock-3.19.1:\n",
            "      Successfully uninstalled filelock-3.19.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0\n",
            "    Uninstalling torch-2.4.0:\n",
            "      Successfully uninstalled torch-2.4.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.0\n",
            "    Uninstalling torchvision-0.19.0:\n",
            "      Successfully uninstalled torchvision-0.19.0\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.4.0\n",
            "    Uninstalling torchaudio-2.4.0:\n",
            "      Successfully uninstalled torchaudio-2.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.7.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 setuptools-80.9.0 sympy-1.14.0 torch-2.4.0 torchaudio-2.4.0 torchvision-0.19.0 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_distutils_hack",
                  "mpmath",
                  "numpy",
                  "setuptools",
                  "sympy",
                  "torch",
                  "torchgen",
                  "torchvision",
                  "triton"
                ]
              },
              "id": "fb473a6de5ce4b6489a09dc2d220c36c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install --upgrade --force-reinstall --no-cache-dir torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCJlMSx_fWbU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# EVALUACIÓN (SIN ENTRENAR) con OpenCV\n",
        "# R3D-18 | t=0.30 + smoothing + histéresis + minDur\n",
        "# =========================\n",
        "import os, re, csv\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.video import r3d_18\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import cv2\n",
        "\n",
        "# ---------- RUTAS ----------\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BASE_DIR   = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "VAL_CSV    = f\"{BASE_DIR}/val.csv\"\n",
        "TEST_CSV   = f\"{BASE_DIR}/test.csv\"\n",
        "CKPT_PATH  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\"\n",
        "\n",
        "# ---------- MODELO ----------\n",
        "NUM_CLASSES = 2\n",
        "CLIP_LEN = 16\n",
        "RESIZE   = 128\n",
        "CROP     = 112\n",
        "\n",
        "val_spatial = T.Compose([T.Resize((RESIZE, RESIZE)), T.CenterCrop(CROP)])\n",
        "\n",
        "class SimpleVideoEval(Dataset):\n",
        "    def __init__(self, csv_path, base_dir):\n",
        "        self.items=[]\n",
        "        self.base_dir=base_dir\n",
        "        with open(csv_path, newline='') as f:\n",
        "            r = csv.reader(f)\n",
        "            for row in r:\n",
        "                if not row: continue\n",
        "                head=row[0].strip().lower()\n",
        "                if head in (\"path\",\"ruta\",\"file\",\"filename\"):  # header\n",
        "                    continue\n",
        "                path=row[0].strip()\n",
        "                y = 0 if str(row[1]).lower() in (\"0\",\"normal\") else 1\n",
        "                if not path.startswith(\"/\"):\n",
        "                    cand=os.path.join(base_dir, path)\n",
        "                    if os.path.exists(cand):\n",
        "                        path=cand\n",
        "                    else:\n",
        "                        sub=\"normal\" if y==0 else \"shoplifting\"\n",
        "                        path=os.path.join(base_dir, sub, path)\n",
        "                self.items.append((path, y))\n",
        "        # Normalización Kinetics\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1,3,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989]).view(1,3,1,1)\n",
        "\n",
        "    def __len__(self): return len(self.items)\n",
        "\n",
        "    def _read_video_cv2(self, path):\n",
        "        cap=cv2.VideoCapture(str(path))\n",
        "        frames=[]\n",
        "        while True:\n",
        "            ok, fr=cap.read()\n",
        "            if not ok: break\n",
        "            fr=cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(fr)\n",
        "        cap.release()\n",
        "        if len(frames)==0:\n",
        "            raise RuntimeError(f\"No se pudo leer: {path}\")\n",
        "        T_total=len(frames)\n",
        "        if T_total>=CLIP_LEN:\n",
        "            start=max(0,(T_total-CLIP_LEN)//2)  # centro para eval\n",
        "            frames=frames[start:start+CLIP_LEN]\n",
        "        else:\n",
        "            last=frames[-1]\n",
        "            frames=frames + [last]*(CLIP_LEN-T_total)\n",
        "        arr=np.stack(frames, axis=0)                 # [T,H,W,3] uint8\n",
        "        ten=torch.from_numpy(arr).permute(0,3,1,2)   # [T,3,H,W]\n",
        "        return ten\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path,y=self.items[idx]\n",
        "        clip=self._read_video_cv2(path)                  # [T,3,H,W]\n",
        "        clip=torch.stack([val_spatial(fr) for fr in clip])\n",
        "        clip=clip.float()/255.0\n",
        "        clip=(clip - self.mean)/self.std\n",
        "        clip=clip.permute(1,0,2,3).contiguous()          # [3,T,H,W]\n",
        "        return clip, torch.tensor(y, dtype=torch.long), path\n",
        "\n",
        "def build_model():\n",
        "    try:\n",
        "        model=r3d_18(weights=\"KINETICS400_V1\")\n",
        "    except TypeError:\n",
        "        model=r3d_18(pretrained=True)\n",
        "    in_feats=model.fc.in_features\n",
        "    model.fc=nn.Sequential(nn.Dropout(0.5), nn.Linear(in_feats, NUM_CLASSES))\n",
        "    sd=torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "    state_dict = sd[\"model\"] if isinstance(sd, dict) and \"model\" in sd else sd\n",
        "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"Checkpoint cargado:\", CKPT_PATH)\n",
        "    if missing or unexpected:\n",
        "        print(\"Missing:\", missing, \"Unexpected:\", unexpected)\n",
        "    return model.to(DEVICE).eval()\n",
        "\n",
        "# ---------- INFERENCIA ----------\n",
        "def infer_loader(model, loader):\n",
        "    probs_all, ys_all, paths_all = [], [], []\n",
        "    soft=nn.Softmax(dim=1)\n",
        "    torch.set_grad_enabled(False)\n",
        "    for x,y,paths in loader:\n",
        "        x=x.to(DEVICE, non_blocking=True)\n",
        "        logits=model(x)\n",
        "        probs=soft(logits)[:,1].detach().cpu().numpy()\n",
        "        probs_all.append(probs)\n",
        "        ys_all.append(y.numpy())\n",
        "        paths_all.extend(list(paths))\n",
        "    return np.concatenate(probs_all), np.concatenate(ys_all), paths_all\n",
        "\n",
        "# ---------- POST-PROCESO ----------\n",
        "HOP_SECONDS   = 0.5   # ajustá si tus clips provienen de hop distinto\n",
        "SMOOTH_K      = 5     # ~2.5 s si hop=0.5\n",
        "HYST_ON       = 0.35\n",
        "HYST_OFF      = 0.25\n",
        "MIN_EVENT_SEC = 1.0\n",
        "COOLDOWN_SEC  = 2.0\n",
        "THRESH_BASE   = 0.30\n",
        "\n",
        "def moving_avg(a,k):\n",
        "    if k<=1: return a\n",
        "    pad=(k-1)//2\n",
        "    a_pad=np.pad(a,(pad,pad),mode='edge')\n",
        "    ker=np.ones(k)/k\n",
        "    return np.convolve(a_pad,ker,mode='valid')\n",
        "\n",
        "def hysteresis_events(probs,on,off,hop_s,min_event_s,cooldown_s):\n",
        "    active=False; start=None; events=[]\n",
        "    for i,p in enumerate(probs):\n",
        "        if not active and p>=on:\n",
        "            active=True; start=i\n",
        "        elif active and p<=off:\n",
        "            end=i\n",
        "            if (end-start)*hop_s>=min_event_s:\n",
        "                events.append([start,end])\n",
        "            active=False\n",
        "    if active:\n",
        "        end=len(probs)\n",
        "        if (end-start)*hop_s>=min_event_s:\n",
        "            events.append([start,end])\n",
        "    merged=[]\n",
        "    for s,e in events:\n",
        "        if not merged: merged.append([s,e])\n",
        "        else:\n",
        "            ps,pe=merged[-1]\n",
        "            if (s-pe)*hop_s<COOLDOWN_SEC:\n",
        "                merged[-1][1]=e\n",
        "            else:\n",
        "                merged.append([s,e])\n",
        "    return merged\n",
        "\n",
        "def events_to_clip_labels(n, events):\n",
        "    y=np.zeros(n, dtype=np.int64)\n",
        "    for s,e in events: y[s:e]=1\n",
        "    return y\n",
        "\n",
        "_idx_pat = re.compile(r\"(?:_|-)(\\d{1,6})(?=\\D*$)\")\n",
        "def infer_vid_and_idx(path):\n",
        "    p=str(path); stem=Path(p).stem\n",
        "    m=_idx_pat.search(stem)\n",
        "    if m:\n",
        "        clip_idx=int(m.group(1))\n",
        "        video_id=stem[:m.start()] or Path(p).parent.name\n",
        "    else:\n",
        "        video_id=Path(p).parent.name\n",
        "        clip_idx=stem\n",
        "    return video_id, clip_idx\n",
        "\n",
        "def natkey(x):\n",
        "    try: return int(x)\n",
        "    except: return x\n",
        "\n",
        "def post_metrics(probs, ys, paths):\n",
        "    # baseline (sin post)\n",
        "    yb=(probs>=THRESH_BASE).astype(int)\n",
        "    cm_base=confusion_matrix(ys, yb, labels=[0,1])\n",
        "    rep_base=classification_report(ys, yb, target_names=[\"normal\",\"hurto\"], digits=3)\n",
        "\n",
        "    # agrupar por video para post-proceso temporal\n",
        "    from collections import defaultdict\n",
        "    buckets=defaultdict(list)\n",
        "    for p,pr,yv in zip(paths, probs, ys):\n",
        "        vid,idx=infer_vid_and_idx(p)\n",
        "        buckets[vid].append((idx, float(pr), int(yv)))\n",
        "\n",
        "    y_true_all=[]; y_pred_all=[]\n",
        "    for vid, triples in buckets.items():\n",
        "        triples.sort(key=lambda t: natkey(t[0]))\n",
        "        pv=np.array([t[1] for t in triples], float)\n",
        "        yv=np.array([t[2] for t in triples], int)\n",
        "        n=len(pv)\n",
        "        sm=moving_avg(pv, SMOOTH_K)\n",
        "        evs=hysteresis_events(sm, HYST_ON, HYST_OFF, HOP_SECONDS, MIN_EVENT_SEC, COOLDOWN_SEC)\n",
        "        yhat=events_to_clip_labels(n, evs)\n",
        "        y_true_all.append(yv); y_pred_all.append(yhat)\n",
        "\n",
        "    if y_true_all:\n",
        "        y_true_all=np.concatenate(y_true_all)\n",
        "        y_pred_all=np.concatenate(y_pred_all)\n",
        "        cm_post=confusion_matrix(y_true_all, y_pred_all, labels=[0,1])\n",
        "        rep_post=classification_report(y_true_all, y_pred_all, target_names=[\"normal\",\"hurto\"], digits=3)\n",
        "    else:\n",
        "        cm_post, rep_post = cm_base, rep_base  # fallback si no se pudo agrupar\n",
        "    return (cm_base, rep_base), (cm_post, rep_post)\n",
        "\n",
        "# ---------- CORRER ----------\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "model=build_model()\n",
        "\n",
        "for split_name, csv_path in [(\"VAL\", VAL_CSV), (\"TEST\", TEST_CSV)]:\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"[{split_name}] No existe:\", csv_path); continue\n",
        "    ds=SimpleVideoEval(csv_path, BASE_DIR)\n",
        "    dl=DataLoader(ds, batch_size=8, shuffle=False, num_workers=0)\n",
        "    probs, ys, paths = infer_loader(model, dl)\n",
        "\n",
        "    print(f\"\\n===== {split_name} =====\")\n",
        "    (cm_b, rep_b), (cm_p, rep_p) = post_metrics(probs, ys, paths)\n",
        "\n",
        "    print(\"\\n--- BASELINE t=0.30 (sin post) ---\")\n",
        "    print(cm_b); print(rep_b)\n",
        "\n",
        "    print(\"\\n--- (POST) smoothing + histéresis + minDur ---\")\n",
        "    print(cm_p); print(rep_p)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9YdTpDCL3fZ",
        "outputId": "e0bbac76-ad4a-4268-bb81-c2345ac9f397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: False\n",
            "Checkpoint cargado: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "\n",
            "===== VAL =====\n",
            "\n",
            "--- BASELINE t=0.30 (sin post) ---\n",
            "[[ 93  25]\n",
            " [304 365]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.234     0.788     0.361       118\n",
            "       hurto      0.936     0.546     0.689       669\n",
            "\n",
            "    accuracy                          0.582       787\n",
            "   macro avg      0.585     0.667     0.525       787\n",
            "weighted avg      0.831     0.582     0.640       787\n",
            "\n",
            "\n",
            "--- (POST) smoothing + histéresis + minDur ---\n",
            "[[101  17]\n",
            " [302 367]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.251     0.856     0.388       118\n",
            "       hurto      0.956     0.549     0.697       669\n",
            "\n",
            "    accuracy                          0.595       787\n",
            "   macro avg      0.603     0.702     0.542       787\n",
            "weighted avg      0.850     0.595     0.651       787\n",
            "\n",
            "\n",
            "===== TEST =====\n",
            "\n",
            "--- BASELINE t=0.30 (sin post) ---\n",
            "[[159  32]\n",
            " [169 410]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.485     0.832     0.613       191\n",
            "       hurto      0.928     0.708     0.803       579\n",
            "\n",
            "    accuracy                          0.739       770\n",
            "   macro avg      0.706     0.770     0.708       770\n",
            "weighted avg      0.818     0.739     0.756       770\n",
            "\n",
            "\n",
            "--- (POST) smoothing + histéresis + minDur ---\n",
            "[[163  28]\n",
            " [158 421]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.508     0.853     0.637       191\n",
            "       hurto      0.938     0.727     0.819       579\n",
            "\n",
            "    accuracy                          0.758       770\n",
            "   macro avg      0.723     0.790     0.728       770\n",
            "weighted avg      0.831     0.758     0.774       770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install av==10.0.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O75QUUX0NRsF",
        "outputId": "36abf0e1-f9cc-455e-cace-496aa70405bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/2.4 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d374O-qcXHaQ",
        "1e6aXTdlEyW8",
        "l0nZ5qLQoxVN",
        "NJnijYtdd7DK",
        "zc4I-AOkpTtG",
        "BSWhg5N3qyBl",
        "7jfOQgmS6szQ",
        "Gb9LYZ4072Hz",
        "kaEj4NPF9cNH",
        "7HmyRxJK96O5",
        "fS7bAIj1xeoB",
        "rnBWdtshxBbY"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}